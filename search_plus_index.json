{"./":{"url":"./","title":"Introduction","keywords":"","body":"1. 技术与业务共同驱动1.1. (初次打开请耐心，github源加载较慢)1.2. 1. 精深训练的理念1.3. 2. 定义合理边界——版本1.4. 3. 内容说明1.5. 4.共勉1.6. 有任何问题联系我1. 技术与业务共同驱动 1.1. (初次打开请耐心，github源加载较慢) 脱离了业务空谈技术都是耍流氓 1.2. 1. 精深训练的理念 热爱 | 重复 | 坚持 不断重复，从不停止。 永远追求做一个极致的产品，关注细节。 深度，广度，高度，均能达到。 1.3. 2. 定义合理边界——版本 边界，代表一类训练的范围，同时也决定着训练能带来的效果。这里使用版本梯度来表示边界。 为每一类训练指定一个合理的版本以及版本内容的规划。 训练评测——能够分享，代表着对一个训练的掌握。常见的自话式分享，PPT式分享。 定义一个合理的版本目标。 1.4. 3. 内容说明 学习别人优秀课程结合个人总结，只用于学习使用，无任何商业价值，转载请联系作者获得授权，并注明出处 1.5. 4.共勉 有一个夜晚，我烧毁了所有的记忆， 从此我的梦就透明了； 有个早晨我扔掉了所有的昨天， 从此我的脚步就轻盈了！ 越过山丘，才发现无人等候！ -- 泰戈尔 1.6. 有任何问题联系我 邮箱：lviter@163.com 感谢大家，打赏码支持 优秀的文档大家一起学习，推荐几个地址： 美团技术团队 阿里云内核月报 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/":{"url":"Java-技能/","title":"Java-技能","keywords":"","body":"1.1. Java core1.1. Java core Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/IO模型/":{"url":"Java-技能/IO模型/","title":"IO模型","keywords":"","body":"1. IO模型1.1. BIO/NIO/AIO区别1.1.1. 同步阻塞IO，BIO1.1.2. 同步非阻塞IO，NIO1.1.3. 异步IO,AIO1.2. nmap1. IO模型 主要有四种基本的I/O模型：同步阻塞I/O（Synchronous Blocking I/O）、同步非阻塞I/O（Synchronous Non-Blocking I/O）、异步I/O（Asynchronous I/O），以及信号驱动I/O（Signal-driven I/O）。但在实际应用中，通常是前三种模型 1.1. BIO/NIO/AIO区别 1.1.1. 同步阻塞IO，BIO 当一个线程发起一个I/O请求时，它会阻塞并等待直到I/O操作完成。在此期间，线程不能执行其他任务。这种模型适用于I/O操作较少且操作时间较短的场景。 java中应用：使用java.io包中的类，如InputStream和OutputStream。它是基于同步阻塞I/O的 1.1.2. 同步非阻塞IO，NIO 在这种模型中，当一个线程发起一个I/O请求时，它不会立即阻塞。如果I/O操作不能立即完成，线程会立即返回并继续执行其他任务。线程需要定期轮询I/O操作的状态，直到操作完成。这种模型适用于I/O操作频繁但每个操作时间较短的场景，例如网络服务器 java中应用：用java.nio包中的类，如Buffer、Channel和Selector。NIO支持同步非阻塞I/O，允许单个线程处理多个I/O连接。 1.1.3. 异步IO,AIO 异步I/O是最高效的I/O模型之一。在这种模型中，线程发起一个I/O请求后立即返回，继续执行其他任务。当I/O操作完成时，操作系统会通知线程（通常是通过回调函数或事件通知）。这种模型适用于高并发的场景，如高性能Web服务器 java中应用：使用java.nio.channels包中的AsynchronousFileChannel和AsynchronousSocketChannel等类。AIO支持真正的异步I/O操作，即操作完成后由系统通知应用程序 1.2. nmap Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/":{"url":"Java-技能/JVM/","title":"JVM","keywords":"","body":"1. JVM1.1. 运行时数据区1.2. 对象1.3. 内存溢出/内存泄露1.4. GC1.5. 类加载机制1. JVM 1.1. 运行时数据区 1.2. 对象 1.3. 内存溢出/内存泄露 1.4. GC 1.5. 类加载机制 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/深入理解JVM-jdk1.7/":{"url":"Java-技能/JVM/深入理解JVM-jdk1.7/","title":"深入理解JVM-jdk1.7","keywords":"","body":"1.1. JDK-1.71.2. 学习书籍1.1. JDK-1.7 不了解jdk1.7如何去分析jdk1.8? 1.2. 学习书籍 《深入理解JVM》--周志明 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/深入理解JVM-jdk1.7/HotSpot虚拟机对象探秘.html":{"url":"Java-技能/JVM/深入理解JVM-jdk1.7/HotSpot虚拟机对象探秘.html","title":"HotSpot虚拟机对象探秘","keywords":"","body":"1.1.1. HotSpot虚拟机对象探秘1.1.1. HotSpot虚拟机对象探秘 如何创建 、如何布局、以及如何访问 对象创建流程 虚拟机遇到一个new指令时，首先去检查指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化过。 类加载检查后，虚拟机将为新生对象分配内存（对象所需内存的大小在类加载完成后便可完全确定），为对象分配空间的任务等于把一块确定大小的内存从堆内划分出来。 3. 假设堆中内存规整，所有用过的内存放一边，空闲的内存放另一边，中间放着一个指针作为分界点的指示器，那分配内存就仅仅是把指针向空闲空间那边挪动一段与对象大小相等的距离，这种分配方式称为--指针碰撞（Bump the Pointer） - 如果堆内存不规整，已使用的内存和空闲内存相互交错，那就没办法进行简单的碰撞了，虚拟机就必须维护一个列表，记录哪块内存可用，分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新表上的记录，这种分配方式称为--空闲列表（Free List） 4. 选择哪种分配方式由Java堆是否规整决定，Java堆是否规整由所用的垃圾收集器是否带有压缩整理功能决定。因此，在使用Serial、ParNew等带Compact过程的收集器时，系统采用的分配算法是指针碰撞，而使用CMS这种基于Mark-Sweep算法的收集器时，通常采用空闲列表。 对象创建在虚拟机中是非常频繁的行为，所以在指针碰撞时并不是线程安全的，可能出现在给A对象分配内存时，指针还没来得及修改，对象B又同时使用了原来的指针来分配内存。解决这个问题有两种方案： 方案一：对分配内存空间的动作进行同步处理，----虚拟机采用CAS配上失败重试的方式保证更新操作的原子性 方案二：把内存分配动作按照线程划分在不同的空间中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）。哪个线程要分配内存，就在哪个线程得TLAB上分配，只有TLAB用完并分配新的TLAB时，才需要同步锁定。可以通过-XX：+/-UseTLAB参数来设定是否使用TLAB。 内存分配完毕，虚拟机需要将分配到得内存空间都初始化为零值（不包括对象头），使用TLAB时，这一过程也可以提前至TLAB分配时进行。这一步操作保证了对象的实例字段在java代码中可以不赋初始值直接使用。 虚拟机继续进行必要设置，如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。信息存放在对象头中， 上面步骤完成，从虚拟机的视角看，一个新的对象已经产生，但从java程序的视角来看，对象创建才刚刚开始，尚未init初始化，所有字段都为零。执行new指令后会接着执行init方法。 下面代码清单是HotSpot虚拟机bytecodeInterpreter.cpp中的代码片段，可以引导了解HotSpot的运作过程 //确保常量池中存放的是已解释的类 if（！constants-＞tag_at（index）.is_unresolved_klass（））{ //断言确保是klassOop和instanceKlassOop（这部分下一节介绍） oop entry=（klassOop）*constants-＞obj_at_addr（index）； assert（entry-＞is_klass（），\"Should be resolved klass\"）； klassOop k_entry=（klassOop）entry； assert（k_entry-＞klass_part（）-＞oop_is_instance（），\"Should be instanceKlass\"）； instanceKlass * ik=（instanceKlass*）k_entry-＞klass_part（）； //确保对象所属类型已经经过初始化阶段 if（ik-＞is_initialized（）＆＆ik-＞can_be_fastpath_allocated（）） {/ /取对象长度 size_t obj_size=ik-＞size_helper（）； oop result=NULL； //记录是否需要将对象所有字段置零值 bool need_zero=！ZeroTLAB； //是否在TLAB中分配对象 if（UseTLAB）{ result=（oop）THREAD-＞tlab（）.allocate（obj_size）； }i f（result==NULL）{ need_zero=true； //直接在eden中分配对象 retry： HeapWord * compare_to=*Universe：heap（）-＞top_addr（）； HeapWord * new_top=compare_to+obj_size； /*cmpxchg是x86中的CAS指令，这里是一个C++方法，通过CAS方式分配空间，如果并发失败， 转到retry中重试，直至成功分配为止*/ if（new_top＜=*Universe：heap（）-＞end_addr（））{ if（Atomic：cmpxchg_ptr（new_top,Universe：heap（）-＞top_addr（），compare_to）！=compare_to）{ goto retry； }r esult=（oop）compare_to； }}i f（result！=NULL）{ //如果需要，则为对象初始化零值 if（need_zero）{ HeapWord * to_zero=（HeapWord*）result+sizeof（oopDesc）/oopSize； obj_size-=sizeof（oopDesc）/oopSize； if（obj_size＞0）{ memset（to_zero，0，obj_size * HeapWordSize）； }}/ /根据是否启用偏向锁来设置对象头信息 if（UseBiasedLocking）{ result-＞set_mark（ik-＞prototype_header（））； }else{ result-＞set_mark（markOopDesc：prototype（））； }r esult-＞set_klass_gap（0）； result-＞set_klass（k_entry）； //将对象引用入栈，继续执行下一条指令 SET_STACK_OBJECT（result，0）； UPDATE_PC_AND_TOS_AND_CONTINUE（3，1）； }}} 对象的内存布局 HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding） HotSpot虚拟机的对象头包括两部分信息: 用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等 被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息，它会根据对象的状态复用自己的存储空间。 类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例 特殊：如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中却无法确定数组的大小。 - 实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。存储顺序会受到虚拟机分配策略参数（FieldsAllocationStyle）和字段在Java源码中定义顺序的影响。HotSpot虚拟机默认的分配策略为longs/doubles、ints、shorts/chars、bytes/booleans、oops（Ordinary Object Pointers），从分配策略中可以看出，相同宽度的字段总是被分配到一起 - 对齐填充并不是必然存在的，仅仅又占位符的作用。由于HotSpot虚拟机内存管理系统要求对象起始地址必须是8字节的整数倍，就是对象大小必须是8字节的整数倍。对象头部分正好是8字节的倍数（1倍或者2倍）因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问定位 在Java栈中保存了本地变量表，本地变量表内又reference数据，通过reference数据来操作堆上的具体对象。由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用，并没有定义这个引用应该通过何种方式去定位、访问堆中的对象的具体位置，所以对象访问方式也是取决于虚拟机实现而定的。目前主流的访问方式有使用 句柄和直接指针两种。 句柄访问 在java堆中会划分出一块内存作为句柄池，reference中存储的信息就是对象的句柄地址，句柄中包含了对象实例数据与类型数据各自的具体地址信息 使用句柄来访问的最大好处就是reference中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要修改 直接指针访问 reference中存储的直接就是对象地址 使用直接指针访问方式的最大好处就是速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本 注意：我们使用的HotSpot使用的就是第二种进行对象访问的。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/深入理解JVM-jdk1.7/JVM调优-jdk1.8.html":{"url":"Java-技能/JVM/深入理解JVM-jdk1.7/JVM调优-jdk1.8.html","title":"JVM调优-jdk1.8","keywords":"","body":"1.1. JVM一些命令以及解释1.1.1. 1. jmap命令概述1.1.2. 2. jmap例子详解1.1.3. 3. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量.1.1.4. 4. -clstats打印classload和jvm heap持久层的信息，包括每个classloader名字，活泼型，地址，父classloader和加载的class数量1.1.5. 5. 将内存详细使用情况打印文件1.1. JVM一些命令以及解释 1.1.1. 1. jmap命令概述 jmap可以输出所有内存中对象，甚至可以将VM中的heap输出，打印出某个java进程(pid)内存内的所有对象情况 1.1.2. 2. jmap例子详解 ```shell jmap -heap 17100 ``` 打印输出内容为： ```shell Attaching to process ID 17100, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.202-b08 using thread-local object allocation. Parallel GC with 8 thread(s) ##新生代采用并行线程处理方式执行垃圾回收 Heap Configuration: ##堆配置 MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 4257218560 (4060.0MB) NewSize = 88604672 (84.5MB) MaxNewSize = 1418723328 (1353.0MB) OldSize = 177733632 (169.5MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: PS Young Generation Eden Space: capacity = 53477376 (51.0MB) used = 2714632 (2.5888748168945312MB) free = 50762744 (48.41112518310547MB) 5.076225131165748% used From Space: capacity = 1048576 (1.0MB) used = 425984 (0.40625MB) free = 622592 (0.59375MB) 40.625% used To Space: capacity = 1048576 (1.0MB) used = 0 (0.0MB) free = 1048576 (1.0MB) 0.0% used PS Old Generation capacity = 265289728 (253.0MB) used = 76556600 (73.01006317138672MB) free = 188733128 (179.98993682861328MB) 28.85773247880898% used 27666 interned Strings occupying 2848840 bytes. ``` 2.1 Heap Configuration: MinHeapFreeRatio: 空间堆空间的最小百分比，公式：HeapFreeRatio =(CurrentFreeHeapSize/CurrentTotalHeapSize) * 100，值区间0-100，默认值为40，如果HeapFreeRatio MaxHeapFreeRatio：解释如上，默认值为 70。如果HeapFreeRatio > MaxHeapFreeRatio，则需要进行堆缩容，缩容的时机应该在每次垃圾回收之后。 MaxHeapSize：JVM堆空间允许的最大值 NewSize：Java新生代堆空间的默认值 MaxNewSize：Java新生代堆空间允许的最大值 OldSize：Java老年代堆空间的默认值 NewRatio：新生代（2个survivor区和Eden区）与老年代（不包括永久区）的堆空间比值。值为2表示新生代：老年代=1:2 SurvivorRatio：两个survivor区和eden区的堆空间比值为8，表示S0:S1:Eden = 1:1:8 MetaspaceSize：JVM元空间默认值 CompressedClassSpaceSize/MaxMetaspaceSize：JVM元空间允许的最大值 G1HeapRegionSize：使用G1垃圾回收算法时，JVM将Heap空间分割若干个Region,该参数指定每个Region空间大小 2.2 Heap Usage: PS Young Generation 新生代情况 Eden区：capacity伊甸区容量，used使用容量，free空闲容量，5.076225131165748% used使用比例 From Space:survivor1区，参考eden区 To Space:survivor2区，参考eden区 PS Old Generation：老年代使用情况 老年代参考新生代eden区说明 1.1.3. 3. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量. 示例： jmap -histo:live 17100 打印信息部分如下： num #instances #bytes class name ---------------------------------------------- 1: 87032 9908272 [C 2: 14488 2776152 [I 3: 19405 2209944 [Ljava.lang.Object; 4: 86495 2075880 java.lang.String 5: 23264 2047232 java.lang.reflect.Method 6: 12563 1400192 java.lang.Class 7: 42188 1350016 java.util.concurrent.ConcurrentHashMap$Node 8: 6467 1089560 [B 9: 6806 510600 [Ljava.util.HashMap$Node; 10: 15589 498848 java.util.HashMap$Node 11: 12411 496440 java.util.LinkedHashMap$Entry 12: 8327 466312 java.util.LinkedHashMap 13: 8346 400608 org.aspectj.weaver.reflect.ShadowMatchImpl 14: 810 374336 [J 15: 316 362224 [Ljava.util.concurrent.ConcurrentHashMap$Node; 16: 546 358176 io.netty.util.internal.shaded.org.jctools.queues.MpscArrayQueue 17: 13957 316720 [Ljava.lang.Class; 18: 18467 295472 java.lang.Object 19: 8346 267072 org.aspectj.weaver.patterns.ExposedState 20: 10817 259608 java.util.ArrayList 21: 3748 160800 [Ljava.lang.String; 22: 4669 149408 java.lang.ref.WeakReference 23: 1517 145632 org.springframework.beans.GenericTypeAwarePropertyDescriptor 24: 5889 141336 org.springframework.core.MethodClassKey 25: 2925 140400 java.util.HashMap 26: 2890 138720 org.springframework.core.ResolvableType 27: 3332 133280 java.lang.ref.SoftReference 28: 2067 132288 org.springframework.core.annotation.TypeMappedAnnotation 29: 1502 120160 java.lang.reflect.Constructor 30: 3564 114048 java.util.LinkedList 31: 3272 104704 java.util.Hashtable$Entry 32: 2514 100560 java.util.WeakHashMap$Entry 33: 4074 97776 sun.reflect.generics.tree.SimpleClassTypeSignature 34: 3815 91560 java.beans.MethodRef 35: 1313 84032 io.netty.buffer.PoolSubpage 36: 1499 83944 java.lang.invoke.MemberName ······ instances:实例数量 bytes:字节大小 class name:类名 1.1.4. 4. -clstats打印classload和jvm heap持久层的信息，包括每个classloader名字，活泼型，地址，父classloader和加载的class数量 示例： jmap -clstats 17100 打印部分信息如下： Attaching to process ID 17100, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.202-b08 finding class loader instances ..done. computing per loader stat ..done. please wait.. computing liveness.liveness analysis may be inaccurate ... class_loader classes bytes parent_loader alive? type 3284 5999233 null live 0x00000006c3d21dd0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d221d0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41a6c18 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c395c390 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c395d190 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c2f004f8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d1c7d8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c40af400 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c39c3390 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d4b5d8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c43e3e30 1 880 null dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c2f008e0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d207c0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c40af018 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41a8408 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d4b9c0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d4bdc0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c395cf80 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3bd4fa8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3bd5da8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c241ac58 7119 12028309 0x00000006c241acb8 dead sun/misc/Launcher$AppClassLoader@0x00000007c000f8d8 0x00000006c3d1e3c8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d233c8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41a7000 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d4a7c8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d1c3f0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41a7c38 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d4b1f0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d22bf8 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41a6830 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41b5430 1 1474 null dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3fdb5d0 1 1476 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c4151038 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c2f01ec0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c3d22fe0 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 0x00000006c41a7a28 1 880 0x00000006c241ac58 dead sun/reflect/DelegatingClassLoader@0x00000007c000a028 ···· total = 308 10750 18404333 N/A alive=1, dead=307 N/A 1.1.5. 5. 将内存详细使用情况打印文件 jmap -dump:format=b,file=m.dat pid 然后可以使用jhat命令发布到本地5000端口上 jhat -port 5000 m.dat Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/深入理解JVM-jdk1.7/OutOfMemoryError异常.html":{"url":"Java-技能/JVM/深入理解JVM-jdk1.7/OutOfMemoryError异常.html","title":"OutOfMemoryError异常","keywords":"","body":"1.1. OutOfMemoryError异常1.1.1. Java堆溢出1.1.2. 虚拟机栈和本地方法栈溢出1.1.3. 方法区和运行时常量池内存溢出1.1.4. 本地直接内存溢出1.1. OutOfMemoryError异常 注：本人测试基于jdk1.8测试，有部分不同但是原理可以了解，感兴趣可以下载jdk1.7配套测试 Java虚拟机规范中描述，除了程序计数器，虚拟机的其他几个运行时区域都有发生OOM异常的可能。 下面的示例代码都基于HotSpot虚拟机运行，设置VM参数可以在IDE的VM options内设置,如图 1.1.1. Java堆溢出 引发思路：Java堆用于存储对象实例，只要不断地创建对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量到达最大堆的容量限制后就会产生内存溢出异常。 - 以下代码需要配置VM，设置java堆大小20MB,不可扩展（将堆的最小值-Xms参数与最大值-Xmx参数设置为一样即可避免堆自动扩展），-XX：+HeapDumpOnOutOfMemoryError可以让虚拟机在出现内存溢出异常时Dump出当前的内存堆转储快照以便事后进行分析 -Xmx：最大堆大小 -Xmx20M -Xms20M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=E:\\jvmlog\\oom.hprof ```z 可以指定hprof文件存放位置，之后使用jdk自带工具jvisualvm.exe打开分析即可 ```java import java.util.ArrayList; import java.util.List; public class HeapOOM { static class OOMObject { } public static void main( String[] args) { List list = new ArrayList(); while (true) { list.add(new OOMObject()); } } } 运行结果： java.lang.OutOfMemoryError: Java heap space Dumping heap to java_pid12092.hprof ... Heap dump file created [28256955 bytes in 0.096 secs] Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3210) at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:267) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:241) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:233) at java.util.ArrayList.add(ArrayList.java:464) at com.llh.jdk.map.HeapOOM.main(HeapOOM.java:14) Java堆内存的OOM异常是实际应用中常见的内存溢出异常情况。当出现Java堆内存溢出时，异常堆栈信息“java.lang.OutOfMemoryError”会跟着进一步提示“Java heap space”。 如果是内存泄露，可进一步通过工具查看泄露对象到GC Roots的引用链。于是就能找到泄露对象是通过怎样的路径与GC Roots相关联并导致垃圾收集器无法自动回收它们的。掌握了泄露对象的类型信息及GC Roots引用链的信息，就可以比较准确地定位出泄露代码的位置。 - 如果不存在泄露，换句话说，就是内存中的对象确实都还必须存活着，那就应当检查虚拟机的堆参数（-Xmx与-Xms），与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 1.1.2. 虚拟机栈和本地方法栈溢出 在java虚拟机栈中描述了两种异常： 如果线程请求的栈深度大于虚拟机所允许的最大深度，将抛出StackOverflowError异常 如果虚拟机在扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常 定义大量的本地变量，增大此方法栈中本地变量表的长度，设置-Xss参数减少栈内存容量 -Xss20M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=E:\\jvmlog\\sof.hprof public class JavaVMStackSOF { private int stackLength = 1; public void stackLeak() { stackLength++; stackLeak(); } public static void main(String[] args) throws Throwable { JavaVMStackSOF oom = new JavaVMStackSOF(); try { oom.stackLeak(); } catch (Throwable e) { System.out.println(\"stack length：\" + oom.stackLength); throw e; } } } 运行结果 stack length：1271382 Exception in thread \"main\" java.lang.StackOverflowError at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) at com.llh.jdk.map.JavaVMStackSOF.stackLeak(JavaVMStackSOF.java:8) ··· 实验结果： 单线程下，无论是由于栈帧太大还是虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出的都是StackOverflowError异常。 3.如果不限于单线程，通过不断建立线程的方式可以产生内存溢出异常，这样产生的内存溢出异常与栈空间是否足够大并不存在任何联系，在这种情况下，为每个线程的栈分配的内存越大，反而越容易产生内存溢出异常。 原因：操作系统分配给每个进程的内存是有限制的，虚拟机提供参数来控制Java堆和方法区的这两部分内存的最大值。剩余的内存为操作系统限制减去Xmx(最大堆容量)，再减去MaxPermSize(最大方法区容量) 。如果虚拟机进程本身耗费的内存不计算在内，剩下的内存由虚拟机栈和本地方法栈瓜分。每个线程分配到的栈容量越大，可以建立的线程数量越少，建立线程时越容易把剩下的内存耗尽 解决：如果是建立多线程导致内存溢出，在不能减少线程数或者更换虚拟机的情况下，通过减少堆的最大堆和减少栈容量来换取更多的线程。 创建线程导致内存溢出 -Xss20M public class JavaVMStackOOM { private void dontStop() { while (true) { } } public void stackLeakByThread() { while (true) { Thread thread = new Thread(this::dontStop); thread.start(); } } public static void main(String[] args) throws Throwable { JavaVMStackOOM oom = new JavaVMStackOOM(); oom.stackLeakByThread(); } } 运行结果 Exception in thread\"main\"java.lang.OutOfMemoryError：unable to create new native thread 注意：在windows上，Java线程是映射到操作系统的内核线程上的，执行此代码会导致操作系统假死。 1.1.3. 方法区和运行时常量池内存溢出 String.intern（）是一个Native方法 作用：如果字符串常量池中已经包含一个等于此String对象的字符串，则返回代表池中这个字符串的String对象； 否则，将此String对象包含的字符串添加到常量池中，并且返回此String对象的引用。 运行时常量池导致的内存溢出(因为笔者使用的jdk1.8，所以设置元空间来测试常量池内存溢出情况) -XX:MetaspaceSize=10M -XX:MaxMetaspaceSize=10M import java.util.ArrayList; import java.util.List; public class RuntimeConstantPoolOOM { public static void main(String[] args) { //使用List保持着常量池引用，避免Full GC回收常量池行为 List list = new ArrayList(); //10MB的PermSize在integer范围内足够产生OOM了 int i = 0; while (true) { list.add(String.valueOf(i++).intern()); } } } 在jdk1.7下会一直运行下去，在看一段代码测试String.intern()方法 import java.util.ArrayList; import java.util.List; public class RuntimeConstantPoolOOM { public static void main(String[] args) { String str1 = new StringBuilder(\"计算机\").append(\"软件\").toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder(\"ja\").append(\"va\").toString(); System.out.println(str2.intern() == str2); } } 在1.7与1.8版本的jdk中，这个代码执行会得到一个true,一个false,jdk1.7的intern() 方法会在常量池中记录首先出现的实例引用，因此intern（）返回的引用和由StringBuilder创建的那个字符串实例是同一个。对str2比较返回false是因为“java”这个字符串在执行StringBuilder.toString（）之前已经出现过，字符串常量池中已经有它的引用了，不符合“首次出现”的原则，而“计算机软件”这个字符串则是首次出现的，因此返回true。 方法区用于存放Class的相关信息，如类名、访问修饰符、常量池、字段描述、方法描述等，这个区域的测试思路是：运行时产生大量的类去填满方法区，直到溢出 笔者采用CGLIB直接操作字节码运行时产生大量的动态类。很多主流框架，如Spring、Hibernate，在对类进行增强时，都会使用到CGLib这类字节码技术，增强的类越多，就需要越大的方法区来保证动态生成的Class可以加载入内存。 -XX:MetaspaceSize=10M -XX:MaxMetaspaceSize=10M -XX:+PrintGCDetails import org.springframework.cglib.proxy.Enhancer; import org.springframework.cglib.proxy.MethodInterceptor; public class JavaMethodAreaOOM { public static void main( String[] args) { while (true) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback((MethodInterceptor) (obj, method, args1, proxy) -> proxy.invokeSuper(obj, args1)); enhancer.create(); } } static class OOMObject { } } 运行结果 Exception in thread \"main\" org.springframework.cglib.core.CodeGenerationException: java.lang.OutOfMemoryError-->Metaspace at org.springframework.cglib.core.ReflectUtils.defineClass(ReflectUtils.java:538) at org.springframework.cglib.core.AbstractClassGenerator.generate(AbstractClassGenerator.java:363) at org.springframework.cglib.proxy.Enhancer.generate(Enhancer.java:585) at org.springframework.cglib.core.AbstractClassGenerator$ClassLoaderData.get(AbstractClassGenerator.java:131) at org.springframework.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:319) at org.springframework.cglib.proxy.Enhancer.createHelper(Enhancer.java:572) at org.springframework.cglib.proxy.Enhancer.create(Enhancer.java:387) at com.llh.jdk.map.JavaMethodAreaOOM.main(JavaMethodAreaOOM.java:14) - 方法区溢出也是一种常见的内存溢出异常，一个类要被垃圾收集器回收掉，判定条件是比较苛刻的。在经常动态生成大量Class的应用中，需要特别注意类的回收状况。这类场景除了上面提到的程序使用了CGLib字节码增强和动态语言之外，常见的还有：大量JSP或动态产生JSP文件的应用（JSP第一次运行时需要编译为Java类）、基于OSGi的应用（即使是同一个类文件，被不同的加载器加载也会视为不同的类）等。 1.1.4. 本地直接内存溢出 DirectMemory容量可通过-XX：MaxDirectMemorySize指定，如果不指定，则默认与Java堆最大值（-Xmx指定）一样 使用unsafe分配本机内存 -XX:MaxDirectMemorySize=50M -XX:+PrintGCDetails import sun.misc.Unsafe; import java.lang.reflect.Field; public class DirectMemoryOOM { private static final int _1MB = 1024 * 1024; public static void main(String[] args) throws Exception { Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) { unsafe.allocateMemory(_1MB); } } } 运行结果 Exception in thread \"main\" java.lang.OutOfMemoryError at sun.misc.Unsafe.allocateMemory(Native Method) at com.llh.jdk.map.DirectMemoryOOM.main(DirectMemoryOOM.java:15) Heap PSYoungGen total 75264K, used 5161K [0x000000076ca00000, 0x0000000771e00000, 0x00000007c0000000) eden space 64512K, 8% used [0x000000076ca00000,0x000000076cf0a638,0x0000000770900000) from space 10752K, 0% used [0x0000000771380000,0x0000000771380000,0x0000000771e00000) to space 10752K, 0% used [0x0000000770900000,0x0000000770900000,0x0000000771380000) ParOldGen total 172032K, used 0K [0x00000006c5e00000, 0x00000006d0600000, 0x000000076ca00000) object space 172032K, 0% used [0x00000006c5e00000,0x00000006c5e00000,0x00000006d0600000) Metaspace used 3348K, capacity 4496K, committed 4864K, reserved 1056768K class space used 366K, capacity 388K, committed 512K, reserved 1048576K 由DirectMemory导致的内存溢出，一个明显的特征是在Heap Dump文件中不会看见明显的异常，如果读者发现OOM之后Dump文件很小，而程序中又直接或间接使用了NIO，那就可以考虑检查一下是不是这方面的原因。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/深入理解JVM-jdk1.7/垃圾收集器与内存分配策略.html":{"url":"Java-技能/JVM/深入理解JVM-jdk1.7/垃圾收集器与内存分配策略.html","title":"垃圾收集器与内存分配策略","keywords":"","body":"1.1. 垃圾收集器与内存分配策略1.1.1. 概述1.1.2. 对象已死吗1.1.3. 垃圾收集算法1.1.4. HotSpot的算法实现1.1. 垃圾收集器与内存分配策略 Java与C++之间有一堵由内存动态分配和垃圾收集技术所围成的“高墙”，墙外面的人想进去，墙里面的人却想出来 1.1.1. 概述 垃圾收集（Garbage Collection,GC） 垃圾收集需要解决的问题 哪些内存需要回收？ 什么时候回收？ 如何回收？ 为什么需要了解垃圾回收 需要排查各种内存溢出，泄露时，当垃圾收集成为系统达到更高并发量的瓶颈时，我们就需要堆垃圾收集以及内存分配实施必要的监控和调节 - 之前了解到了Java内存运行时区域的各个部分，其中程序计数器、虚拟机栈、本地方法栈3个区域是随线程而生，随线程而灭；栈中的栈帧随着方法的进入和退出执行者出栈和入栈的操作。每一个栈帧分配多少内存基本上在类结构确定后便已知，因此这几个区域的内存分配和回收都具备确定性，在这几个区域内就不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。 而Java堆和方法区不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间时才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器关注的就是这部分内存。 1.1.2. 对象已死吗 堆里存放着java世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前，第一件事要确定的就是哪些对象是可以被回收的（不可能再被任何途径使用的对象）。 判断对象是否存活的算法： 引用计数算法 可达性分析算法 引用计数算法 算法实现：给对象中添加一个引用计数器，每当由一个地方引用的时候，计数器值就加1；当引用失效时，计数器值减1；任何时刻计数器值为0的对象就是不可能再被使用的。 java不用此算法原因：很难解决对象之间相互循环引用的问题 如下例子，testGC() 方法，对象objA和objB都有字段instance，赋值令objA.instance=objB及objB.instance=objA，除此之外，这两个对象再无任何引用，实际上这两个对象已经不可能再被访问，但是它们因为互相引用着对方，导致它们的引用计数都不为0，于是引用计数算法无法通知GC收集器回收它们 -XX:+PrintGCDetails public class ReferenceCountingGC { public Object instance = null; private static final int _1MB = 1024 * 1024; /** * 这个成员属性的唯一意义就是占点内存，以便能在GC日志中看清楚是否被回收过 */ private byte[] bigSize = new byte[2 * _1MB]; public static void testGC() { ReferenceCountingGC objA = new ReferenceCountingGC(); ReferenceCountingGC objB = new ReferenceCountingGC(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; //假设在这行发生GC,objA和objB是否能被回收？ System.gc(); } public static void main(String[] args) { testGC(); } } 运行结果 [GC (System.gc()) [PSYoungGen: 7966K->824K(75264K)] 7966K->832K(247296K), 0.0009400 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 824K->0K(75264K)] [ParOldGen: 8K->688K(172032K)] 832K->688K(247296K), [Metaspace: 3111K->3111K(1056768K)], 0.0047815 secs] [Times: user=0.09 sys=0.00, real=0.01 secs] Heap PSYoungGen total 75264K, used 1935K [0x000000076ca00000, 0x0000000771e00000, 0x00000007c0000000) eden space 64512K, 3% used [0x000000076ca00000,0x000000076cbe3fb8,0x0000000770900000) from space 10752K, 0% used [0x0000000770900000,0x0000000770900000,0x0000000771380000) to space 10752K, 0% used [0x0000000771380000,0x0000000771380000,0x0000000771e00000) ParOldGen total 172032K, used 688K [0x00000006c5e00000, 0x00000006d0600000, 0x000000076ca00000) object space 172032K, 0% used [0x00000006c5e00000,0x00000006c5eac260,0x00000006d0600000) Metaspace used 3138K, capacity 4496K, committed 4864K, reserved 1056768K class space used 342K, capacity 388K, committed 512K, reserved 1048576K Process finished with exit code 0 运行结果中可以清楚看到，GC日志中包含“7966K->824K(75264K)”，意味着虚拟机并没有因为这两个对象互相引用就不回收它们，这也从侧面说明虚拟机并不是通过引用计数算法来判断对象是否存活的 可达性分析算法 在主流的商用程序语言（Java、C#，甚至包括前面提到的古老的Lisp）的主流实现中，都是称通过可达性分析（Reachability Analysis）来判定对象是否存活的 算法思路：通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。 如图，object5、object6、object7虽有关联，但是它们到GC roots时不可达的，所以被判定是可回收的对象 Java语言中，可作为GC Roots的对象包括： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI(Native方法)引用的对象 再谈引用 Java对引用的概念分为强引用（Strong Reference），软引用（Soft Reference），弱引用（Weak Reference），虚引用（Phantom Reference）4种。 SoftReference，WeakReference，PhantomReference 强引用在程序代码中普遍存在的，类似“Object obj=new Object()”这类的引用，只要强引用还在，垃圾收集器永远不会回收掉被引用的对象 软引用描述一些有用但是非必需的对象，软引用关联的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。 弱引用也是用来描述非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 生存还是死亡 即使在可达性分析算法中不可达的对象，要真正宣告一个对象死亡，至少要经历两次标记过程：如果对象在进行可达性分析后发现没有与GC Roots相连接的引用链，那它将会被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行finalize（）方法，当对象没有覆盖finalize（）方法，或者finalize（）方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”。 如果这个对象被判定有必要执行finalize()方法，那这个对象将会放置在一个F-Queue的队列中，并在稍后由一个由虚拟机自动建立的、低优先级的Finalizer线程去执行它。 - finalize（）方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize（）中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。 一次对象自我拯救演示 public class FinalizeEscapeGC { public static FinalizeEscapeGC SAVE_HOOK = null; public void isAlive() { System.out.println(\"yes,i am still alive：)\"); } @Override protected void finalize() throws Throwable { super.finalize(); System.out.println(\"finalize mehtod executed!\"); FinalizeEscapeGC.SAVE_HOOK = this; } public static void main(String[] args) throws Throwable { SAVE_HOOK = new FinalizeEscapeGC(); //对象第一次成功拯救自己 SAVE_HOOK = null; System.gc(); //因为finalize方法优先级很低，所以暂停0.5秒以等待它 Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\"no,i am dead：(\"); } //下面这段代码与上面的完全相同，但是这次自救却失败了 SAVE_HOOK = null; System.gc(); //因为finalize方法优先级很低，所以暂停0.5秒以等待它 Thread.sleep(500); if (SAVE_HOOK != null) { SAVE_HOOK.isAlive(); } else { System.out.println(\"no,i am dead：(\"); } } } 运行结果： \"C:\\Program Files\\Java\\jdk1.8.0_261\\bin\\java.exe\" \"-javaagent:E:\\IntelliJ IDEA 2020.1.3\\lib\\idea_rt.jar=52343:E:\\IntelliJ IDEA 2020.1.3\\bin\" -Dfile.encoding=UTF-8 -classpath \"C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\charsets.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\deploy.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\access-bridge-64.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\cldrdata.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\dnsns.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\jaccess.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\jfxrt.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\localedata.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\nashorn.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\sunec.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\sunjce_provider.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\sunmscapi.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\sunpkcs11.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\ext\\zipfs.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\javaws.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\jce.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\jfr.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\jfxswt.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\jsse.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\management-agent.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\plugin.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\resources.jar;C:\\Program Files\\Java\\jdk1.8.0_261\\jre\\lib\\rt.jar;F:\\data\\play-jdk\\target\\classes;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter\\2.4.0\\spring-boot-starter-2.4.0.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\boot\\spring-boot\\2.4.0\\spring-boot-2.4.0.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\spring-context\\5.3.1\\spring-context-5.3.1.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\spring-aop\\5.3.1\\spring-aop-5.3.1.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\spring-beans\\5.3.1\\spring-beans-5.3.1.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\spring-expression\\5.3.1\\spring-expression-5.3.1.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\boot\\spring-boot-autoconfigure\\2.4.0\\spring-boot-autoconfigure-2.4.0.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-logging\\2.4.0\\spring-boot-starter-logging-2.4.0.jar;C:\\Users\\Administrator\\.m2\\repository\\ch\\qos\\logback\\logback-classic\\1.2.3\\logback-classic-1.2.3.jar;C:\\Users\\Administrator\\.m2\\repository\\ch\\qos\\logback\\logback-core\\1.2.3\\logback-core-1.2.3.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\apache\\logging\\log4j\\log4j-to-slf4j\\2.13.3\\log4j-to-slf4j-2.13.3.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\apache\\logging\\log4j\\log4j-api\\2.13.3\\log4j-api-2.13.3.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\slf4j\\jul-to-slf4j\\1.7.30\\jul-to-slf4j-1.7.30.jar;C:\\Users\\Administrator\\.m2\\repository\\jakarta\\annotation\\jakarta.annotation-api\\1.3.5\\jakarta.annotation-api-1.3.5.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\spring-core\\5.3.1\\spring-core-5.3.1.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\springframework\\spring-jcl\\5.3.1\\spring-jcl-5.3.1.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\yaml\\snakeyaml\\1.27\\snakeyaml-1.27.jar;C:\\Users\\Administrator\\.m2\\repository\\org\\slf4j\\slf4j-api\\1.7.30\\slf4j-api-1.7.30.jar\" com.llh.jdk.map.FinalizeEscapeGC finalize mehtod executed! yes,i am still alive：) no,i am dead：( Process finished with exit code 0 代码理解：将对象赋值为空并且调用gc的时候，线程等待由虚拟机自动建立的、低优先级的finalizer线程去执行，执行时找到finalize() 方法，对象成功拯救自己，所以对象不为空；再次gc回收，finalize（）方法已经被虚拟机调用过，所以判定为方法不会再次执行，已被彻底回收。 - 代码中有两段完全一样的代码片段，执行结果却是一次逃脱成功，一次失败，这是因为任何一个对象的finalize（）方法都只会被系统自动调用一次，如果对象面临下一次回收，它的finalize（）方法不会被再次执行，因此第二段代码的自救行动失败了。 回收方法区 Java虚拟机规范中可以不要求虚拟机在方法区实现垃圾收集，而且在方法区内进行垃圾收集的“性价比”一般比较低：在堆中，尤其是在新生代中，常规应用进行一次垃圾收集一般可以回收70%～95%的空间，而永久代的垃圾收集效率远低于此 永久代的垃圾收集主要回收两部分内容： - 废弃常量：以常量池中字面量的回收为例，假如一个字符串“abc”已经进入了常量池中，但是当前系统没有任何一个String对象是叫做“abc”的，换句话说，就是没有任何String对象引用常量池中的“abc”常量，也没有其他地方引用了这个字面量，如果这时发生内存回收，而且必要的话，这个“abc”常量就会被系统清理出常量池 无用的类： 该类所有的实例都已经被回收，也就是Java堆中不存在该类的任何实例 加载该类的ClassLoader已经被回收 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法 - 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样，不使用了就必然会回收。是否对类进行回收，HotSpot虚拟机提供了-Xnoclassgc参数进行控制，还可以使用-verbose：class以及-XX：+TraceClassLoading、-XX：+TraceClassUnLoading查看类加载和卸载信息，其中-verbose：class和-XX：+TraceClassLoading可以在Product版的虚拟机中使用，-XX：+TraceClassUnLoading参数需要FastDebug版的虚拟机支持。 1.1.3. 垃圾收集算法 标记-清除算法 最基础的收集算法是“标记-清除”（Mark-Sweep）算法。 首先标记出所有需要回收的对象，标记完成后统一回收所有被标记的对象。 标记-清除算法主要由两个不足： 一个是效率问题，标记和清除两个过程的效率都不高； 另一个是空间问题，标记清除后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前出发一次垃圾收集的动作。 复制算法 为了解决效率问题，一种称为“复制”（Copying）的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为了原来的一半，未免太高了一点。 新生代中的对象98%是“朝生夕死”的，所以并不需要按照1：1的比例来划分内存空间，而是将内存分为一块比较大的Eden空间和两块较小的Survivor空间,每次使用Eden和其中一块Survivor。当发生回收时，将Eden和Survivor中还存活着的对象一次性复制到另一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。 HotSpot虚拟机默认Eden和Survivor的大小比例是8：1，也就是每次新生代中可用内存空间为整个新生代容量的90%（80%+10%）。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 内存的分配担保就好比我们去银行借款，如果我们信誉很好，在98%的情况下都能按时偿还，于是银行可能会默认我们下一次也能按时按量地偿还贷款，只需要有一个担保人能保证如果我不能还款时，可以从他的账户扣钱，那银行就认为没有风险了。内存的分配担保也一样，如果另外一块Survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。 标记-整理算法 复制收集算法在对象存活率较高时就要进行较多的复制操作，效率就会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法 当前商业虚拟机的垃圾收集都采用‘分代收集’（Generational Collection）算法。 分代收集算法根据对象存活周期的不同将内存划分为几块。 把Java堆分为新生代和老年代 新生代中，每次垃圾收集时都有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。 老年代中，因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。 1.1.4. HotSpot的算法实现 上面介绍了对象存活判定算法和垃圾收集算法，而在HotSpot虚拟机上实现这些算法时，必须对算法的执行效率有严格的考量，才能保证虚拟机高效运行。 枚举根节点 可达性分析中从GC Roots节点找引用链这个操作为例，可以作为GC Roots的节点主要在全局性的引用（常量或静态属性）与执行上下文（栈帧中的本地变量表）中，很多应用在方法区就有数百兆，逐个检查这个里面的引用，会消耗很多时间 - 可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行----这里“一致性”的意思是指在整个分析期间整个执行系统看起来就像被冻结在某个时间点上，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致GC进行时必须停顿所有Java执行线程（Sun将这件事情称为“Stop The World”）的其中一个重要原因，即使是在号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 目前主流的Java虚拟机都是准确式GC,所以当执行系统停顿下来后，并不需要一个不漏地检查完所有执行上下文和全局的引用位置，虚拟机应当是有办法直接得知哪些地方存放着对象引用 - HotSpot的实现中，是使用一组OopMap的数据结构来达到这个目的的,在类加载完成的时候，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在特定的位置记录下栈和寄存器中哪些位置是引用。这样，GC在扫描时就可以直接得知这些信息了。 安全点（Safepoint） HotSpot没有为每条指令都生成OopMap,只是在特定的位置记录了这些信息，这些位置称为“安全点”，即程序执行时并非在所有地方都能停顿下来开始GC，只有在到达安全点时才能暂停。 safepoint的选定既不能太少以致于让GC等待时间太长，也不能过于频繁以致于过分增大运行时的负荷。如方法调用、循环跳转、异常跳转，所以具有这些功能的指令才会产生SafePoint 如何在GC发生时让所有线程（这里不包括执行JNI调用的线程）都“跑”到最近的安全点上再停顿下来。 抢先式中断（Preemptive Suspension），不需要线程的执行代码主动去配合，GC发生时，首先把所有的线程全部中断，如果发现有中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。现在几乎没有虚拟机实现采用抢先式中断来暂停线程从而响应GC事件 主动式中断（Voluntary Suspension）当GC需要中断线程的时候，不直接对线程操作，仅仅简单地设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起，轮询标志的地方和安全点是重合的，另外再加上创建对象需要分配内存的地方 安全区域（Safe Region） Safepoint机制保证了程序执行时，在不太长的时间内就会遇到可进入GC的Safepoint， Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/JVM/深入理解JVM-jdk1.7/自动内存管理机制.html":{"url":"Java-技能/JVM/深入理解JVM-jdk1.7/自动内存管理机制.html","title":"自动内存管理机制","keywords":"","body":"1.1. Java内存区域与内存溢出1.1.1. 运行时数据区1.1. Java内存区域与内存溢出 Java与C++之间有一堵由内存动态分配和垃圾收集技术所围成的“高墙”，墙外面的人想 进去，墙里面的人却想出来 1.1.1. 运行时数据区 所有线程共享的数据区：方法区、堆 线程隔离的数据区：虚拟机栈、本地方法栈、程序计数器 程序计数器(Program Counter Register) 线程私有 一块内存较小的内存空间 字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复etc基础功能都需要依赖这个计数器 - Java虚拟机的多线程：通过线程轮流切换并分配处理器执行时间的方式来实现，任何一个确定时刻，一个处理器都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域 Java虚拟机栈 线程私有 每个方法在执行的同时都会创建一个栈帧（stack frame）用于存储局部变量表（栈内存）、操作数栈、动态链接，方法出口等信息。 每个方法从调用直至执行完成的过程，对应着一个栈帧在虚拟机栈中入栈到出栈的过程 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，可能是一个指向对象起始地址的指针，也可能是指向一个代表对象的句柄）和returnAddress类型（指向了一条字节码指令的地址） 64位的long和double占用2各局部变量空间。方法运行期间不会改变局部变量表的大小 这个区域有两种异常状况：线程请求栈深度大于虚拟机所允许的深度，抛出StackOverflowError异常；如果虚拟机栈可以动态扩展，扩展时无法申请到足够的内存，则会抛出OutOfMemoryError异常 总结：栈帧： 局部变量表 各种基本数据类型 对象引用 returnAddress类型 操作数栈 动态链接 方法出口etc 本地方法栈（Native Method Stack） 线程私有 与虚拟机栈的区别：虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的Native方法服务 Sun HotSpot虚拟机将本地方法栈与虚拟机栈合二为一 与虚拟机栈相同，也会抛出两种异常 Java堆（Java Heap） 大部分应用，堆是java虚拟机所管理的内存中最大的一块。 被所有线程共享的一块内存区域 虚拟机启动时创建 存放对象实例，几乎所有的对象实例都在这里分配内存。Java虚拟机规范中描述：所有的对象实例以及数组都要在堆上分配，但是随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换[2] 优化技术将会导致一些微妙的变化发生，所有的对象都分配在堆上也渐渐变得不是那么“绝对”。 垃圾收集器管理的主要区域，也被称为GC堆（Garbage Collected Heap） 从内存回收角度看，现在收集器基本都采用分代收集算法，所以Java堆还可以细分：新生代、老年代；再细致的有Eden空间、From Survivor空间、To Survivor空间。 从内存分配的角度看，线程共享的堆中能划分多个线程私有的分配缓冲区（Thread Local Allocation Buffer,TLAB），划分的目的是为了更好的回收内存，或者更快的分配内存。 堆可以处于物理上不连续的内存空间中。既可以实现城固定大小的，也可以是可扩展的（通过-Xmx和-Xms控制）。 堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常 方法区（Method Area） 所有线程共享的一块区域 存储被虚拟机加载的类信息、常量、静态b变量、即时编译器编译后的代码等 1.7的虚拟机规范将方法区描述为堆的一个逻辑部分，但它却是非堆（non-heap） 在hotspot虚拟机上开发者来说，方法区也被称为永久代，但是两者并不等价，是因为hotspot的设计团队把GC的分代收集扩展到了方法区。其他虚拟机（如BEA JRockit、IBM J9等）不存在永久代的概念 方法区的GC回收也可以不实现 方法区（永久代的概念在1.8被取消，元空间来代替） 运行时常量池（Runtime Constant Pool） 是方法区的一部分 存放编译器生成的各种字面量和符号引用，这部分内容在类加载后进入方法区的运行时常量池 运行时常量池相对于class文件常量池的一个重要特征就是具备动态性，常量并非一定只有编译期才能产生，运行期间也可能将新的常量放入池中，比如string的intern()方法 常量池无法申请到内存的时候会抛出OutOfMemoryError异常 直接内存（Direct Memory） 不是虚拟机运行时数据区的一部分 1.4时候新加入NIO（New input/output）类，引入基于通道channel与缓冲区buffer的I/O方式，可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作 - 本机直接内存分配不会受到堆大小的限制，但是会受到本机内存大小以及处理器寻址空间的限制。在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，当忽略直接内存时，使得各个区域总和大于物理内存限制，导致动态扩展时出现OutOfMemoryError异常 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/AQS.html":{"url":"Java-技能/多线程/AQS.html","title":"AQS","keywords":"","body":"1. AQS1.1. 基础概览1.1.1. 基础属性1. AQS AbstractQueuedSynchronizer:抽象队列同步器，定义了多线程访问共享资源的同步器框架 1.1. 基础概览 包名：java.util.concurrent.locks，可见属于juc多线程并发包，锁下 用到AQS实现的：ReentrantLock/Semaphore/CountDownLatch 1.1.1. 基础属性 属性代码块： /** * Head of the wait queue, lazily initialized. Except for * initialization, it is modified only via method setHead. Note: * If head exists, its waitStatus is guaranteed not to be * CANCELLED. */ private transient volatile Node head; /** * Tail of the wait queue, lazily initialized. Modified only via * method enq to add new wait node. */ private transient volatile Node tail; /** * The synchronization state. */ private volatile int state; 它维护了一个volatile int state（代表共享资源）和一个FIFO（先进先出）线程等待队列（多线程争用资源被阻塞时会进入此队列） 引用 从ReentrantLock的实现看AQS的原理及应用 Java并发之AQS详解 Java并发编程AQS详解 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/CAS自旋锁.html":{"url":"Java-技能/多线程/CAS自旋锁.html","title":"CAS自旋锁","keywords":"","body":"1.1. CAS自旋锁1.1.1. CAS可能造成的问题1.1.2. 自己实现自旋锁1.2. CAS引发的ABA问题1.1. CAS自旋锁 unsafe类的compareAndSwap()方法,实现比较并交换 1.1.1. CAS可能造成的问题 进入死循环，导致cpu占用过高 引发ABA问题 1.1.2. 自己实现自旋锁 尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁。好处：减少线程上下文切换的消耗，缺点：会耗CPU public class SpinLockDemo { /** * 原子引用线程 */ AtomicReference atomicReference = new AtomicReference(); public void myLock() { Thread thread = Thread.currentThread(); System.out.println(Thread.currentThread().getName() + \"come in\"); while (!atomicReference.compareAndSet(null, thread)) { System.out.println(\"尝试\"); } } public void myUnlock() { Thread thread = Thread.currentThread(); atomicReference.compareAndSet(thread, null); System.out.println(Thread.currentThread().getName() + \"invoked my unlock\"); } public static void main(String[] args) { SpinLockDemo spinLockDemo = new SpinLockDemo(); new Thread(() -> { spinLockDemo.myLock(); try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } spinLockDemo.myUnlock(); }, \"AA\").start(); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } new Thread(() -> { spinLockDemo.myLock(); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } spinLockDemo.myUnlock(); }, \"BB\").start(); } } 1.2. CAS引发的ABA问题 /** * 解决ABA问题（原子引用？版本号？） */ public class ABADemo { /** * 原子引用 */ static AtomicReference atomicReference = new AtomicReference<>(100); static AtomicStampedReference atomicStampedReference = new AtomicStampedReference(100, 1); public static void main(String[] args) { System.out.println(\"=========================产生ABA问题===========================\"); new Thread(() -> { atomicReference.compareAndSet(100, 101); atomicReference.compareAndSet(101, 100); }, \"t1\").start(); new Thread(() -> { //保证t1线程完成一次ABA操作 try { Thread.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(atomicReference.compareAndSet(100, 2019) + \":\" + atomicReference.get()); }, \"t2\").start(); System.out.println(\"=========================解决ABA问题===========================\"); new Thread(() -> { int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + \"初始版本号\" + stamp); try { Thread.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } atomicStampedReference.compareAndSet(100, 101, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"2次版本号\" + atomicStampedReference.getStamp()); atomicStampedReference.compareAndSet(101, 100, atomicStampedReference.getStamp(), atomicStampedReference.getStamp() + 1); System.out.println(Thread.currentThread().getName() + \"3次版本号\" + atomicStampedReference.getStamp()); }, \"t3\").start(); new Thread(() -> { //等待t3拿到相同的版本号 int stamp = atomicStampedReference.getStamp(); System.out.println(Thread.currentThread().getName() + \"初始版本号\" + stamp); try { Thread.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } boolean result = atomicStampedReference.compareAndSet(100, 2019, stamp, stamp + 1); System.out.println(Thread.currentThread().getName() + \":\" + result + \":\" + atomicStampedReference.getStamp()); System.out.println(\"最新值：\" + atomicStampedReference.getReference()); }, \"t4\").start(); } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/Java锁.html":{"url":"Java-技能/多线程/Java锁.html","title":"Java锁","keywords":"","body":"1.1. 锁1.1.1. 公平锁1.1.2. 非公平锁1.1.3. 可重入锁1.1.4. ReentrantLock可重入锁1.1.5. 自旋锁1.1.6. 独占锁1.1.7. 共享锁1.1.8. 读写锁ReadWriteLockDemo1.1. 锁 公平锁/非公平锁/可重入锁(又名递归锁)/自旋锁/独占锁（写锁）/共享锁（读锁）/互斥锁 1.1.1. 公平锁 多个线程按照申请锁的顺序来获取锁，队列先来后到 1.1.2. 非公平锁 多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。 如果尝试失败，再采用类似公平锁的方式 高并发场景，有可能会造成优先级反转或者饥饿现象 1.1.3. 可重入锁 可重入锁又被称为递归锁 同一线程外层函数获得锁之后，内层递归函数仍然能获取该锁的代码，在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁（即：线程可以进入任何一个它已经拥有的锁所同步着的代码块） 典型 ReentrantLock/Synchronized典型的可重入锁 作用 避免死锁 代码 public class ReentrantLockTest { public static void main(String[] args) { ReentrantLockTest reentrantLockTest = new ReentrantLockTest(); new Thread(() -> { reentrantLockTest.b(); }, \"b1\").start(); Thread thread = new Thread(() -> { reentrantLockTest.a(); }, \"a1\"); thread.start(); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(thread.getState()); } synchronized void a() { System.out.println(Thread.currentThread().getName() + \"进入A方法\"); try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } b(); } synchronized void b() { System.out.println(Thread.currentThread().getName() + \"进入B方法\"); try { Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \"退出b方法\"); } } 1.1.4. ReentrantLock可重入锁 初始化ReentrantLock默认非公平锁 /** * Creates an instance of {@code ReentrantLock}. * This is equivalent to using {@code ReentrantLock(false)}. */ public ReentrantLock(){ sync=new NonfairSync(); } 公平锁（有参构造方法，可以指定是否为公平锁）:多个线程按照申请锁的顺序来获取锁，队列先来后到；非公平锁：抢锁的线程默认非公平 /** * Creates an instance of {@code ReentrantLock} with the * given fairness policy. * * @param fair {@code true} if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair){ sync=fair?new FairSync():new NonfairSync(); } java除了使用关键字synchronized外，还可以使用ReentrantLock实现独占锁的功能。而且ReentrantLock相比synchronized而言功能更加丰富，使用起来更为灵活，也更适合复杂的并发场景。这篇文章主要是从使用的角度来分析一下ReentrantLock 1.1.5. 自旋锁 尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁。好处：减少线程上下文切换的消耗，缺点：会耗CPU 1.1.6. 独占锁 指该锁一次只能被一个线程持有，对synchronized/reentrantLock而言都是独占锁 1.1.7. 共享锁 可以被多个线程所持有 1.1.8. 读写锁ReadWriteLockDemo Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/JMM内存模型Volatile关键字.html":{"url":"Java-技能/多线程/JMM内存模型Volatile关键字.html","title":"JMM内存模型Volatile关键字","keywords":"","body":"1.1. 并发编程1.2. 并发编程的三个概念1.2.1. 原子性1.2.2. 可见性1.2.3. 有序性1.3. Java内存模型1.3.1. valatile1.1. 并发编程 在并发编程的操作中，程序运行过程中，会将需要运算修改的数据从主内存复制一份到线程中，在线程计算结束会将此数据重新刷入主内存，在并发中，就有可能造成一些问题，比如： i=i+1; 线程执行时，会从主内存中读取i的值，然后复制一份到高速缓存中，然后CPU指令进行+1操作，然后将数据写入到高速缓存，再将高速缓存中i最新的值刷到主内存。 单线程不会有任何问题，但是多线程场景下就会有问题了。如： （缓存一致性问题）假设 初始值为0，那么在两个线程执行完成之后我们期望的结果应该是2，但是，当多线程并发时，线程A和b拿到初始值都为0，A进行+1操作，写回高速缓存，写回主内存；此时线程B中初始值依然为0，也进行+1操作，写回高速缓存，写回主内存，这是得到的最终结果是1，而不是2 1.2. 并发编程的三个概念 在并发编程中，通常会遇到三个问题：原子性、可见性、有序性 1.2.1. 原子性 一个操作或者多个操作，要么全部执行并且执行的过程不会被任何因素打断，要么都不执行 1.2.2. 可见性 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看到修改的值 例子： //线程1执行的代码 int i=0; i=10; //线程2执行的代码 j=i; 假设有A.B两个线程并发操作，A操作执行到i=10这句话时，会将初始值i=0加载到线程A的高速缓存中去做修改赋值10，那么线程A的高速缓存中的值变为了10，但是却没有立即写回到主内存。这时候线程B执行j=i操作，它会先去主内存读取值加载到B的高速缓存中，这时主内存中i的值还是0，那么就会造成j的值为0，而不是10 1.2.3. 有序性 即程序执行的顺序按照代码的先后顺序执行。 JMM优化会在编译后的代码进行指令重排序，指令重排序的意义：使指令更加符合CPU的执行特性，最大限度的发挥机器的性能，提高程序的执行效率。在多线程操作下，指令重排序可能导致程序不能保证有序性。 什么叫指令重排？处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。处理器在进行重排序时是会考虑指令之间的数据依赖性 。 多线程执行指令重排导致异常的例子如下： //线程1: context=loadContext(); //语句1 inited=true; //语句2 //线程2: while(!inited){ sleep() } doSomethingwithconfig(context); 如上流程，有线程1，2并发执行，在编译后可能会对代码进行指令重排，因为线程1内的两行代码并没有数据依赖性，所以可能先执行语句2，这时候线程2在判断时，跳过循环，直接执行doSomethingwithconfig(context) 方法，然而，此时线程1还未初始化context，所以导致程序出错 指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性 1.3. Java内存模型 Java内存模型规定所有的变量都是存在主存当中，每个线程都有自己的工作内存。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。 如图，如果要修改主内存的10这个值，那么T1,T2线程都会先从主内存中将10这个值拷贝回自己的工作内存中去，修改完之后再刷新回主内存 1.3.1. valatile 了解了java的内存模型，那么可以正式来看一下volatile是怎么解决这些问题的吧 volatile关键字的两层语义 一旦一个共享变量（类的成员变量、静态成员变量）被volatile修饰之后，那就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性 禁止进行指令重排序 volatile不能保证原子性 原子性可以使用synchronized来保证，但是会造成执行效率慢的问题 可以使用atomic原子类保证，atomic原子类原理是使用CAS自旋锁（compare and swap）,比较并交换 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/Synchronized锁详解.html":{"url":"Java-技能/多线程/Synchronized锁详解.html","title":"Synchronized锁详解","keywords":"","body":"1.1. synchronized锁1.1.1. synchronized注意点1.1.2. 锁类型1.1.3. synchronized锁升级过程1.1. synchronized锁 1.1.1. synchronized注意点 一把锁只能同时被一个线程获取，没有获得锁的线程只能等待 每个实例对应自己的一把锁(this)，不同实例之间互不影响 synchronized修饰的方法，正常执行完及抛出异常，都会释放锁 1.1.2. 锁类型 锁膨胀方向：无锁-->偏向锁-->轻量级锁-->重量级锁(过程不可逆，但偏向锁可被重置为无锁) 1.1.3. synchronized锁升级过程 无锁-->偏向锁 线程访问锁对象时，会将当前线程id写入锁对象的对象头内，升级为偏向锁，不会主动释放锁，此后，再有线程获取锁会判断与对象头 内的线程id是否相同，一样则继续获得锁 如果不一致，需要判断对象头内记录的线程是否存活，如果没存活，锁对象重置为无锁状态 如果存活，立即查找该线程的栈帧信息，如果还是需要持有，那么暂停当前线程，撤销偏向锁，升级为轻量级锁 轻量级锁原理 线程1获取轻量级锁时会把锁对象的对象头MarkWord复制一份到线程1的栈帧中创建的用于存储锁记录的空间（DisplacedMarkWord）,然后使用CAS把对象头中的内容替换为线程1存储的锁记录的地址； - 如果在复制对象头的同时，线程2也准备获取锁，复制了对象头到线程2的锁记录空间中，但是在线程2CAS的时候，发现线程1已经把对象头换了，线程2的CAS失败，那么线程2就尝试使用自旋锁来等待线程1释放锁。自旋锁简单来说就是让线程2在循环中不断CAS 升级重量锁 - 如果自旋的时间太长也不行，因为自旋是要耗CPU的，因此自旋的次数是有限制的，比如10次或者100次，如果自旋次数达到了线程1还没有释放锁，或者线程1还在执行，线程2还在自旋等待，这时又有一个线程3过来竞争这个锁对象，那么这个时候轻量级锁就会膨胀为重量级锁。重量级锁把除了拥有锁的线程都阻塞，防止CPU空转 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/ThreadLocal详解.html":{"url":"Java-技能/多线程/ThreadLocal详解.html","title":"ThreadLocal详解","keywords":"","body":"1.1. ThreadLocal1.1.1. ThreadLocal内存泄露问题1.1. ThreadLocal 概念：线程局部变量，一个ThreadLocal在一个线程中是共享的，在不同线程之间又是隔离的（每个线程智能看到自己线程的值） 源码解析： set方法：通过Thread.currentThread()方法获取了当前的线程引用，并传给了getMap(Thread) 方法获取一个ThreadLocalMap的实例 每个Thread对象都有一个ThreadLocalMap，当创建一个ThreadLocal的时候，就会将该ThreadLocal对象添加到该Map中，其中键就是ThreadLocal，值可以是任意类型 1.1.1. ThreadLocal内存泄露问题 为什么ThreadLocal会导致内存泄露 前提需要了解 强引用：当JVM内存空间不足时，宁愿程序抛出OOM使程序异常终止也不愿回收具有强引用的存活者的对象 弱引用：在GC的时候，不管内存空间足不足都会回收这个对象 为何会导致内存泄露？ - ThreadLocalMap中的key为ThreadLocal的弱引用：ThreadLocalMap中的Entry对象继承了WeakReference弱引用类，在Entry的构造方法中，会以key作为参数传入到父类的构造方法中 key为弱引用，每次GC则会回收导致key为null，但是value是强引用，垃圾回收不到也无法访问到，则会导致OOM内存溢出 避免？不使用的时候，主动remove()移除 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/一部分并发理论基础.html":{"url":"Java-技能/多线程/一部分并发理论基础.html","title":"一部分并发理论基础","keywords":"","body":"1.1. 并发理论基础1.1.1. 并发编程bug源头1.1.2. Java内存模型，如何解决可见性、有序性1.1.3. 原子性问题解决，互斥锁1.1.4. 死锁，怎么办1.1.5. 用等待-通知机制优化循环等待(线程间协作方式)1.1.6. 安全性、活跃性以及性能问题1.1.7. 管程，并发编程的万能钥匙1.1.8. Java线程的生命周期1.1. 并发理论基础 为什么要使用多线程：降低延迟，提高吞吐量 多线程应用场景：并发编程领域，提升性能本质就是提升硬件使用率，就是提升I/O利用率和CPU利用率 创建多少线程合适： 对于CPU密集型的计算场景，线程数=CPU核数+1就是合适的 对于I/O密集型的计算场景，最佳线程数=CPU核数*(1+I/O耗时/CPU耗时) 1.1.1. 并发编程bug源头 可见性、有序性、原子性问题 缓存导致的可见性问题：一个线程对共享变量的修改，另外一个线程能够立刻看到，称为可见性 线程切换带来的原子性问题：一个或者多个操作在CPU执行的过程中不被中断的特性称为原子性 编译优化带来的有序性问题 如：双端检锁机制实例化单例bean的时候，在getInstance() 方法中，先判断instance是否为空，再加锁，再判断一次是否为空，再去实例化 有序性问题描述：当有A,B两个线程同时访问，同时判断实例为空，其中一个线程获得锁，进入方法内实例化；另一个线程等待A线程执行完，执行完后获得锁对象，再次判断不为空，不会再去实例化 我们以为的流程： 分配一块内存M 在内存M上初始化instance 然后把M的地址赋值给instance 然而JMM内存优化后的是： 分配一块内存M 把M的地址赋值给instance 在内存M上初始化instance 解决：增加volatile禁止指令重排序 1.1.2. Java内存模型，如何解决可见性、有序性 导致可见性原因是缓存优化，导致有序性是编译器优化，那最直接的解决方案就是禁用缓存优化，禁用编译器优化；但是，会有性能问题 按需禁用缓存优化编译器优化 Java内存模型规范了JVM如何提供按需禁用缓存以及编译优化的方法 volatile synchronized final 六项Happens-before规则：前面一个操作的结果对后续是可见的 对一个volatile变量的写操作相对于后续对这个volatile变量的读操作可见 传递性：如果A happens-before B，B happens-before C,那么A happens-before C 管程中锁的规则：对一个锁的解锁Happens-before于后续对这个锁的加锁（管程：通用的同步原语，Java中synchronized是对管程的实现） 1.1.3. 原子性问题解决，互斥锁 互斥：同一个时刻只有一个线程在执行 synchronized关键字：修饰静态方法，锁住的是当前Class对象；修饰非静态方法，锁定的是当前实例对象this 可以使用一把锁保护多个资源，但是不可以使用多把锁保护同一个资源 保护没有关联关系的多个资源 如电影票和球场门票 用同一把锁保护多个资源会编程多个资源串行化 优化：细粒度锁，用不同的锁保护资源进行精细化管理，以优化性能 保护有关联关系的多个资源：选择粒度更大的锁，这个锁能覆盖所有相关的资源 原子性本质：多个资源间有一致性的要求，操作的中间状态对外不可见 1.1.4. 死锁，怎么办 细粒度锁虽然可以优化性能，但是代价就是可能会导致死锁。一组互相竞争资源的线程因互相等待，导致\"永久\"阻塞的现象 死锁发生条件 互斥，共享资源x和y只能被一个线程占用 占有且等待，线程T1已经取得共享资源x，在等待共享资源y的时候，不释放共享资源x。 破坏：一次性申请所有资源，就不存在等待了，如增加管理员 不可抢占：其他线程不能强行抢占线程T1占有的资源。 破坏：能够主动释放它占有的资源，synchronized做不到，因为如果申请不到会阻塞，可以使用juc下的Lock 循环等待：线程T1等待线程T2占有的资源，线程T2在等待线程T1占有的资源，就是循环等待 破坏：需要对资源进行排序，然后按顺序申请资源 1.1.5. 用等待-通知机制优化循环等待(线程间协作方式) 等待-通知机制：线程首先获取互斥锁，当线程要求的条件不满足时，就释放互斥锁，进入等待状态；当要求的条件满足时，通知等待的线程，重新获取互斥锁 用synchronized实现等待通知机制：wait()、notify()、notifyAll() 尽量使用notifyAll()而不是notify()，因为notify()是随机通知等待队列中的一个线程，而notifyAll()是通知等待队列中的所有线程。(notify()风险在于等待队列中的某个线程可能永远不会被通知到) wait()方法和sleep()区别 wait()方法会释放\"锁标识\"，在synchronized代码块内别的线程可以访问到共享资源 sleep()方法需要指定等待事件，它可以让当前正在执行的线程在指定的时间内暂停执行，进入阻塞状态，不会释放\"锁标识\"，该方法可以让其他同优先级或者高优先级的线程得到执行的机会 1.1.6. 安全性、活跃性以及性能问题 安全性问题 本质：正确性，程序按照我们的期望执行，线程安全 如何解决？ 避免原子性问题，可见性问题，有序性问题 互斥 什么情况下才需要考虑？ 存在共享数据并且数据会发生变化，通俗讲就是多个线程会同时读写同一数据 活跃性问题 操作无法执行下去，如死锁、活锁、饥饿 死锁：线程互相等待，一直等下去，就永久阻塞了 活锁：两人互相谦让，先走，a以为b不走刚准备走，b也以为a不走刚准备走，就又会相撞 解决：每个人增加一个随机等待时间，相当于每个线程等待时间不同，相撞的概率就会降低。Raft知名的分布式一致性算法 饥饿：某线程因无法访问所需资源而无法执行下去，线程优先级不均，则有可能某个线程一直无法获得调度；持有锁的线程，如果执行时间较长，也有可能导致饥饿 解决：唤醒线程采取类似notifyAll()的方式 性能问题 过度使用锁，导致串行化范围过大 方案层解决 使用无锁的算法或者数据结构 如线程本地存储ThreadLocal 写时复制copy-on-write 乐观锁 Java并发包内的原子类也是无锁的数据结构 减少锁持有的时间 互斥锁本质将并行的程序串行化，所以增加并行的解决方案即是减少持有锁的时间 细粒度锁 java并发包内的ConcurrentHashMap，分段锁技术 读写锁，读无锁，写互斥 指标 吞吐量：单位时间内能处理的请求数量。吞吐量越高、性能越好 延迟：发出请求收到响应的时间。延迟越小、性能越好 并发量：能同时处理的请求数量，并发量增加，延迟也会增加 1.1.7. 管程，并发编程的万能钥匙 管程和信号量是等价的，用管程能实现信号量，能用信号量实现管程，管程就是管理共享变量以及对共享变量的操作过程，让他们支持并发 MESA模型解决互斥问题：将共享变量及对共享变量的操作统一封装起来 MESA模型解决同步问题 管程就是一个对象监视器，任何线程想要访问该资源，都需要先进入监控范围。进入之后，接受检查，不符合条件，继续等待，直到被通知，然后继续进入监视器 1.1.8. Java线程的生命周期 五态模型/六种状态 通用的线程生命周期 五态模型 初始状态：线程已被创建，但是还不允许分配CPU执行。编程语言特有，在编程预言层面被创建，而在操作系统内还没有创建 可运行状态：线程可以分配CPU执行，操作系统线程已被创建 运行状态：有空闲CPU的时候，操作系统会将其分配给一个可运行状态的线程，此时此线程处于运行状态 休眠状态：如果运行状态的线程调用阻塞API或者某个等待某个事件，那线程状态就会变为休眠状态。休眠状态的线程会释放CPU使用权 终止状态：线程执行完，或者执行异常就会进入终止状态，也意味着线程生命周期的结束 Java线程的生命周期 六种状态，简化为三种 New初始化 Runable可运行/运行状态 Blocked阻塞状态 Waiting无时限等待状态 TIMED_WAITING有时限等待状态 Terminated终止状态 Java线程状态转换 RUNABLE转BLOCKED 一种场景，synchronized隐式锁 RUNABLE转WAITING 场景一：获得synchronized隐式锁的线程，调用wait()方法 场景二：调用无参方法Thread.join()，如在b线程中A.join()A线程join，则会B线程转到WAITING状态，同步等待A线程执行完毕，再由WAITING转为RUNABLE 场景三：调用LockSupport.park()方法线程进入WAITING状态，再调用LockSupport.unpark()方法可以唤醒线程进入RUNABLE状态 RUNABLE与TIMED_WAITING状态转换 调用带超时参数的Thread.sleep(long millis)方法 调用带超时参数的Thread.join(long millis) 获得synchronized隐式锁的线程，调用wait(long timeout)方法 调用带超时参数的LockSupport.parkNanos(Object blocker,long deadline) 调用带超时参数的LockSupport.parkUntil(long deadline) NEW到RUNABLE 继承Thread对象，重写run()方法：调用线程对象的start()方法 实现Runable接口，重写run()方法：调用线程对象的start()方法 RUNABLE到TERMINATED run()方法后会自动进入TERMINTED 个人链接：https://www.mubucm.com/doc/547p4gdcUMl Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/二部分并发工具类详解.html":{"url":"Java-技能/多线程/二部分并发工具类详解.html","title":"二部分并发工具类详解","keywords":"","body":"1. 并发工具类1.1. Lock和Condition1.1.1. Lock1.1.2. 三个用锁最佳实践1.2. Semaphore：如何快速实现一个限流器1.2.1. 模型1.3. ReadWriteLock1.3.1. 三大基本原则1.3.2. 快速实现一个缓存1.4. StampedLock1.4.1. 与ReadWriteLock区别1.4.2. 注意事项1.4.3. 模板代码1.5. CountDownLatch和CyclicBarrier：让多线程步调一致1.5.1. CyclicBarrier1.5.2. 区别1.6. 并发容器1.7. 原子类：无锁工具类1.8. Executor与线程池1.9. Future1.10. CompletionStage接口1. 并发工具类 1.1. Lock和Condition condition,解决同步问题 共同点：Lock和Condition实现的管程，线程等待和通知可以调用await(),signal(),signalAll()方法 1.1.1. Lock 解决互斥问题 弥补synchorinzed不能破坏不可抢占条件 void lockInterruptibly()（throws InterruptedException）方法，能够响应中断，即在我们给阻塞的线程发送中断信号时，能够唤醒它，那阻塞的线程就有机会释放持有的锁，这样就可以破坏不可抢占条件 boolean tryLock(long time,TimeUnit unit)(throws InterruptedException) 支持超时，如果线程在一段时间内没有获取到锁，不是进入阻塞状态，而是返回异常，那这个线程也有机会释放持有的锁 boolean tryLock()非阻塞的获取锁,如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，这个线程也会释放持有的锁 ReentrantLock 可重入锁（线程可以重复获取同一把锁）， 内部有一个volatile修饰的state成员变量保证可见性 ReentrantLock有两个构造方法 无参构造方法默认非公平锁 有一个fair参数的构造方法，传入true就构造一个公平锁 锁都对应一个线程等待队列，如果一个线程没有获得锁，就会放入等待队列，当有线程释放锁时，会从等待队列中唤醒一个等待的线程 公平锁的唤醒策略就是谁等待的时间长，就唤醒谁 非公平锁的唤醒策略多线程模式不会按照线程顺序获取锁，有可能等待时间短的线程先被唤醒 1.1.2. 三个用锁最佳实践 永远只在更新对象的成员变量时加锁 永远只在访问可变的成员变量时加锁 永远不在调用其它对象的方法时加锁 1.2. Semaphore：如何快速实现一个限流器 信号量模型：一个计数器、一个等待队列、三个方法 1.2.1. 模型 计数器和等待队列对外透明，所以使用三个方法对外访问 init()设置计数器的初始值 up()计数器的值加1；如果此时计数器的值小于或者等于0，则唤醒一个等待队列中的线程，并将其从等待队列中移除 down()计数器的值减1，如果计数器的值小于0，则当前线程被阻塞 信号量的down(),up()操作也被称为P操作V操作，PV原语 在Java里，down()对应acquire(),up()对应release() 使用场景： 连接池、对象池、线程池 1.3. ReadWriteLock 读多写少场景，缓存元数据，缓存基础数据等 1.3.1. 三大基本原则 允许多个线程同时读共享变量 只允许一个线程写共享变量 当一个写线程在执行写操作，此时禁止读线程读共享变量 1.3.2. 快速实现一个缓存 Cache工具类，一个读缓存方法get()，另一个是写缓存方法put()。 class Cache { final Map m = new HashMap<>(); final ReadWriteLock rwl = new ReentrantReadWriteLock(); // 读锁 final Lock r = rwl.readLock(); // 写锁 final Lock w = rwl.writeLock(); // 读缓存 V get(K key) { r.lock(); try { return m.get(key); } finally { r.unlock(); } } // 写缓存 V put(K key, V value) { w.lock(); try { return m.put(key, v); } finally { w.unlock(); } } } 缓存数据初始化 一次性加载：如在程序启动时，将数据查询出来，调用示例方法中的put()方法 懒加载：在需要用到数据，缓存内不存在时，再去查询数据库放入缓存中 1.4. StampedLock 性能比读写锁更快的锁。支持三种模式：写锁、悲观读锁、乐观读（无锁） 1.4.1. 与ReadWriteLock区别 StampedLock的写锁、悲观读锁的语义和ReadWriteLock的写锁、读锁的语义非常类似，允许多线程同时获取悲观读锁，只允许一个线程获取写锁，写锁和悲观读锁互斥 不同：StampedLock里写锁和悲观读锁加锁成功后，会返回stamp；解锁时，需要传入stamp StampedLock性能之所以比ReadWriteLock要好， 关键是StampedLock支持乐观读方式。ReadWriteLock支持多线程同时读，但是与写操作时候互斥的，写操作会被阻塞；而StampedLock提供的乐观读，是允许一个线程获取写锁的，就是不是所有的写操作都被阻塞 1.4.2. 注意事项 不支持重入 悲观读锁、写锁都不支持条件变量 使用StampedLock一定不要调用中断interrupt()操作，如果需要支持中断，使用可中断的悲观读锁readLockInterruptibly()和写锁writeLockInterruptibly() 1.4.3. 模板代码 读模板 final StampedLock sl= new StampedLock(); // 乐观读 long stamp= sl.tryOptimisticRead(); // 读入方法局部变量 ...... // 校验stamp if(!sl.validate(stamp)){ // 升级为悲观读锁 stamp=sl.readLock(); try{ // 读入方法局部变量 ..... }finally{ //释放悲观读锁 sl.unlockRead(stamp); } } //使用方法局部变量执行业务操作 ...... 写 代码 long stamp=sl.writeLock(); try{ // 写共享变量 ...... }finally{ sl.unlockWrite(stamp); } 1.5. CountDownLatch和CyclicBarrier：让多线程步调一致 CountDownLatch计数器操作 声明创建CountDownLatch,在线程调用时实现countDown()减减操作，主线程中使用await()方法等待CountDownLatch内的初始化大小为0，再继续执行主线城操作 1.5.1. CyclicBarrier 创建一个计数器初始值为N的CyclicBarieer,传入一个回调函数，当计数器减到0的时候，会调用这个回调函数。调用await()将计数器减1，当计数器变成0时，会调用barrier的回调函数 计数器有自动重置的功能，减到0时，会自动重置设置的N初始值 1.5.2. 区别 CountDownLatch主要用于解决一个线程等待多个线程的场景 CyclicBarrier是一组线程之间互相等待 1.6. 并发容器 四大类容器：List，Map，Set，Queue 同步容器：sunchronized 用迭代器遍历容器存在安全性问题 线程不安全代码 List list=Collections.synchronizedList(new ArrayList()); Iterator i=list.iterator(); while(i.hasNext()) foo(i.next()); 线程安全代码 List list=Collections.synchronizedList(new ArrayList()); synchronized (list){ Iterator i=list.iterator(); while(i.hasNext()) foo(i.next()); } 性能差，串行度太高 1.7. 原子类：无锁工具类 1.8. Executor与线程池 1.9. Future 1.10. CompletionStage接口 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/多线程/多线程面试题总结.html":{"url":"Java-技能/多线程/多线程面试题总结.html","title":"多线程面试题总结","keywords":"","body":"1.1. 为什么程序计数器、虚拟机栈和本地方法栈是线程私有的？为什么堆和方法区是线程共享的呢？1.2. Java线程的状态1.1. 为什么程序计数器、虚拟机栈和本地方法栈是线程私有的？为什么堆和方法区是线程共享的呢？ 程序计数器为什么是私有的？ 程序计数器有以下作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 （所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置）。 虚拟机栈和本地方法栈为什么是私有的？ 虚拟机栈：每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在Java虚拟机栈中入栈和出栈的过程。 本地方法栈：和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 （所以）为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的 堆和方法区是所有线程共享的资源 堆是进程中最大的一块内存，主要用于存放新创建的对象（所有对象都在这分配内存） 方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译后的代码数据 1.2. Java线程的状态 初始状态、运行状态、阻塞状态、等待状态、超时等待状态、终止状态。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换 线程创建后处于初始状态（New）,调用start()方法后开始运行，线程这时候处于可运行状态（READY） 可运行状态的线程获得了CPU时间片后就处于运行状态（RUNNING） 当线程执行wait()方法之后，线程进入等待状态(WAITING),进入等待状态的线程需要依靠其他线程的通知才能返回到运行状态（notify() ）。而超时等待状态（TIME_WAITING）相当于在等待的基础上增加了超时限制，【sleep(long millis)/wait(long millis)】,当超时时间到达后java线程将会返回到运行状态。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/数据处理/json.html":{"url":"Java-技能/数据处理/json.html","title":"Json","keywords":"","body":"1. 为什么用json传输数据1. 为什么用json传输数据 JavaScript Object Notation json可读性高，更简洁，更容易理解 JSON 的解析速度比 XML 更快（因为 XML 与 HTML 很像，在解析大型 XML 文件时需要消耗额外的内存），存储同样的数据，JSON 格式所占的存储空间更小 传送速度快，占用内存小 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/网络/http/HTTP常见错误码.html":{"url":"Java-技能/网络/http/HTTP常见错误码.html","title":"HTTP常见错误码","keywords":"","body":" 200：正确的请求返回正确的结果，如果不想细分正确的请求结果都可以直接返回200。 201：表示资源被正确的创建。比如说，我们 POST 用户名、密码正确创建了一个用户就可以返回 201。 202：请求是正确的，但是结果正在处理中，这时候客户端可以通过轮询等机制继续请求。 203：请求的代理服务器修改了源服务器返回的 200 中的内容，我们通过代理服务器向服务器 A 请求用户信息，服务器 A 正常响应，但代理服务器命中了缓存并返回了自己的缓存内容，这时候它返回 203 告诉我们这部分信息不一定是最新的，我们可以自行判断并处理。 300：请求成功，但结果有多种选择。 301：请求成功，但是资源被永久转移。比如说，我们下载的东西不在这个地址需要去到新的地址。 303：使用 GET 来访问新的地址来获取资源。 304：请求的资源并没有被修改过。 308：使用原有的地址请求方式来通过新地址获取资源。 400：请求出现错误，比如请求头不对等。 401：没有提供认证信息。请求的时候没有带上 Token 等。 402：为以后需要所保留的状态码。 403：请求的资源不允许访问。就是说没有权限。 404：请求的内容不存在。 406：请求的资源并不符合要求。 408：客户端请求超时。 413：请求体过大。 415：类型不正确。 416：请求的区间无效。 500：服务器错误。 501：请求还没有被实现。 502：网关错误。 503：服务暂时不可用。服务器正好在更新代码重启。 505：请求的 HTTP 版本不支持。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/网络/http/HTTP常见面试.html":{"url":"Java-技能/网络/http/HTTP常见面试.html","title":"HTTP常见面试","keywords":"","body":" Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/网络/TCP协议.html":{"url":"Java-技能/网络/TCP协议.html","title":"TCP协议","keywords":"","body":"1.1. TCP三次握手、四次挥手1.1.1. TCP三次握手（Three-Way Handshake）1.1.2. TCP四次挥手（Four-Way Wavehand）1.2. TCP,UDP协议的区别1.1. TCP三次握手、四次挥手 TCP报文格式比较重要的字段有： 序号（sequence number）：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记 确认号（acknowledgement number）：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1 标志位（Flags）：共6个，即URG、ACK、PSH、RST、SYN、FIN等。具体含义如下： URG：紧急指针（urgent pointer）有效 ACK：确认序号有效 PSH：接收方应该尽快将这个报文交给应用层 RST：重置连接 SYN：发起一个新连接 FIN：释放一个连接 1.1.1. TCP三次握手（Three-Way Handshake） 三次握手==TCP建立连接，这个连接必须是一方主动打开，另一方被动打开，如图（来源于网络） 客户端向服务器端发送一段TCP报文:SYN=1,seq=x 标记位为SYN，表示“请求建立新连接”; 序号为Seq=X（X一般为1） 随后客户端进入SYN-SENT阶段 服务器端接收到来自客户端的TCP报文之后，结束LISTEN阶段。并返回一段TCP报文，其中：SYN=1,ACK=1,seq=y,ack=x+1 标志位为SYN和ACK，表示“确认客户端的报文Seq序号有效，服务器能正常接收客户端发送的数据，并同意创建新连接” 序号为seq=y 确认号为Ack=x+1，表示收到客户端的序号Seq并将其值加1作为自己确认号Ack的值；随后服务器端进入SYN-RCVD阶段 客户端接收到来自服务器端的确认收到数据的TCP报文之后，明确了从客户端到服务器的数据传输是正常的，结束SYN-SENT阶段。并返回最后一段TCP报文。其中：ACK=1,seq=x+1,ack=y+1 标志位为ACK，表示“确认收到服务器端同意连接的信号” 序号为Seq=x+1，表示收到服务器端的确认号Ack，并将其值作为自己的序号值 确认号为Ack=y+1，表示收到服务器端序号Seq，并将其值加1作为自己的确认号Ack的值 随后客户端进入ESTABLISHED阶段 服务器收到来自客户端的“确认收到服务器数据”的TCP报文之后，明确了从服务器到客户端的数据传输是正常的。结束SYN-SENT阶段，进入ESTABLISHED阶段 为什么要三次握手 目的：双方确认自己与对方的发送与接收是正常的，建立可靠的通信信道 第一次握手：Client什么都不能确认;Server端接收正常，对方发送正常 第二次握手：Client确定自己发送正常，接收正常，对方发送正常，接收正常;Server确认对方发送正常，自己接收正常 第三次握手：Client确定自己发送正常，接收正常，对方发送正常，接收正常;Server确认对方发送正常，自己接收正常，自己发送正常，对方接收正常 三次握手可以携带数据吗 第三次可以，前两次不可以，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据，疯狂着重复发 SYN 报文，这会让服务器花费大量的内存空间来缓存这些报文，这样服务器就更容易被攻击 1.1.2. TCP四次挥手（Four-Way Wavehand） 四次挥手即TCP连接的释放(解除)。连接的释放必须是一方主动释放，另一方被动释放 首先客户端想要释放连接，向服务器端发送一段TCP报文，其中：FIN=1,seq=u 标记位为FIN，表示“请求释放连接“ 序号为Seq=U 随后客户端进入FIN-WAIT-1阶段，即半关闭阶段。并且停止在客户端到服务器端方向上发送数据，但是客户端仍然能接收从服务器端传输过来的数据。 服务器端接收到从客户端发出的TCP报文之后，确认了客户端想要释放连接，随后服务器端结束ESTABLISHED阶段，进入CLOSE-WAIT阶段（半关闭状态）并返回一段TCP报文,其中：ACK=1,seq=v,ack=u+1 标记位为ACK，表示“接收到客户端发送的释放连接的请求” 序号为Seq=v 确认号为Ack=U+1，表示是在收到客户端报文的基础上，将其序号Seq值加1作为本段报文确认号Ack的值 随后服务器端开始准备释放服务器端到客户端方向上的连接 客户端收到从服务器端发出的TCP报文之后，确认了服务器收到了客户端发出的释放连接请求，随后客户端结束FIN-WAIT-1阶段，进入FIN-WAIT-2阶段 服务器端自从发出ACK确认报文之后，经过CLOSED-WAIT阶段，做好了释放服务器端到客户端方向上的连接准备，再次向客户端发出一段TCP报文，其中：FIN=1,ACK=1,seq=w,ack=u+1 标记位为FIN，ACK，表示“已经准备好释放连接了” 序号为Seq=W 确认号为Ack=U+1；表示是在收到客户端报文的基础上，将其序号Seq值加1作为本段报文确认号Ack的值 随后服务器端结束CLOSE-WAIT阶段，进入LAST-ACK阶段。并且停止在服务器端到客户端的方向上发送数据，但是服务器端仍然能够接收从客户端传输过来的数据。 客户端收到从服务器端发出的TCP报文，确认了服务器端已做好释放连接的准备，结束FIN-WAIT-2阶段，进入TIME-WAIT阶段，并向服务器端发送一段报文，其中：ACK=1,seq=u+1,ack=w+1 标记位为ACK，表示“接收到服务器准备好释放连接的信号” 序号为Seq=U+1；表示是在收到了服务器端报文的基础上，将其确认号Ack值作为本段报文序号的值 确认号为Ack=W+1；表示是在收到了服务器端报文的基础上，将其序号Seq值作为本段报文确认号的值 随后客户端开始在TIME-WAIT阶段等待2MSL 服务器端收到从客户端发出的TCP报文之后结束LAST-ACK阶段，进入CLOSED阶段。由此正式确认关闭服务器端到客户端方向上的连接;客户端等待完2MSL之后，结束TIME-WAIT阶段，进入CLOSED阶段，由此完成\" 四次挥手\" 为什么要等待2MSL MSL（Maximum Segment Lifetime），TCP允许不同的实现可以设置不同的MSL值。 保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。 防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中 如果已建立了连接，客户端故障了如何处理 TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接 1.2. TCP,UDP协议的区别 TCP 优点： 可靠，稳定 TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源。 缺点：慢，效率低，占用系统资源高，易被攻击，因为TCP有确认机制、三次握手机制，这些也导致TCP容易被人利用，实现DOS、DDOS、CC等攻击 应用：浏览器，用的HTTP FlashFXP，用的FTP Outlook，用的POP、SMTP Putty，用的Telnet、SSH QQ文件传输 UDP: 优点： UDP是一个无状态的传输协议，所以它在传递数据时非常快 缺点：网络质量不好，就会很容易丢包，不可靠，不稳定 应用：QQ语音 QQ视频 TFTP 直播 总结： TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接 每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信 TCP首部开销20字节;UDP的首部开销小，只有8个字节 TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/网络/计算机网络.html":{"url":"Java-技能/网络/计算机网络.html","title":"计算机网络","keywords":"","body":"1.1. OSI七层模型1.1. OSI七层模型 来源 OSI（Open System Interconnect），即开放式系统互联。 一般都叫OSI参考模型，是ISO（国际标准化组织）组织在1985年研究的网络互连模型。 七层模型划分 ISO开放互联系统参考模型：（下3层主要任务数据通信，上3层主要任务数据处理） 物理层： 参考模型的最低层 利用传输介质为数据链路层提供物理连接，实现比特流的传输 物理层常用的传输介质：集线器、中继器、调制解调器、网线、双绞线、同轴电缆 数据链路层： 介质访问控制（MAC）和逻辑链路控制（LLC） 通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路 通过差错控制，流量控制，使有差错的物理线路变为无差错的数据链路，即提供可靠的通过物理介质传输数据的方法 网络层 通信子网的最高一层，在下两层的基础上向资源子网提供服务 IP选址，路由选择 通过路由选择算法，为报文或分组通过通信子网选择最适当的路径 路由算法：当源节点和目的节点之间存在多条路径时，本层可以根据路由算法，通过网络为数据分组选择最佳路径，并将信息从最合适的路径由发送端传送到接收端 数据链路层是解决同一网络内节点之间的通信，而网络层主要解决不同子网间的通信 IP协议层 传输层 建立，管理，维护端到端的连接 该层是通信子网和资源子网接口和桥梁，起到承上启下的作用 该层常见的协议有TCP/IP中的TCP协议、Novell网络中的SPX协议和微软的NetBIOS/NetBEUI协议 提供建立、维护和拆除传输连接的功能。传输层在网络层的基础上为高层提供“面向连接”和“面向无连接”的两种服务 用户数据协议 UDP（User Datagram Protocol）--提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性） 会话层 组织和协调两个会话进程之间的通信，并对数据交换进行管理和维护。 当建立会话时，用户必须提供他们想要连接的远程地址。而这些地址与MAC（介质访问控制子层）地址或网络层的逻辑地址不同，它们是为用户专门设计的，更便于用户记忆。域名（DN）就是一种网络上使用的远程地址 表示层 数据格式转换，数据加密 对来自应用层的命令和数据进行解释，对各种语法赋予相应的含义，并按照一定的格式传送给会话层。 压缩和解压缩：为了减少数据的传输量，这一层还负责数据的压缩与恢复 应用层 参考模型的最高层 直接向用户提供服务，完成用户希望在网络上完成的各种工作 - 对于不同的网络应用需要不同的应用层协议，（HTTP，HTTPS，FTP，POP3、SMTP）文件服务、目录服务、文件传输服务（FTP）、远程登录服务（Telnet）、电子邮件服务（E-mail）、打印服务、安全服务、网络管理服务、数据库服务 如图，总结 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/":{"url":"Java-技能/设计模式/","title":"设计模式","keywords":"","body":"1.1. 设计模式1.1. 设计模式 创建型模型 单例模式 简单工厂模式 行为模型 模板方法模式 策略模式 命令模式 责任链模式 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/创建型模型/":{"url":"Java-技能/设计模式/创建型模型/","title":"创建型模型","keywords":"","body":"1.1. 创建型模型1.1. 创建型模型 单例模式 简单工厂模式 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/创建型模型/单例模式.html":{"url":"Java-技能/设计模式/创建型模型/单例模式.html","title":"单例模式","keywords":"","body":"1.1. 单例模式1.1.1. 单例模式优点和缺点1.1.2. 单例模式的应用场景1.1.3. 实现单例的模式的几种方式1.1.4. 实例代码1.1. 单例模式 在有些系统中，为了节省内存资源、保证数据内容的一致性，对某些类要求只能创建一个实例，这就是所谓的单例模式。 1.1.1. 单例模式优点和缺点 优点： 单例模式可以保证内存里只有一个实例，减少了内存的开销。 可以避免对资源的多重占用。 单例模式设置全局访问点，可以优化和共享资源的访问。 缺点： 单例模式一般没有接口，扩展困难。如果要扩展，则除了修改原来的代码，没有第二种途径，违背开闭原则。 在并发测试中，单例模式不利于代码调试。在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。 单例模式的功能代码通常写在一个类中，如果功能设计不合理，则很容易违背单一职责原则。1.1.2. 单例模式的应用场景 需要频繁创建的一些类，使用单例可以降低系统的内存压力，减少 GC。 某类需要频繁实例化，而创建的对象又频繁被销毁的时候，如多线程的线程池、网络连接池等。 频繁访问数据库或文件的对象。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。如 Web 中的配置对象、数据库的连接池等。1.1.3. 实现单例的模式的几种方式 饿汉模式 懒汉模式 DCL双端检锁机制 枚举模式 静态内部类模式1.1.4. 实例代码 ```java /** @author lviter 单例模式的四种实现： 饿汉 懒汉 DCL双端检锁机制 静态内部类 枚举模式 */ public class Singleton { public static void main(String[] args) { SingletonEnum s1 = SingletonEnum.INSTANCE.getInstance(); SingletonEnum s2 = SingletonEnum.INSTANCE.getInstance(); System.out.println(s1 == s2); } } /** 懒汉式--线程不安全 */ class SingletonLazy { private static SingletonLazy singletonLazy = null; private SingletonLazy() { System.out.println(\"懒汉式---线程不安全的构造方法\"); } public static SingletonLazy getSingletonLazy() { if (singletonLazy == null) { return new SingletonLazy(); } return singletonLazy; } } /** 饿汉式 没有加锁，执行效率会提高。缺点是类加载时就初始化，浪费内存 基于classloder机制避免了多线程的同步问题 */ class SingletonHungry { private static final SingletonHungry singletonHungry = new SingletonHungry(); private SingletonHungry() { System.out.println(\"饿汉式\"); } public static SingletonHungry getSingletonHungry() { return singletonHungry; } } /** DCL:double check lock,双端检锁机制--在同步锁前后都增加检查操作 多线程安全，采用双锁机制，安全且在多线程下保持高性能。 */ class SingletonDcl { private volatile static SingletonDcl singletonDcl; private SingletonDcl() { System.out.println(\"DCL双端检锁--线程安全，支持高性能\"); } /** 同步锁前后都增加检查 * @return */ public static SingletonDcl getSingletonDcl() { if (singletonDcl == null) { synchronized (SingletonDcl.class) { if (singletonDcl == null) { singletonDcl = new SingletonDcl(); } } } return singletonDcl; } } /** 静态内部类的构造方法只会在调用他的时候触发，所以是线程安全的 */ class SingletonStaticInternal { private SingletonStaticInternal() { System.out.println(\"这里是静态内部类的方式\"); } private static class holder { private static final SingletonStaticInternal instance = new SingletonStaticInternal(); } public static SingletonStaticInternal getInstance() { return holder.instance; } } /** 枚举单例(单例模式的最佳实现方法) 既可以避免多线程同步问题；还可以防止通过反射和反序列化来重新创建新的对象 / enum SingletonEnum { /* 单例 */ INSTANCE; public SingletonEnum getInstance() { return INSTANCE; } public void m() { System.out.println(\"枚举类\"); } } ``` Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/创建型模型/简单工厂模式.html":{"url":"Java-技能/设计模式/创建型模型/简单工厂模式.html","title":"简单工厂模式","keywords":"","body":"1.1. 简单工厂模式（SimpleFactory）1.1. 简单工厂模式（SimpleFactory） 定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。这满足创建型模式中所要求的“创建与使用相分离”的特点。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/行为模型/":{"url":"Java-技能/设计模式/行为模型/","title":"行为模型","keywords":"","body":"1.1. 行为模型1.1. 行为模型 模板方法模式 策略模式 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/行为模型/命令模式.html":{"url":"Java-技能/设计模式/行为模型/命令模式.html","title":"命令模式","keywords":"","body":"1.1. 行为模型：命令模式1.1.1. 定义与特点1.1.2. 模式结构与实现1.1. 行为模型：命令模式 如看电视时，我们只需要轻轻一按遥控器就能完成频道的切换，这就是命令模式，将换台请求和换台处理完全解耦了。电视机遥控器（命令发送者）通过按钮（具体命令）来遥控电视机（命令接收者）。 1.1.1. 定义与特点 定义：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 优点： 引入抽象接口降低系统耦合度 扩展性好，增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，且满足“开闭原则”。 可与多种设计模式组合，如结合备忘录模式，实现命令的撤销与恢复；结合装饰器模式，更加灵活增加日志记录 缺点： 增加系统的复杂性1.1.2. 模式结构与实现 模式结构 包含以下主要角色： 抽象命令类（command）:声明执行命令的接口，拥有抽象方法execute() 具体命令类（concrete command）:抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作 实现者/接收者（Receiver）:执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）:请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者 模式的代码实现 代码 示例：增加日志操作 对外模板调用方法：RecordLogTemplate.classimport org.springframework.stereotype.Component; /** * @author lvite */ @Component public class RecordLogTemplate { /** * 添加日志 */ public void addLog() { AbstractLogCommand cmd = new ConcreteLogCommand(); LogInvoker logInvoker = new LogInvoker(cmd); logInvoker.call(); } } 抽象命令类AbstractLogCommand.class ```java /** @author lviter 日志抽象命令模式 */ public abstract class AbstractLogCommand { /** 记录日志 */ public abstract void recordLog(); } - 抽象命令的具体实现`ConcreteLogCommand.class` ```java import org.springframework.stereotype.Component; /** * @author lviter 命令模式实际实现 */ @Component public class ConcreteLogCommand extends AbstractLogCommand { @Override public void recordLog() { LogReceiver logReceiver = new LogReceiver(); logReceiver.addLog(); } } 日志实际调用者LogInvoker.class /** * @author lviter 记录日志调用者 */ public class LogInvoker { private AbstractLogCommand abstractLogCommand; public LogInvoker(AbstractLogCommand abstractLogCommand) { this.abstractLogCommand = abstractLogCommand; } public void call() { abstractLogCommand.recordLog(); } } 日志实现LogReceiver.class ```java import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; /** @author lviter 写日志实现 */ @Component public class LogReceiver { private static Logger log = LoggerFactory.getLogger(LogReceiver.class); public void addLog() { log.info(\"============================写日志操作========================\"); } } ``` Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/行为模型/模板方法设计模式.html":{"url":"Java-技能/设计模式/行为模型/模板方法设计模式.html","title":"模板方法设计模式","keywords":"","body":"1.1. 行为模型：模板方法1.1.1. 模式定义与特点1.1.2. 模式的应用实例1.1. 行为模型：模板方法 设计一个系统时知道了算法所需的关键步骤，而且确定了这些步骤的执行顺序，但某些步骤的具体实现还未知，或者说某些步骤的实现与具体的环境相关 如：去银行办理业务一般要经过以下4个流程：取号、排队、办理具体业务、对银行工作人员进行评分等，其中取号、排队和对银行工作人员进行评分的业务对每个客户是一样的，可以在父类中实现，但是办理具体业务却因人而异，它可能是存款、取款或者转账等，可以延迟到子类中实现 1.1.1. 模式定义与特点 该模式的主要优点 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 该模式主要缺点 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象，间接地增加了系统实现的复杂度。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度 由于继承关系自身的缺点，如果父类添加新的抽象方法，则所有子类都要改一遍。 1.1.2. 模式的应用实例 如下代码，根据业务需求，手机号注册以及账密注册两种方式，注册流程都相同，现业务的区别之处在于存储redis的namespace不同，因此封装了不变部分，扩展可变部分，实现模板方法模式 ```java import com.dadi01.scrm.foundation.model.error.ErrorEnum; import com.dadi01.scrm.foundation.model.exception.ScrmException; import com.dadi01.scrm.foundation.utils.date.DateRelatedUtils; import com.dadi01.scrm.service.member.api.dto.request.AbstractRegisterRq; import com.dadi01.scrm.service.member.provider.domain.MemberDomain; import com.dadi01.scrm.service.member.provider.factory.AuthenticatorFactory; import com.dadi01.scrm.service.member.provider.service.base.IBaseMemberService; import com.dadi01.scrm.service.member.provider.util.CheckModelUtils; import com.dadi01.scrm.service.member.provider.util.KeyGenerateUtil; import org.apache.commons.lang3.StringUtils; import org.springframework.beans.BeanUtils; import org.springframework.stereotype.Component; import java.text.DateFormat; import java.text.ParseException; import java.text.SimpleDateFormat; import java.util.HashMap; import java.util.Map; /** @author lviter */ @Component public abstract class RegisterTemplate { private final AuthenticatorFactory authenticatorFactory; private final IBaseMemberService baseMemberService; private final KeyGenerateUtil keyGenerateUtil; public RegisterTemplate(AuthenticatorFactory authenticatorFactory, IBaseMemberService baseMemberService, KeyGenerateUtil keyGenerateUtil) { this.authenticatorFactory = authenticatorFactory; this.baseMemberService = baseMemberService; this.keyGenerateUtil = keyGenerateUtil; } /** 注册统一模板方法 * @param tenantId @param account @param abstractRegisterRq */ public void registerTemplate(Integer tenantId, String account, AbstractRegisterRq abstractRegisterRq) { Long cacheSize = getCache(tenantId, account); if (null == cacheSize || cacheSize //创建账号 Long id = generateKey(); saveMember(id, abstractRegisterRq); //放入缓存 saveCache(abstractRegisterRq.getTenantId(), account, id); } else { registerMemberDomainValidate(cacheSize, tenantId); } } /** * 生成分布式主键 * * @return */ public Long generateKey() { return keyGenerateUtil.generate(); } /** * 获取缓存内手机号/账号对应的id * * @param tenantId */ public Long getCache(Integer tenantId, String account) { return (Long) getLoginUserFromCache(tenantId).get(account); } /** * 结果存入缓存 * * @param tenantId * @param account * @param id */ public void saveCache(Integer tenantId, String account, Long id) { Map cache = new HashMap<>(2); cache.put(account, id); saveLoginUserToCache(tenantId, cache); } /** * 从缓存内获取hash集合 * * @param tenantId * @return */ public abstract Map getLoginUserFromCache(Integer tenantId); /** * 存储缓存 * * @param tenantId * @param cache */ public abstract void saveLoginUserToCache(Integer tenantId, Map cache); /** * 保存会员信息 * * @param id * @param abstractRegisterRq */ public void saveMember(Long id, AbstractRegisterRq abstractRegisterRq) { MemberDomain memberDomain = new MemberDomain(id); BeanUtils.copyProperties(abstractRegisterRq, memberDomain); CheckModelUtils.checkField(memberDomain); if (StringUtils.isNotBlank(abstractRegisterRq.getBirth())) { DateFormat fmt = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); DateRelatedUtils.convertStringToLongDateString(abstractRegisterRq.getBirth()); try { memberDomain.setBirth(fmt.parse(abstractRegisterRq.getBirth())); } catch (ParseException e) { throw new ScrmException(ErrorEnum.MEMBER_DATE_FORMAT_ERROR.build()); } } baseMemberService.save(memberDomain); } /** * 校验账户 * * @param id * @param tenantId */ public void registerMemberDomainValidate(Long id, Integer tenantId) { authenticatorFactory.registerMemberDomainValidate(id, tenantId); } } 上面代码为抽象类，封装了不变的方法实现 ```java import com.dadi01.scrm.foundation.model.constant.RedisConstant; import com.dadi01.scrm.service.member.provider.factory.AuthenticatorFactory; import com.dadi01.scrm.service.member.provider.service.base.IBaseMemberService; import com.dadi01.scrm.service.member.provider.util.KeyGenerateUtil; import com.dadi01.scrm.service.member.provider.util.RedisUtil; import org.springframework.stereotype.Component; import java.util.Map; /** * @author lviter */ @Component public class PasswordRegister extends RegisterTemplate { private final RedisUtil redisUtils; public PasswordRegister(AuthenticatorFactory authenticatorFactory, IBaseMemberService baseMemberService, KeyGenerateUtil keyGenerateUtil, RedisUtil redisUtils) { super(authenticatorFactory, baseMemberService, keyGenerateUtil); this.redisUtils = redisUtils; } /** * 获取登录账户从缓存中是否存在 * * @param tenantId * @return */ @Override public Map getLoginUserFromCache(Integer tenantId) { return redisUtils.hmget(RedisConstant.REDIS_PASSWORD_NAMESPACE.concat(String.valueOf(tenantId))); } /** * 保存账户到缓存内 * * @param tenantId * @param memberPhoneCache */ @Override public void saveLoginUserToCache(Integer tenantId, Map memberPhoneCache) { redisUtils.hmset(RedisConstant.REDIS_PASSWORD_NAMESPACE.concat(String.valueOf(tenantId)), memberPhoneCache); } } 上面为账密方式注册，重写了可变的方法 import com.dadi01.scrm.foundation.model.constant.RedisConstant; import com.dadi01.scrm.service.member.provider.factory.AuthenticatorFactory; import com.dadi01.scrm.service.member.provider.service.base.IBaseMemberService; import com.dadi01.scrm.service.member.provider.util.KeyGenerateUtil; import com.dadi01.scrm.service.member.provider.util.RedisUtil; import org.springframework.stereotype.Component; import java.util.Map; /** * @author lviter */ @Component public class SmsRegister extends RegisterTemplate { private final RedisUtil redisUtils; public SmsRegister(AuthenticatorFactory authenticatorFactory, IBaseMemberService baseMemberService, KeyGenerateUtil keyGenerateUtil, RedisUtil redisUtils) { super(authenticatorFactory, baseMemberService, keyGenerateUtil); this.redisUtils = redisUtils; } @Override public Map getLoginUserFromCache(Integer tenantId) { return redisUtils.hmget(RedisConstant.REDIS_PHONE_NAMESPACE.concat(String.valueOf(tenantId))); } @Override public void saveLoginUserToCache(Integer tenantId, Map cache) { redisUtils.hmset(RedisConstant.REDIS_PHONE_NAMESPACE.concat(String.valueOf(tenantId)), cache); } } 上面为手机号方式注册，重写了可变的方法 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/行为模型/策略模式.html":{"url":"Java-技能/设计模式/行为模型/策略模式.html","title":"策略模式","keywords":"","body":"1.1. 策略模式(Strategy Pattern)1.1.1. 介绍1.1.2. 实现1.1.3. 注意1.1. 策略模式(Strategy Pattern) 代码参考 一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式1.1.1. 介绍 应用背景：1.聚合登录的认证中心，有多种登录方式，返回需要同样的token结构，入参结构固定，登陆方式不同，所以认证中心的设计，在实现便捷扩展不同登录方式时可以只增加不同登录方式的实现。2.调度任务的异常通知方式，可以实现微信/钉钉/飞书等不同方式的通知实现 优点：1、应用可以自由切换2、避免使用多重条件判断3、扩展性良好 缺点：策略类会增多，所有策略类都会堆外暴漏 实现：实现同一个接口 1.1.2. 实现 创建一个定义认证操作的接口AuthService,聚合多种服务实现，如AppleAuthServiceImpl,WechatAuthServiceImpl等 创建一个接口 ```java import com.security.spring.web.rq.GetTokenMessageDTO; import com.security.spring.web.rs.TokenMessageDTO; /** @author Administrator */ public interface AuthService { TokenMessageDTO doAuth(GetTokenMessageDTO getTokenMessageDTO); } 2. 创建实现接口的实现类 ```java import com.security.spring.auth.AuthService; import com.security.spring.web.rq.GetTokenMessageDTO; import com.security.spring.web.rs.TokenMessageDTO; import org.springframework.stereotype.Service; /** * @author Administrator */ @Service public class AppleAuthServiceImpl implements AuthService { @Override public TokenMessageDTO doAuth(GetTokenMessageDTO getTokenMessageDTO) { return null; } } import com.security.spring.auth.AuthService; import com.security.spring.web.rq.GetTokenMessageDTO; import com.security.spring.web.rs.TokenMessageDTO; import org.springframework.stereotype.Service; /** * @author Administrator */ @Service public class WechatAuthServiceImpl implements AuthService { @Override public TokenMessageDTO doAuth(GetTokenMessageDTO getTokenMessageDTO) { return null; } } 调用方采用获取到所有实现类的方式，装载入集合内分别调用 ```java import com.security.spring.auth.AuthService; import com.security.spring.service.AuthorizationService; import com.security.spring.web.rq.GetTokenMessageDTO; import com.security.spring.web.rs.TokenMessageDTO; import org.springframework.beans.BeansException; import org.springframework.beans.factory.InitializingBean; import org.springframework.context.ApplicationContext; import org.springframework.context.ApplicationContextAware; import org.springframework.stereotype.Service; import org.springframework.util.CollectionUtils; import java.util.ArrayList; import java.util.List; import java.util.Map; import java.util.concurrent.atomic.AtomicReference; /** @author Administrator */ @Service public class AuthorizationServiceImpl implements AuthorizationService, ApplicationContextAware, InitializingBean { private ApplicationContext applicationContext; private List authServices; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; } @Override public void afterPropertiesSet() { Map serviceBeanMap = applicationContext.getBeansOfType(AuthService.class); if (serviceBeanMap.size() > 0) { authServices = new ArrayList<>(serviceBeanMap.values()); } } @Override public TokenMessageDTO doAuth(GetTokenMessageDTO getTokenMessageDTO) { AtomicReference tokenMessageDTO = new AtomicReference<>(new TokenMessageDTO()); if (!CollectionUtils.isEmpty(authServices)) { authServices.forEach(authService -> { try { TokenMessageDTO messageDTO = authService.doAuth(getTokenMessageDTO); if (null != messageDTO) { tokenMessageDTO.set(messageDTO); } } catch (Exception e) { System.out.println(\"异常\"); } }); } return tokenMessageDTO.get(); } } ``` 1.1.3. 注意 实现service的实现类需要判断类型，返回异常或者空皆可处理，应用与认证中心需要改造，应用通知类则可支持多种渠道同事通知 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/行为模型/责任链模式.html":{"url":"Java-技能/设计模式/行为模型/责任链模式.html","title":"责任链模式","keywords":"","body":"1.1. 行为模型：责任链模式1.1. 行为模型：责任链模式 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/设计模式/设计模式基础概念.html":{"url":"Java-技能/设计模式/设计模式基础概念.html","title":"设计模式基础概念","keywords":"","body":"1. 设计模式1.1. 如何正确使用1.2. 设计原则1. 设计模式 1.1. 如何正确使用 1.2. 设计原则 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/":{"url":"Java-技能/集合/","title":"集合","keywords":"","body":"1. 集合框架1. 集合框架 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/":{"url":"Java-技能/集合/Collection/","title":"Collection","keywords":"","body":"1.1. Interface Collection(java.util)1.2. 数组可以充当集合，为什么还需要其他的集合类1.3. Collection1.4. 集合内遗留类1.5. 常用集合详解1.1. Interface Collection(java.util) 经典图解 所有已知的实现类： AbstractCollection ， AbstractList ， AbstractQueue ， AbstractSequentialList ， AbstractSet ， ArrayBlockingQueue ， ArrayDeque ， ArrayList ， AttributeList ， BeanContextServicesSupport ， BeanContextSupport ， ConcurrentHashMap.KeySetView ， ConcurrentLinkedDeque ， ConcurrentLinkedQueue ， ConcurrentSkipListSet ， CopyOnWriteArrayList ， CopyOnWriteArraySet ， DelayQueue ， EnumSet ， HashSet ， JobStateReasons ， LinkedBlockingDeque ， LinkedBlockingQueue ， LinkedHashSet ， LinkedList ， LinkedTransferQueue ， PriorityBlockingQueue ， PriorityQueue ， RoleList ， RoleUnresolvedList ， Stack ， SynchronousQueue ， TreeSet ， Vector 1.2. 数组可以充当集合，为什么还需要其他的集合类 数组初始化后大小不可变 数组只能按索引顺序存取 1.3. Collection java.util包中主要提供了三种类型的集合： List 一种有序列表的集合 Set 一种保证没有重复元素的集合 Map 一种键值对查找的映射表集合 1.4. 集合内遗留类 Hashtable：一种新城安全的Map实现 Vector:一种线程安全的List实现 Stack:基于Vector实现的LIFO栈 1.5. 常用集合详解 ArrayList CopyOnWriteArrayList Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/ArrayList详解.html":{"url":"Java-技能/集合/Collection/ArrayList详解.html","title":"ArrayList详解","keywords":"","body":"1.1. ArrayList1.2. ArrayList源码解读1.3. 解决arraylist线程安全问题1.1. ArrayList ArrayList是实现了List的动态数组，每个ArrayList实例都有一个容量，容量用来指定数 组的大小。默认初始容量是10，ArrayList中元素增加，容量也会不断的自动增长。每次添加元素时，ArrayList都会检查是否需要进行扩容. 随机元素时间复杂度O(1),插入删除操作需要大量移动元素，效率较低（链表特性） 容器底层采用数组存储，每次扩容1.5倍 ArrayList的实现中大量地调用了Arrays.copyof()和System.arraycopy()方法，其实Arrays.copyof()内部也是调用System.arraycopy()。System.arraycopy()为Native方法 可以存储null值 ArrayList 每次修改（增加、删除）容器时，都是修改自身的 modCount；在生成迭代器时，迭代器会保存该 modCount 值，迭代器每次获取元素时，会比较自身的 modCount 与 ArrayList 的 modCount是否相等，来判断容器是否已经被修改，如果被修改了则抛出异 常（fast-fail 机制）。 1.2. ArrayList源码解读 底层使用数组/** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */ transient Object[] elementData; // non-private to simplify nested class access transient为变量修饰符，当持久化对象时，可能有一个特殊的对象数据成员，我们不想用serialization机制来保存它。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上关键字transient。当一个对象被序列化的时候，transient型变量的值不包括在序列化的表示中，然而非transient型的变量是被包括进去的。 ArrayList提供了add(E e)、add(int index, E element)、addAll(Collection c)、addAll(int index, Collection c)、set(int index, E element)这个五个方法来实现ArrayList增加 /** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return true (as specified by {@link Collection#add}) */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } add 方法第一步操作会去判断是否扩容，size为数组内已有元素的长度；第二步操作是将新的值e插入数组的尾部 /** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException {@inheritDoc} */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } 将element插入数组内指定index的位置 1.3. 解决arraylist线程安全问题 在多线程并发场景下，araylist的add操作可能导致：插入null值；少插入值；插入值超过数组长度。解决arraylist并发导致的问题，有以下几个解决方案： 使用Collections.synchronizedList()方法，相当于synchronized同步锁，不建议使用，影响性能 使用CopyOnWriteArrayList，ReentrantLock可重入锁 ```java public class CopyOnWriteArrayListTest { public static void main(String[] args) { CopyOnWriteArrayList copyOnWriteArrayList = new CopyOnWriteArrayList(); for (int i = 0; i { copyOnWriteArrayList.add(UUID.randomUUID().toString().substring(0, 8)); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(copyOnWriteArrayList); }, \"t1\").start(); } } } class ArrayListDemo { public static List addArrayList() { List strings = new ArrayList<>(); strings.add(UUID.randomUUID().toString().substring(0, 8)); return strings; } } class ArrayListSync { public static List addArrayList() { List strings = Collections.synchronizedList(new ArrayList<>()); strings.add(UUID.randomUUID().toString().substring(0, 8)); return strings; } } ``` Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/CopyOnWriteArrayList详解.html":{"url":"Java-技能/集合/Collection/CopyOnWriteArrayList详解.html","title":"CopyOnWriteArrayList详解","keywords":"","body":"1.1. 写入时复制思想1.2. 使用场景1.3. CopyOnWriteArrayList1.1. 写入时复制思想 写入时复制（CopyOnWrite，简称COW）思想是计算机程序设计领域中的一种优化策略。其核心思想是，如果有多个调用者（Callers）同时要求相同的资源（如内存或者是磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者视图修改资源内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的（transparently）。此做法主要的优点是如果调用者没有修改资源，就不会有副本（private copy）被创建，因此多个调用者只是读取操作时可以共享同一份资源 1.2. 使用场景 读多写少的情况 1.3. CopyOnWriteArrayList 是线程安全的，add操作会先copy一份原数组，数组长度+1，修改完之后，将原来的引用指向新copy的数组。以下是源码部分： /** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */ public boolean add(E e){ final ReentrantLock lock=this.lock; lock.lock(); try{ Object[]elements=getArray(); int len=elements.length; Object[]newElements=Arrays.copyOf(elements,len+1); newElements[len]=e; // 然后把副本数组赋值给volatile修饰的变量 setArray(newElements); return true; }finally{ lock.unlock(); } } ReetrantLock是可重入锁 ，同一时间只有一个线程可以更新,CopyOnWriteArrayList使用可重入锁不适用synchronized的原因是整个CopyOnWriteArrayList使用的是同一把锁，在操作add/set/remove这些方法的时候同一时间只能有一个线程修改成功 可以看到Object[] newElements = Arrays.copyOf(elements, len + 1);new了一个新的数组，由原来的数组copy过来，size设置为原来的数组长度+1. 关键问题，写线程现在把副本数组给修改完了，现在怎么才能让读线程感知到这个变化？===加上volatile关键字（可见性） Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/HashSet详解.html":{"url":"Java-技能/集合/Collection/HashSet详解.html","title":"HashSet详解","keywords":"","body":"1.1. HashSet1.1. HashSet HashSet是线程不安全的 底层使用HashMap,hastSet的add值使用的HashMap的key做存储，如下源代码：/** * Constructs a new, empty set; the backing HashMap instance has * default initial capacity (16) and load factor (0.75). */ public HashSet() { map = new HashMap<>(); } 上面代码是初始化HashSet的时候底层使用HashMap初始化长度16，0.75的加载因子public boolean add(E e) { return map.put(e, PRESENT)==null; } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/Java集合面试.html":{"url":"Java-技能/集合/Collection/Java集合面试.html","title":"Java集合面试","keywords":"","body":" Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/Map详解.html":{"url":"Java-技能/集合/Collection/Map详解.html","title":"Map详解","keywords":"","body":"1.1. Map详解1.1. Map详解 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Collection/Queue.html":{"url":"Java-技能/集合/Collection/Queue.html","title":"Queue","keywords":"","body":"1. Queue1.1. ConcurrentLinkedQueue1.2. ConcurrentLinkedDeque1.3. BlockingQueue1. Queue //TODO: 1.1. ConcurrentLinkedQueue 1.2. ConcurrentLinkedDeque 1.3. BlockingQueue Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/集合/Fail-Fast.html":{"url":"Java-技能/集合/Fail-Fast.html","title":"Fail Fast","keywords":"","body":"1. Fail-fast1. Fail-fast Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"Java-技能/常用lambda表达式.html":{"url":"Java-技能/常用lambda表达式.html","title":"常用lambda表达式","keywords":"","body":"1. lambda表达式汇总1.1. 去重1. lambda表达式汇总 1.1. 去重 List lists = list.stream().collect( Collectors.collectingAndThen( Collectors.toCollection( () -> new TreeSet<>(Comparator.comparing(AffiliationLocationDTO::getCreationDate)) ), ArrayList::new ) ); Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"linux/":{"url":"linux/","title":"Linux","keywords":"","body":"1. Linux系统1. Linux系统 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"linux/centos/基础操作命令.html":{"url":"linux/centos/基础操作命令.html","title":"基础操作命令","keywords":"","body":" 查看系统服务进程 ps aux 指定进程查看 ps -ef | grep jar,ps -aux|grep sendmail 查看内存使用情况 free -m 清除使用缓存 echo 1 > /proc/sys/vm/drop_caches `echo 2 > /proc/sys/vm/drop_caches echo 3 > /proc/sys/vm/drop_caches linux关闭防火墙 systemctl stop firewalld.service #停止firewall systemctl disable firewalld.service #禁止firewall开机启动 firewall-cmd --state #查看默认防火墙状态（关闭后显示notrunning，开启后显示running） netstat -anp|grep 21001(查看端口在被哪个进程使用) Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/":{"url":"中间件/","title":"中间件","keywords":"","body":"1.1. 中间件1.1. 中间件 es redis postgressql RabbitMQ Kafka Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/Es-Nested嵌套查询.html":{"url":"中间件/ElasticSearch/Es-Nested嵌套查询.html","title":"Es-Nested嵌套查询","keywords":"","body":"1.1. ES-嵌套查询1.1.1. 背景1.1. ES-嵌套查询 ES索引结构本身存储是扁平化存储，如下例子 1.1.1. 背景 看如下示例,一个订单信息，对应多个费用项数据 { \"orderNumber\": \"YY2201-12345678\", \"remark\": \"这里是备注\", \"waybillNumbers\": [\"YD2201-12345678\", \"YD2201-12345679\"], \"creationDate\": 1663658432000, \"costItemInfos\": [ { \"name\": \"扣款1\", \"amount\": 34, \"fromSource\": 8, \"comment\": \"因为啥扣款\" }, { \"name\": \"扣款2\", \"amount\": 38, \"fromSource\": 9, \"comment\": \"因为啥扣款2\" }, { \"name\": \"补贴1\", \"amount\": 33, \"fromSource\": 7, \"comment\": \"因为啥扣款3\" } ] } 问题 如果我们现在想查询{\"name\": \"扣款2\",\"amount\":34}的订单，发现依然可以把上面的数据查询出来，然而实际上，我们并不存在扣款2 金额34的费用项。 GET /order/_search?pretty { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"costItemInfos.name\": \"扣款2\" } }, { \"match\": { \"costItemInfos.amount\": 34 } } ] } } } 原因分析 因为ES（lucene）存储结构是扁平化存储，如示例的文档在es内存储结构实际上是这样的： { \"orderNumber\": [ YY2201-12345678, YY2201-12345679 ], \"remark\": [ 这里是备注,这里是备注2 ], \"waybillNumbers\": [ \"YD2201-12345678\", \"YD2201-12345679\" ], \"creationDate\": [ 1663658432000 ] \"costItemInfos.name\": [ 扣款1,扣款2,扣款3 ], \"costItemInfos.amount\": [ 33, 34, 38 ], \"costItemInfos.fromSource\": [ 7, 8, 9 ], \"costItemInfos.comment\": [ 因为啥扣款,因为啥扣款2,因为啥扣款3 ] } 所以根据金额以及费用项名称来查询是可以匹配到的，然而费用项名称与金额的关系已经不存在 如何解决？ 解决只需要将costItemInfos的类型指定为Nested类型即可 PUT /order_new { \"mappings\": { \"order\": { \"properties\": { \"orderNumber\": { \"type\": \"keyword\" }, \"remark\": { \"fields\": { \"raw\": { \"null_value\": \"\", \"type\": \"keyword\" } }, \"type\": \"text\" }, \"waybillNumbers\": { \"type\": \"keyword\" }, \"creationDate\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" }, \"costItemInfos\": { \"type\": \"nested\", \"properties\": { \"name\": { \"type\": \"text\" }, \"amount\": { \"type\": \"double\" }, \"fromSource\": { \"type\": \"keyword\" }, \"comment\": { \"type\": \"text\" } } } } } } } 那么相对应的查询方式做一下改变，使用Nested查询 GET /blog_new/_search?pretty { \"query\": { \"bool\": { \"must\": [ { \"nested\": { \"path\": \"costItemInfos\", \"query\": { \"bool\": { \"must\": [ { \"match\": { \"costItemInfos.name\": \"扣款1\" } }, { \"match\": { \"costItemInfos.amount\": 34 } } ] } } } } ] } } } 扩展，高阶查询，nested聚合分组统计{ \"query\": { \"bool\": { \"must\": [ { \"terms\": { \"userId\": [ 12312312312312 ] } } ] } }, \"aggs\": { \"itemNest\": { \"nested\": { \"path\": \"costItemInfos\" }, \"aggs\": { \"costItemCodeGroup\": { \"terms\": { \"field\": \"costItemInfos.costCode\" }, \"aggs\": { \"amount\": { \"sum\": { \"field\": \"costItemInfos.amount\" } }, \"orderCount\": { \"value_count\": { \"field\": \"_id\" } } } } } } } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/Es7.4.2-RestHighLevelClient增删改查.html":{"url":"中间件/ElasticSearch/Es7.4.2-RestHighLevelClient增删改查.html","title":"Es7.4.2-RestHighLevelClient增删改查","keywords":"","body":"1.1.1. java操作es方式1.1.2. java使用方式1.1.1. java操作es方式 http 操作es使用的方式为http方式，需要springboot的pom依赖，我使用的版本为：7.4.2，es对应的版本为：7.4.2，springboot的版本为：2.2.1.RELEASE 1.1.2. java使用方式 引入pom依赖 org.elasticsearch elasticsearch org.elasticsearch.client elasticsearch-rest-client org.elasticsearch.client elasticsearch-rest-high-level-client es配置 elasticsearch.port=9200 elasticsearch.username=elastic elasticsearch.password=123 elasticsearch.cluster.address=http://p.es.net elasticsearch.shards=1 elasticsearch.replicas=0 elasticsearch.connect_timeout=5000 elasticsearch.socket_timeout=60000 ElasticSearchConfig.class package com.dadi01.scrm.service.mot.provider.config; import com.dadi01.scrm.foundation.model.constant.StringPool; import lombok.extern.slf4j.Slf4j; import org.apache.http.Header; import org.apache.http.HttpHost; import org.apache.http.auth.AuthScope; import org.apache.http.auth.UsernamePasswordCredentials; import org.apache.http.client.CredentialsProvider; import org.apache.http.impl.client.BasicCredentialsProvider; import org.apache.http.message.BasicHeader; import org.elasticsearch.client.*; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import java.util.ArrayList; import java.util.List; /** * @author lviter */ @Slf4j @Configuration public class ElasticSearchConfig { @Value(\"${elasticsearch.cluster.address}\") private String clusterAddress; @Value(\"${elasticsearch.username}\") private String username; @Value(\"${elasticsearch.password}\") private String password; @Value(\"${elasticsearch.shards}\") private Integer numberOfShards; @Value(\"${elasticsearch.replicas}\") private Integer numberOfReplicas; @Value(\"${elasticsearch.connect_timeout}\") private Long connectTimeout; @Value(\"${elasticsearch.socket_timeout}\") private Long socketTimeout; public static RestHighLevelClient client = null; public Integer getNumberOfShards() { return numberOfShards; } public Integer getNumberOfReplicas() { return numberOfReplicas; } /** * @return 连接es */ @Bean public RestHighLevelClient restClient() { final CredentialsProvider credentialsProvider = new BasicCredentialsProvider(); credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(username, password)); Header[] defaultHeaders = {new BasicHeader(\"content-type\", \"application/json\")}; RestClientBuilder restClientBuilder = RestClient.builder(HttpHost.create(clusterAddress)); restClientBuilder .setHttpClientConfigCallback(httpAsyncClientBuilder -> httpAsyncClientBuilder.setDefaultCredentialsProvider(credentialsProvider)) .setDefaultHeaders(defaultHeaders) .setRequestConfigCallback(requestConfigBuilder -> { // 连接5秒超时，套接字连接60s超时 return requestConfigBuilder.setConnectTimeout(connectTimeout.intValue()).setSocketTimeout(socketTimeout.intValue()); }) .setHttpClientConfigCallback(httpClientBuilder -> { httpClientBuilder.disableAuthCaching(); return httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider); }); client = new RestHighLevelClient(restClientBuilder); return client; } private List createHttpHostList() { List hostList = new ArrayList<>(); String[] hostNamesPort; if (!clusterAddress.contains(StringPool.COMMA)) { hostNamesPort = new String[]{clusterAddress}; } else { hostNamesPort = clusterAddress.split(\",\"); } for (String host : hostNamesPort) { hostList.add(new HttpHost(host.substring(0, host.indexOf(StringPool.COLON)), Integer.parseInt(host.substring(host.indexOf(StringPool.COLON) + 1)))); } return hostList; } } EsSettingsConstant.class package com.dadi01.scrm.service.mot.provider.constant; /** * @author lviter */ public class EsSettingsConstant { /** * 数据分片数 */ public final static String NUMBER_OF_SHARDS = \"index.number_of_shards\"; /** * 数据备份数 */ public final static String NUMBER_OF_REPLICAS = \"index.number_of_replicas\"; /** * 分页查询es限制最大条数 */ public final static String MAX_RESULT_WINDOW = \"index.max_result_window\"; /** * 最大一亿 */ public final static String MAX_RESULT_WINDOW_VALUE = \"100000000\"; } ElasticSearchServiceImpl.class es通用增删改查，分页等 package com.dadi01.scrm.service.mot.provider.impl; import com.alibaba.fastjson.JSON; import com.dadi01.scrm.foundation.model.constant.StringPool; import com.dadi01.scrm.foundation.model.dto.PageData; import com.dadi01.scrm.foundation.model.dto.ResultDTO; import com.dadi01.scrm.foundation.model.dto.ResultListDTO; import com.dadi01.scrm.foundation.model.dto.ResultPageDTO; import com.dadi01.scrm.foundation.model.error.ErrorEnum; import com.dadi01.scrm.foundation.model.exception.ScrmException; import com.dadi01.scrm.service.mot.api.IElasticSearchService; import com.dadi01.scrm.service.mot.api.common.EsLogActionEnum; import com.dadi01.scrm.service.mot.api.dto.elasticsearch.CrowdMessageDTO; import com.dadi01.scrm.service.mot.api.dto.elasticsearch.MemberDTO; import com.dadi01.scrm.service.mot.api.dto.elasticsearch.OperatingLogDTO; import com.dadi01.scrm.service.mot.provider.config.ElasticSearchConfig; import com.dadi01.scrm.service.mot.provider.constant.EsIndexConstant; import com.dadi01.scrm.service.mot.provider.constant.EsSettingsConstant; import com.dadi01.scrm.service.mot.provider.util.JsonUtils; import com.google.gson.Gson; import lombok.extern.slf4j.Slf4j; import org.apache.commons.lang3.StringUtils; import org.elasticsearch.action.admin.indices.alias.get.GetAliasesRequest; import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest; import org.elasticsearch.action.admin.indices.get.GetIndexRequest; import org.elasticsearch.action.bulk.BulkRequest; import org.elasticsearch.action.delete.DeleteRequest; import org.elasticsearch.action.delete.DeleteResponse; import org.elasticsearch.action.get.GetRequest; import org.elasticsearch.action.get.GetResponse; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.search.MultiSearchRequest; import org.elasticsearch.action.search.MultiSearchResponse; import org.elasticsearch.action.search.SearchRequest; import org.elasticsearch.action.search.SearchResponse; import org.elasticsearch.action.support.master.AcknowledgedResponse; import org.elasticsearch.action.update.UpdateRequest; import org.elasticsearch.action.update.UpdateResponse; import org.elasticsearch.client.GetAliasesResponse; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.client.indices.CreateIndexRequest; import org.elasticsearch.client.indices.CreateIndexResponse; import org.elasticsearch.cluster.metadata.AliasMetaData; import org.elasticsearch.common.settings.Settings; import org.elasticsearch.common.xcontent.XContentType; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.search.SearchHit; import org.elasticsearch.search.builder.SearchSourceBuilder; import org.elasticsearch.search.sort.SortOrder; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.scheduling.annotation.Async; import org.springframework.stereotype.Service; import java.io.IOException; import java.lang.reflect.Field; import java.util.*; import java.util.concurrent.atomic.AtomicLong; /** * @author lviter */ @Slf4j @Service public class ElasticSearchServiceImpl implements IElasticSearchService { @Autowired private RestHighLevelClient restHighLevelClient; @Autowired private ElasticSearchConfig elasticSearchConfig; private static AtomicLong i = new AtomicLong(0); @Override public ResultDTO getElasticSearchInfo() { SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); // SearchRequest SearchRequest searchRequest = new SearchRequest(); searchRequest.source(searchSourceBuilder); // 查询ES SearchResponse searchResponse = null; try { searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es查询异常{}\", JsonUtils.objectToJson(e)); } return ResultDTO.success(searchResponse); } @Override public ResultDTO addCrowdMessage(String index, CrowdMessageDTO crowdMessageDTO) { if (StringUtils.isBlank(index)) { throw new ScrmException(ErrorEnum.MOT_ES_INDEX_NOT_NULL.build()); } IndexRequest indexRequest = new IndexRequest(index); Long createTime = System.currentTimeMillis(); crowdMessageDTO.setCreateTime(createTime); String source = JSON.toJSONString(crowdMessageDTO); try { indexRequest.source(source, XContentType.JSON); restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es add data filed{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_ADD_DATA_FAILED.build()); } addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.INSERT, index, new Gson().toJson(source)); return ResultDTO.success(true); } /** * 批量添加es数据 * * @param crowdMessages * @return */ @Override public ResultDTO addBatchCrowdMessage(String index, List crowdMessages) { if (crowdMessages.size() > 100000) { log.error(\"es add batch data too large{}\", crowdMessages.size()); throw new ScrmException(ErrorEnum.MOT_ES_ADD_DATA_FAILED.build()); } BulkRequest request = new BulkRequest(); crowdMessages.forEach(crowdMessageDTO -> { crowdMessageDTO.setCreateTime(System.currentTimeMillis()); crowdMessageDTO.setSort(i.getAndIncrement()); String source = JSON.toJSONString(crowdMessageDTO); request.add(new IndexRequest(index).source(source, XContentType.JSON)); }); esBatchAdd(request, index); return ResultDTO.success(); } @Override public ResultDTO addBatchMember(String index, List members) { if (members.size() > 100000) { log.error(\"es add batch data too large{}\", members.size()); throw new ScrmException(ErrorEnum.MOT_ES_ADD_DATA_FAILED.build()); } BulkRequest request = new BulkRequest(); members.forEach(member -> { member.setCreateTime(System.currentTimeMillis()); String source = JSON.toJSONString(member); request.add(new IndexRequest(index).source(source, XContentType.JSON)); }); esBatchAdd(request, index); return ResultDTO.success(); } /** * 批量插入数据 * * @param bulkRequest * @param index */ private void esBatchAdd(BulkRequest bulkRequest, String index) { try { restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT); } catch (Exception e) { log.error(\"es add batch data filed{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_ADD_DATA_FAILED.build()); } log.info(\"插入数据----------------------{}\", bulkRequest.requests().size()); addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.INSERT, index, String.valueOf(bulkRequest.requests().size())); } /** * 创建索引 * * @param index */ @Override public ResultDTO createIndexResponse(String index) { //创建索引,如果索引已存在，返回错误信息 if (checkIndexExists(index)) { log.info(\"索引已存在{}\", index); throw new ScrmException(ErrorEnum.MOT_ES_INDEX_ALREADY_EXIST.build()); } //创建索引 CreateIndexRequest createIndexRequest = new CreateIndexRequest(index); //设置分片 createIndexRequest.settings( Settings.builder().put(EsSettingsConstant.NUMBER_OF_SHARDS, elasticSearchConfig.getNumberOfShards()) .put(EsSettingsConstant.NUMBER_OF_REPLICAS, elasticSearchConfig.getNumberOfReplicas()) .put(EsSettingsConstant.MAX_RESULT_WINDOW, EsSettingsConstant.MAX_RESULT_WINDOW_VALUE)); CreateIndexResponse createIndexResponse = null; try { createIndexResponse = restHighLevelClient.indices().create(createIndexRequest, RequestOptions.DEFAULT); boolean acknowledged = createIndexResponse.isAcknowledged(); boolean shardsAcknowledged = createIndexResponse.isShardsAcknowledged(); if (acknowledged && shardsAcknowledged) { addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.CREATE, index, new Gson().toJson(createIndexResponse)); log.info(\"索引创建成功{}\", index); } } catch (IOException e) { log.error(\"index create failed{}\", JsonUtils.objectToJson(e)); addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.CREATE, index, new Gson().toJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_INDEX_CREATE_FAILED.build()); } return ResultDTO.success(); } /** * 判断索引是否存在 * * @param indexName * @return * @throws IOException */ public boolean checkIndexExists(String indexName) { GetIndexRequest request = new GetIndexRequest().indices(indexName); try { return restHighLevelClient.indices().exists(request, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"判断索引是否存在，操作异常！\"); } return false; } @Override public ResultPageDTO pageQuery(Integer page, Integer rows, String index, Integer status) { SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder() .from((page - 1) * rows) .size(rows) .sort(\"sort\", SortOrder.DESC) .trackTotalHits(true); if (status != null) { searchSourceBuilder.query(QueryBuilders.termQuery(\"status\", status)); } SearchResponse searchResponse = pageQuerySearchResponse(searchSourceBuilder, index); long total = searchResponse.getHits().getTotalHits().value; // 遍历封装列表对象 List crowdMessages = new ArrayList<>(); SearchHit[] searchHits = searchResponse.getHits().getHits(); for (SearchHit searchHit : searchHits) { crowdMessages.add(JSON.parseObject(searchHit.getSourceAsString(), CrowdMessageDTO.class).setId(searchHit.getId())); } return ResultPageDTO.success(new PageData().setData(crowdMessages).setPageSize(crowdMessages.size()).setTotal((int) total)); } @Override public ResultPageDTO> pageQueryByIndex(Integer page, Integer rows, String index) { SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder() .from((page - 1) * rows) .size(rows) .trackTotalHits(true); SearchResponse searchResponse = pageQuerySearchResponse(searchSourceBuilder, index); long total = searchResponse.getHits().getTotalHits().value; List> resultList = new ArrayList<>(); // 遍历封装列表对象 SearchHit[] searchHits = searchResponse.getHits().getHits(); for (SearchHit searchHit : searchHits) { searchHit.getSourceAsMap().put(\"id\", searchHit.getId()); resultList.add(searchHit.getSourceAsMap()); } return ResultPageDTO.success(new PageData>().setData(resultList).setPageSize(resultList.size()).setTotal((int) total)); } /** * 分页查询搜索es * * @param searchSourceBuilder * @param index * @return */ private SearchResponse pageQuerySearchResponse(SearchSourceBuilder searchSourceBuilder, String index) { SearchRequest searchRequest = new SearchRequest() .source(searchSourceBuilder) .indices(index); SearchResponse searchResponse; try { log.info(\"查询es入参：{}\", new Gson().toJson(searchRequest)); searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es page query failed{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_PAGE_QUERY_FAILED.build()); } return searchResponse; } @Override public ResultDTO getById(String index, String id) { if (StringUtils.isBlank(index)) { index = EsIndexConstant.ES_TEST.getIndex(); } GetRequest getRequest = new GetRequest(index, id); GetResponse getResponse = null; try { log.info(\"根据编号查询数据，rq:{}\", new Gson().toJson(getRequest)); getResponse = restHighLevelClient.get(getRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es查询异常{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_QUERY_FAILED.build()); } CrowdMessageDTO crowdMessageDTO = JSON.parseObject(getResponse.getSourceAsString(), CrowdMessageDTO.class).setId(getResponse.getId()); return ResultDTO.success(crowdMessageDTO); } @Override public ResultListDTO getMemberList(List memberIds) { List memberList = new ArrayList<>(); MultiSearchRequest request = new MultiSearchRequest(); memberIds.forEach(memberId -> { SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder().query(QueryBuilders.matchPhraseQuery(\"memberId\", memberId)); request.add(new SearchRequest() .source(searchSourceBuilder) .indices(EsIndexConstant.ES_MEMBER_INFO.getIndex())); }); try { MultiSearchResponse response = restHighLevelClient.msearch(request, RequestOptions.DEFAULT); for (MultiSearchResponse.Item item : response.getResponses()) { log.info(JsonUtils.objectToJson(item)); for (SearchHit hit : item.getResponse().getHits().getHits()) { memberList.add(JsonUtils.jsonToPojo(hit.getSourceAsString(), MemberDTO.class)); } } } catch (IOException e) { log.error(\"es查询异常{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_QUERY_FAILED.build()); } return ResultListDTO.success(memberList); } @Override public ResultDTO update(String index, CrowdMessageDTO crowdMessageDTO) { if (StringUtils.isBlank(index)) { throw new ScrmException(ErrorEnum.MOT_ES_INDEX_NOT_NULL.build()); } UpdateRequest updateRequest = new UpdateRequest(index, crowdMessageDTO.getId()); updateRequest.retryOnConflict(3); updateRequest.doc(JSON.toJSONString(crowdMessageDTO), XContentType.JSON); // 操作ES UpdateResponse updateResponse = null; try { log.info(\"更新数据,rq:{}\", new Gson().toJson(updateRequest)); updateResponse = restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es update failed{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_UPDATE_FAILED.build()); } return ResultDTO.success(updateResponse); } @Override @Async public ResultDTO updateBatch(String index, List crowdMessages) { batchUpdate(index, crowdMessages); addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.UPDATE, index, String.valueOf(crowdMessages.size())); return ResultDTO.success(); } /** * 批量修改 * * @param index * @param crowdMessages */ private void batchUpdate(String index, List crowdMessages) { BulkRequest bulkRequest = new BulkRequest(); crowdMessages.forEach(crowdMessageDTO -> bulkRequest.add(new UpdateRequest(index, crowdMessageDTO.getId()).doc(JSON.toJSONString(crowdMessageDTO), XContentType.JSON))); try { restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es add batch data filed{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_UPDATE_FAILED.build()); } } @Override public ResultDTO deleteById(String index, String id) { if (StringUtils.isBlank(index)) { index = EsIndexConstant.ES_TEST.getIndex(); } DeleteRequest deleteRequest = new DeleteRequest(index, id); // 操作ES DeleteResponse deleteResponse = null; try { log.info(\"删除数据根据ID,rq:{}\", new Gson().toJson(deleteRequest)); deleteResponse = restHighLevelClient.delete(deleteRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"es查询异常{}\", JsonUtils.objectToJson(e)); e.printStackTrace(); } return ResultDTO.success(deleteResponse); } @Override public ResultDTO deleteIndex(String index) { DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(index); try { AcknowledgedResponse deleteResponse = restHighLevelClient.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); boolean acknowledged = deleteResponse.isAcknowledged(); if (acknowledged) { return ResultDTO.success(); } addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.DELETE, index, new Gson().toJson(deleteResponse)); } catch (IOException e) { log.error(\"es delete index failed{}\", JsonUtils.objectToJson(e)); addOperatingLog(EsIndexConstant.ES_OPERATING_LOG.getIndex(), EsLogActionEnum.DELETE, index, new Gson().toJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_INDEX_DELETE_FAIL.build()); } return ResultDTO.success(); } @Override public ResultDTO> getAlias() { Set indices; GetAliasesRequest request = new GetAliasesRequest(); try { GetAliasesResponse getAliasesResponse = restHighLevelClient.indices().getAlias(request, RequestOptions.DEFAULT); Map> map = getAliasesResponse.getAliases(); indices = map.keySet(); indices.removeIf(str -> str.startsWith(StringPool.DOT)); return ResultDTO.success(indices); } catch (IOException e) { log.error(\"es get indices failed{}\", JsonUtils.objectToJson(e)); throw new ScrmException(ErrorEnum.MOT_ES_QUERY_FAILED.build()); } } @Override public ResultDTO updateAllByKey(String index, String key, String value) { // UpdateRequest updateRequest = new UpdateRequest(index); // restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); return null; } @Override @Async public ResultDTO addOperatingLog(String index, EsLogActionEnum esLogActionEnum, String operateIndex, String comment) { //创建索引,如果索引不存在，就创建索引 // String index = EsIndexConstant.ES_OPERATING_LOG.getIndex(); if (!checkIndexExists(index)) { createIndexResponse(index); } IndexRequest indexRequest = new IndexRequest(index); OperatingLogDTO operatingLogDTO = new OperatingLogDTO(); operatingLogDTO.setCreateTime(System.currentTimeMillis()); operatingLogDTO.setLogAction(esLogActionEnum.getKey()); operatingLogDTO.setLogModule(operateIndex); operatingLogDTO.setComment(comment); String source = JSON.toJSONString(operatingLogDTO); indexRequest.source(source, XContentType.JSON); try { restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); } catch (IOException e) { log.error(\"add operating log fail!\"); } return ResultDTO.success(); } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/Es基础查询.html":{"url":"中间件/ElasticSearch/Es基础查询.html","title":"Es基础查询","keywords":"","body":"1.1. 查询是否存在字段1.1. 查询是否存在字段 POST /vts_finance_cost_confirmed_order/_search { \"query\": { \"bool\": { \"must\": { \"exists\": { \"field\": \"rentCarOwnOrgIdRoute\" } } } } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/Es基础语法.html":{"url":"中间件/ElasticSearch/Es基础语法.html","title":"Es基础语法","keywords":"","body":"1.1. 示例1.2. 基本概念1.3. 操作1.4. 索引1.1. 示例 1.2. 基本概念 Elasticsearch是面向文档型数据库，一条数据在这里就是一个文档，用JSON作为文档序列化的格式，如: { \"name\" : \"John\", \"sex\" : \"Male\", \"age\" : 25, \"birthDate\": \"1990/05/01\", \"about\" : \"I love to go rock climbing\", \"interests\": [ \"sports\", \"music\" ] } es与关系型数据术语对照表： 关系数据库 ⇒ 数据库 ⇒ 表 ⇒ 行 ⇒ 列(Columns) Elasticsearch ⇒ 索引(Index) ⇒ 类型(type) ⇒ 文档(Docments) ⇒ 字段(Fields) 一个es集群包含多个索引，包含很多类型。这些类型中包含很多的文档，每个文档包含很多的字段。ES的交互，可以使用JAVA API，也可以使用HTTP的restful API方式。 1.3. 操作 创建文档类型的索引 PUT /website/blog/123 { \"title\": \"My first blog entry\", \"text\": \"Just trying this out...\", \"date\": \"2014/01/01\" } website: 文档存放位置 blog: 文档表示的对象类型 123: 文档唯一标识 查询 GET /website/blog/123?_source=title,text 删除DELETE /website/blog/123 1.4. 索引 ES索引精髓：一切设计都是为了提高搜索的性能 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/es复杂统计-绝对值.html":{"url":"中间件/ElasticSearch/es复杂统计-绝对值.html","title":"es复杂统计-绝对值","keywords":"","body":"1.1. 复杂统计示例1.2. java代码示例1.3. 核心点1.1. 复杂统计示例 POST /vts_finance_insurance_policy/_search { \"query\": { \"bool\": { \"filter\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"term\": { \"enabledFlag\": { \"value\": \"1\", \"boost\": 1.0 } } }, { \"term\": { \"tmsBatchNumber\": { \"value\": \"PC24041600006\", \"boost\": 1.0 } } }, { \"range\": { \"bidTime\": { \"from\": null, \"to\": \"1715737414000\", \"include_lower\": true, \"include_upper\": false, \"boost\": 1.0 } } }, { \"bool\": { \"must_not\": [ { \"term\": { \"bidTime\": { \"value\": \"-62167420800000\", \"boost\": 1.0 } } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"_source\": { \"includes\": [], \"excludes\": [] }, \"aggregations\": { \"totalInsureAmount\": { \"sum\": { \"script\": { \"source\": \"Math.abs(doc['premiumAmount'].value)\", \"lang\": \"painless\" } } }, \"totalDeductionAmount\": { \"sum\": { \"script\": { \"source\": \"Math.abs(doc['deductionAmount'].value)\", \"lang\": \"painless\" } } }, \"unInsuredNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"policyStatus\": [ \"10\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"insuredNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"policyStatus\": [ \"20\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"insureCancelNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"policyStatus\": [ \"30\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"insureFailNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"policyStatus\": [ \"40\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"insureCancelFailNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"policyStatus\": [ \"50\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"pendingPusNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"tmsBillStatus\": [ \"100\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"tmsApproveNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"tmsBillStatus\": [ \"200\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"tmsConfirmNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"tmsBillStatus\": [ \"300\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } }, \"tmsPaiNum\": { \"filter\": { \"bool\": { \"must\": [ { \"terms\": { \"tmsBillStatus\": [ \"400\" ], \"boost\": 1.0 } }, { \"terms\": { \"enabledFlag\": [ \"1\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, \"aggregations\": { \"insuranceOrderNumber\": { \"value_count\": { \"field\": \"id\" } } } } } } 1.2. java代码示例 import org.elasticsearch.index.query.QueryBuilder; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.script.Script; import org.elasticsearch.script.ScriptType; import org.elasticsearch.search.aggregations.AggregationBuilder; import org.elasticsearch.search.aggregations.AggregationBuilders; import org.elasticsearch.search.aggregations.metrics.scripted.ScriptedMetricAggregationBuilder; import org.elasticsearch.search.builder.SearchSourceBuilder; import java.util.Collections; /** * @Description: * @author: 582401 * @date: 2022/4/1 */ public class InsurancePolicyPreSearch extends BaseGSPreSearch { /** * 待投保数量 */ public static final String UN_INSURED_NUM = \"unInsuredNum\"; /** * 已投保数量 */ public static final String INSURED_NUM = \"insuredNum\"; /** * 取消投保数量 */ public static final String INSURED_CANCEL_NUM = \"insureCancelNum\"; /** * 投保失败数量 */ public static final String INSURE_FAIL_NUM = \"insureFailNum\"; /** * 退保失败数量 */ public static final String INSURE_CANCEL_FAIL_NUM = \"insureCancelFailNum\"; /** * 待推送TMS数量 */ public static final String PENDING_PUS_NUM = \"pendingPusNum\"; /** * 待对账数量 */ public static final String TMS_APPROVE_NUM = \"tmsApproveNum\"; /** * 已对账数量 */ public static final String TMS_CONFIRM_NUM = \"tmsConfirmNum\"; /** * 已支付数量 */ public static final String TMS_PAI_NUM = \"tmsPaiNum\"; /** * 订单数量 */ public static final String ORDER_NUMBER = \"insuranceOrderNumber\"; /** * 保单状态分组 */ public static final String POLICY_STATUS_GROUP = \"policyStatusGroup\"; /** * 投保总金额 */ public static final String TOTAL_INSURE_AMOUNT = \"totalInsureAmount\"; /** * 扣款总金额 */ public static final String TOTAL_DEDUCTION_AMOUNT = \"totalDeductionAmount\"; @Override public void buildAggregation(SearchSourceBuilder sourceBuilder) { //待投保订单数 QueryBuilder unInsuredNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"policyStatus\", \"10\")) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder unInsuredNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.UN_INSURED_NUM, unInsuredNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //已投保订单数 QueryBuilder insuredNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"policyStatus\", \"20\")) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder insuredNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.INSURED_NUM, insuredNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //取消投保数量 QueryBuilder insureCancelNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"policyStatus\", \"30\")) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder insureCancelNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.INSURED_CANCEL_NUM, insureCancelNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //投保失败数量insureFailNum QueryBuilder insureFailNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"policyStatus\", \"40\")) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder insureFailNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.INSURE_FAIL_NUM, insureFailNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //退保失败数量insureCancelFailNum QueryBuilder insureCancelFailNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"policyStatus\", \"50\")) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder insureCancelFailNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.INSURE_CANCEL_FAIL_NUM, insureCancelFailNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //待推送TMS数量pendingPusNum QueryBuilder pendingPusNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"tmsBillStatus\", TmsBillStatusEnum.PENDING_PUSH.getCode())) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder pendingPusNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.PENDING_PUS_NUM, pendingPusNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //待对账数量tmsApproveNum QueryBuilder tmsApproveNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"tmsBillStatus\", TmsBillStatusEnum.TMS_APPROVE.getCode())) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder tmsApproveNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.TMS_APPROVE_NUM, tmsApproveNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //已对账数量tmsConfirmNum QueryBuilder tmsConfirmNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"tmsBillStatus\", TmsBillStatusEnum.TMS_CONFIRM.getCode())) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder tmsConfirmNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.TMS_CONFIRM_NUM, tmsConfirmNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); //已支付数量tmsPaiNum QueryBuilder tmsPaiNumQuery = QueryBuilders.boolQuery() .must(QueryBuilders.termsQuery(\"tmsBillStatus\", TmsBillStatusEnum.TMS_PAID.getCode())) .must(QueryBuilders.termsQuery(\"enabledFlag\", EnabledFlagEnum.YES.getCode())); AggregationBuilder tmsPaiNumBuilder = AggregationBuilders.filter(InsurancePolicyPreSearch.TMS_PAI_NUM, tmsPaiNumQuery) .subAggregation(AggregationBuilders.count(InsurancePolicyPreSearch.ORDER_NUMBER).field(\"id\")); sourceBuilder .aggregation(AggregationBuilders.sum(InsurancePolicyPreSearch.TOTAL_INSURE_AMOUNT) .script(new Script(ScriptType.INLINE, \"painless\", \"Math.abs(doc['premiumAmount'].value)\", Collections.emptyMap())) ) .aggregation(AggregationBuilders.sum(InsurancePolicyPreSearch.TOTAL_DEDUCTION_AMOUNT) .script(new Script(ScriptType.INLINE, \"painless\", \"Math.abs(doc['deductionAmount'].value)\", Collections.emptyMap())) ) .aggregation(unInsuredNumBuilder) .aggregation(insuredNumBuilder) .aggregation(insureCancelNumBuilder) .aggregation(insureFailNumBuilder) .aggregation(insureCancelFailNumBuilder) .aggregation(pendingPusNumBuilder) .aggregation(tmsApproveNumBuilder) .aggregation(tmsConfirmNumBuilder) .aggregation(tmsPaiNumBuilder); } } 查询取值代码 public InsurancePolicyStatisticsDTO statisticsInsurancePolicy(InsurancePolicyBO insurancePolicyBO) { Pagination pagination = (Pagination) esPreHandleUtils.newPagination(insurancePolicyBO); //组装es通用查询条件 String authCode = KeyValueConfigClient.getValue(VtsBaseConfigConstant.INSURANCE_STATISTICS_AUTH_CODE, EsConstant.INSURANCE_POLICY_AUTH_CODE); SimpleQuery simpleQuery = getSimpleQuery(pagination, authCode); SearchResponse searchResponse = esPreHandleUtils.getEsGroupStatisticsClient().searchResult(simpleQuery, new InsurancePolicyPreSearch()); InsurancePolicyStatisticsDTO insurancePolicyStatisticsDTO = new InsurancePolicyStatisticsDTO(); //待投保数量 ParsedFilter unInsuredNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.UN_INSURED_NUM); insurancePolicyStatisticsDTO.setUnInsuredNum(Objects.nonNull(unInsuredNum) ? Integer.parseInt(String.valueOf(unInsuredNum.getDocCount())) : 0); //已投保数量 ParsedFilter insuredNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.INSURED_NUM); insurancePolicyStatisticsDTO.setInsuredNum(Objects.nonNull(insuredNum) ? Integer.parseInt(String.valueOf(insuredNum.getDocCount())) : 0); //取消投保数量 ParsedFilter insureCancelNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.INSURED_CANCEL_NUM); insurancePolicyStatisticsDTO.setInsureCancelNum(Objects.nonNull(insureCancelNum) ? Integer.parseInt(String.valueOf(insureCancelNum.getDocCount())) : 0); //投保失败数量 ParsedFilter insureFailNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.INSURE_FAIL_NUM); insurancePolicyStatisticsDTO.setInsureFailNum(Objects.nonNull(insureFailNum) ? Integer.parseInt(String.valueOf(insureFailNum.getDocCount())) : 0); //退保失败数量 ParsedFilter insureCancelFailNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.INSURE_CANCEL_FAIL_NUM); insurancePolicyStatisticsDTO.setInsureCancelFailNum(Objects.nonNull(insureCancelFailNum) ? Integer.parseInt(String.valueOf(insureCancelFailNum.getDocCount())) : 0); //待推送TMS数量 ParsedFilter pendingPusNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.PENDING_PUS_NUM); insurancePolicyStatisticsDTO.setPendingPusNum(Objects.nonNull(pendingPusNum) ? Integer.parseInt(String.valueOf(pendingPusNum.getDocCount())) : 0); //待对账数量 ParsedFilter tmsApproveNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.TMS_APPROVE_NUM); insurancePolicyStatisticsDTO.setTmsApproveNum(Objects.nonNull(tmsApproveNum) ? Integer.parseInt(String.valueOf(tmsApproveNum.getDocCount())) : 0); //已对账数量 ParsedFilter tmsConfirmNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.TMS_CONFIRM_NUM); insurancePolicyStatisticsDTO.setTmsConfirmNum(Objects.nonNull(tmsConfirmNum) ? Integer.parseInt(String.valueOf(tmsConfirmNum.getDocCount())) : 0); //已支付数量 ParsedFilter tmsPaiNum = searchResponse.getAggregations().get(InsurancePolicyPreSearch.TMS_PAI_NUM); insurancePolicyStatisticsDTO.setTmsPaiNum(Objects.nonNull(tmsPaiNum) ? Integer.parseInt(String.valueOf(tmsPaiNum.getDocCount())) : 0); //投保总金额 ParsedSum totalInsureAmount = searchResponse.getAggregations().get(InsurancePolicyPreSearch.TOTAL_INSURE_AMOUNT); BigDecimal policyTotalAmount = (Objects.nonNull(totalInsureAmount)) ? BigDecimal.valueOf(Math.abs(totalInsureAmount.getValue())).setScale(2, RoundingMode.HALF_UP) : BigDecimal.ZERO; insurancePolicyStatisticsDTO.setPolicyTotalAmount(policyTotalAmount); //扣款总金额 ParsedSum totalDeductionAmount = searchResponse.getAggregations().get(InsurancePolicyPreSearch.TOTAL_DEDUCTION_AMOUNT); BigDecimal deductionTotalAmount = (Objects.nonNull(totalDeductionAmount)) ? BigDecimal.valueOf(Math.abs(totalDeductionAmount.getValue())).setScale(2, RoundingMode.HALF_UP) : BigDecimal.ZERO; insurancePolicyStatisticsDTO.setDeductionTotalAmount(deductionTotalAmount); return insurancePolicyStatisticsDTO; } 1.3. 核心点 sourceBuilder .aggregation(AggregationBuilders.sum(InsurancePolicyPreSearch.TOTAL_INSURE_AMOUNT) .script(new Script(ScriptType.INLINE, \"painless\", \"Math.abs(doc['premiumAmount'].value)\", Collections.emptyMap())) ) .aggregation(AggregationBuilders.sum(InsurancePolicyPreSearch.TOTAL_DEDUCTION_AMOUNT) .script(new Script(ScriptType.INLINE, \"painless\", \"Math.abs(doc['deductionAmount'].value)\", Collections.emptyMap())) ); 字段未double类型，用脚本统计绝对值的聚合值 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/es通用查询.html":{"url":"中间件/ElasticSearch/es通用查询.html","title":"es通用查询","keywords":"","body":"1.1. 精确查找数据1.2. 判断某个字段是否为空1.3. 复杂查找1.4. 新增字段1.5. 新建索引1.1. 精确查找数据 GET ecs_vts_finance_payable_bill_new/_search { \"query\": { \"term\": { \"tradeNumber\": \"JY2204-30032801\" } } } 1.2. 判断某个字段是否为空 GET /vts_finance_fee_confirmed_order/_search { \"query\": { \"bool\": { \"must_not\": { \"exists\": { \"field\": \"transportOrder\" } } } } } 1.3. 复杂查找 POST ecs_vts_finance_fee_confirmed_order/_search { \"query\": { \"bool\": { \"filter\": [ { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"term\": { \"enabledFlag\": { \"value\": \"1\", \"boost\": 1.0 } } }, { \"bool\": { \"should\": [ { \"bool\": { \"must_not\": [ { \"terms\": { \"transportOrder.transportCapacityType\": [ \"4\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"bool\": { \"should\": [ { \"bool\": { \"must_not\": [ { \"terms\": { \"payableBillDetail.orderSource\": [ \"SHEIN\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"term\": { \"approveStatus\": { \"value\": \"100\", \"boost\": 1.0 } } }, { \"terms\": { \"businessType\": [ \"1\" ], \"boost\": 1.0 } }, { \"bool\": { \"should\": [ { \"terms\": { \"transportOrder.orderStatus\": [ \"50\" ], \"boost\": 1.0 } }, { \"bool\": { \"must\": [ { \"terms\": { \"transportOrder.orderStatus\": [ \"40\" ], \"boost\": 1.0 } }, { \"term\": { \"transportOrder.receiptFlag\": { \"value\": \"10\", \"boost\": 1.0 } } }, { \"range\": { \"transportOrder.signReportDate\": { \"to\": \"1705852799000\", \"include_lower\": true, \"include_upper\": true, \"boost\": 1.0 } } }, { \"bool\": { \"must_not\": [ { \"term\": { \"transportOrder.signReportDate\": { \"value\": \"-62167420800000\", \"boost\": 1.0 } } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"term\": { \"transportOrder.expressStatus\": { \"value\": \"40\", \"boost\": 1.0 } } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"bool\": { \"must\": [ { \"terms\": { \"transportOrder.orderStatus\": [ \"40\" ], \"boost\": 1.0 } }, { \"term\": { \"transportOrder.receiptFlag\": { \"value\": \"10\", \"boost\": 1.0 } } }, { \"term\": { \"transportOrder.expressStatus\": { \"value\": \"60\", \"boost\": 1.0 } } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"bool\": { \"must\": [ { \"terms\": { \"transportOrder.orderStatus\": [ \"40\" ], \"boost\": 1.0 } }, { \"range\": { \"transportOrder.signReportDate\": { \"to\": \"1705852799000\", \"include_lower\": true, \"include_upper\": true, \"boost\": 1.0 } } }, { \"bool\": { \"must_not\": [ { \"term\": { \"transportOrder.signReportDate\": { \"value\": \"-62167420800000\", \"boost\": 1.0 } } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } }, { \"terms\": { \"transportOrder.receiptFlag\": [ \"20\" ], \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } ], \"adjust_pure_negative\": true, \"boost\": 1.0 } } } 1.4. 新增字段 PUT vts_finance_payable_bill/_mapping/ecs_vts_finance_payable_bill { \"properties\": { \"placeOrderBy\": { \"fields\": { \"raw\": { \"null_value\": \"\", \"type\": \"keyword\" } }, \"type\": \"text\" } } } 1.5. 新建索引 PUT vts_finance_fee_confirmed_order { \"aliases\": { \"ecs_vts_finance_fee_confirmed_order\": {} }, \"settings\": { \"index\": { \"refresh_interval\": \"1s\", \"max_inner_result_window\": \"100000\", \"max_result_window\": \"100000\", \"number_of_replicas\": \"2\", \"number_of_shards\": \"5\" } }, \"mappings\": { \"ecs_vts_finance_fee_confirmed_order\": { \"dynamic\": \"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"tradeNumber\": { \"type\": \"keyword\" }, \"demandOrderCode\": { \"type\": \"keyword\" }, \"approveStatus\": { \"type\": \"keyword\" }, \"frozenStatus\": { \"type\": \"keyword\" }, \"payChannel\": { \"type\": \"keyword\" }, \"confirmTime\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"confirmUserName\": { \"type\": \"keyword\" }, \"userName\": { \"type\": \"keyword\" }, \"carModel\": { \"type\": \"keyword\" }, \"userMobile\": { \"type\": \"keyword\" }, \"userType\": { \"type\": \"keyword\" }, \"bidTime\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"bidAmount\": { \"type\": \"double\" }, \"carNumber\": { \"type\": \"keyword\" }, \"carLength\": { \"type\": \"keyword\" }, \"platformCode\": { \"type\": \"keyword\" }, \"businessType\": { \"type\": \"keyword\" }, \"waybillNumbers\": { \"type\": \"keyword\" }, \"enabledFlag\": { \"type\": \"keyword\" }, \"payableBillDetail\": { \"properties\": { \"tradeNumber\": { \"type\": \"keyword\" }, \"confirmedTimeoutStatus\": { \"type\": \"integer\" }, \"confirmedUseTime\": { \"type\": \"integer\" }, \"hllAppealStatus\": { \"type\": \"integer\" }, \"goodsWeight\": { \"type\": \"double\" }, \"goodsVolume\": { \"type\": \"double\" }, \"customerRemark\": { \"type\": \"keyword\" } } }, \"orderTagList\": { \"type\": \"nested\", \"properties\": { \"demandOrderId\": { \"type\": \"long\" }, \"demandOrderCode\": { \"type\": \"keyword\" }, \"tagCode\": { \"type\": \"keyword\" }, \"tagName\": { \"type\": \"keyword\" }, \"remark\": { \"type\": \"keyword\" } } }, \"transportOrder\": { \"properties\": { \"id\": { \"type\": \"long\" }, \"expressStatus\": { \"type\": \"keyword\" }, \"receiptFlag\": { \"type\": \"keyword\" }, \"confirmCostFlag\": { \"type\": \"keyword\" }, \"demandOrderCode\": { \"type\": \"keyword\" }, \"tradeCode\": { \"type\": \"keyword\" }, \"costWorkOrderAuditFlag\": { \"type\": \"keyword\" }, \"frozenName\": { \"type\": \"keyword\" }, \"thirdOrderCode\": { \"type\": \"keyword\" }, \"transportCapacityCode\": { \"type\": \"keyword\" }, \"transportCapacityType\": { \"type\": \"keyword\" }, \"driverName\": { \"type\": \"keyword\" }, \"userName\": { \"type\": \"keyword\" }, \"winType\": { \"type\": \"keyword\" }, \"driverPhone\": { \"type\": \"keyword\" }, \"plateNo\": { \"type\": \"keyword\" }, \"carLength\": { \"type\": \"keyword\" }, \"carType\": { \"type\": \"keyword\" }, \"dispatchDriverDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"businessType\": { \"type\": \"keyword\" }, \"serviceType\": { \"type\": \"keyword\" }, \"startThreeAddress\": { \"type\": \"keyword\" }, \"endThreeAddress\": { \"type\": \"keyword\" }, \"placeOrderPerson\": { \"type\": \"keyword\" }, \"placeOrderTime\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"companyNo\": { \"type\": \"keyword\" }, \"loadingTime\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"finishDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"destinationRequireDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"expectedArriveDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"returnDistance\": { \"type\": \"double\" }, \"referenceDistance\": { \"type\": \"double\" }, \"needCarType\": { \"type\": \"keyword\" }, \"waybillCode\": { \"type\": \"keyword\" }, \"requireArrivalTime\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"loadService\": { \"type\": \"keyword\" }, \"unloadService\": { \"type\": \"keyword\" }, \"startDepartmentName\": { \"type\": \"keyword\" }, \"endDepartmentName\": { \"type\": \"keyword\" }, \"locationDepartmentName\": { \"type\": \"keyword\" }, \"needCarLength\": { \"type\": \"double\" }, \"insuredMoney\": { \"type\": \"double\" }, \"platformType\": { \"type\": \"keyword\" }, \"ycContractType\": { \"type\": \"keyword\" }, \"ycContractName\": { \"type\": \"keyword\" }, \"ycContractId\": { \"type\": \"keyword\" }, \"ycDepartmentName\": { \"type\": \"keyword\" }, \"ycTaiwanNum\": { \"type\": \"keyword\" }, \"signReportDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"orderStatus\": { \"type\": \"keyword\" }, \"shipperReceiptStatus\": { \"type\": \"integer\" }, \"signDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"exceptionType\": { \"type\": \"integer\" }, \"orderRiskFlag\": { \"type\": \"keyword\" }, \"orderRiskStatus\": { \"type\": \"keyword\" }, \"cancelCause\": { \"type\": \"keyword\" }, \"arrivePlateNo\": { \"type\": \"keyword\" }, \"orderStatusSub\": { \"type\": \"keyword\" }, \"arrivePlateFlag\": { \"type\": \"keyword\" }, \"vehicleInspectionStatus\": { \"type\": \"keyword\" }, \"pickGoodsFlag\": { \"type\": \"keyword\" }, \"takeTrackRate\": { \"type\": \"double\" }, \"deliveryTrackRate\": { \"type\": \"double\" }, \"score\": { \"type\": \"double\" }, \"executeDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"deliveryDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"pickupSignDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"trackRate\": { \"type\": \"double\" }, \"cancelDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"cancelOperatorName\": { \"type\": \"keyword\" }, \"pickupReportDate\": { \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\", \"type\": \"date\" }, \"orderRemark\": { \"type\": \"keyword\" }, \"warningFollowup\": { \"type\": \"nested\", \"properties\": { \"id\": { \"type\": \"long\" }, \"demandOrderCode\": { \"type\": \"keyword\" }, \"warningContent\": { \"type\": \"keyword\" }, \"warningContentCode\": { \"type\": \"keyword\" } } }, \"orderVehicle\": { \"properties\": { \"tradeCode\": { \"type\": \"keyword\" }, \"leakingUsableFlag\": { \"type\": \"keyword\" }, \"pdaLeakingStatus\": { \"type\": \"keyword\" }, \"ponchosAuditStatus\": { \"type\": \"keyword\" }, \"ponchosStatus\": { \"type\": \"keyword\" } } }, \"extra\": { \"properties\": { \"tradeCode\": { \"type\": \"keyword\" }, \"demandOrderCode\": { \"type\": \"keyword\" }, \"loadServiceType\": { \"type\": \"keyword\" }, \"unloadServiceType\": { \"type\": \"keyword\" }, \"unFrozenByName\": { \"type\": \"keyword\" }, \"projectOrderFlag\": { \"type\": \"keyword\" }, \"projectCode\": { \"type\": \"keyword\" }, \"projectFollower\": { \"type\": \"keyword\" }, \"projectTenderType\": { \"type\": \"integer\" }, \"receiptType\": { \"type\": \"keyword\" }, \"projectLineCode\": { \"type\": \"keyword\" }, \"contractBorrowLineName\": { \"type\": \"keyword\" }, \"pdaOutRangeFlag\": { \"type\": \"keyword\" }, \"appOutRangeFlag\": { \"type\": \"keyword\" }, \"lastRemarkBy\": { \"type\": \"keyword\" }, \"businessAreaName\": { \"type\": \"text\", \"fields\": { \"raw\": { \"type\": \"keyword\", \"null_value\": \"\" } } }, \"expensiveFlag\": { \"type\": \"keyword\" }, \"vehicleInspectionStatus\": { \"type\": \"keyword\" }, \"pdaPlateFakedType\": { \"type\": \"keyword\" }, \"driverPlateFakedType\": { \"type\": \"keyword\" }, \"driverBlacklistFlag\": { \"type\": \"keyword\" }, \"plateBlacklistFlag\": { \"type\": \"keyword\" }, \"carTypeLabel\": { \"type\": \"keyword\" }, \"deliveryTimeoutFlag\": { \"type\": \"keyword\" }, \"arriveDriversNumber\": { \"type\": \"integer\" }, \"requireDriversNumber\": { \"type\": \"integer\" }, \"beforeAssignFlag\": { \"type\": \"keyword\" }, \"contractBorrowFlag\": { \"type\": \"keyword\" } } } } } } } } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/安装ELK.html":{"url":"中间件/ElasticSearch/安装ELK.html","title":"安装ELK","keywords":"","body":"1.1. 使用docker安装es1.1.1. ES1.1.2. kibana1.1.3. logstash1.1. 使用docker安装es 1.1.1. ES docker下载es7.3.0镜像docker pull docker.elastic.co/elasticsearch/elasticsearch:7.3.0 docker创建一个网络，方便elk使用docker network create esnet 启动docker镜像docker run --name es -p 9200:9200 -p 9300:9300 --network esnet -e \"discovery.type=single-node\" bdaab402b220 参数说明run:运行 --name:容器名称 -p:端口映射 --network:使用网卡 -e:配置 bdaab402b220: 镜像id 启动之后 进入容器，可以自行配置es集群docker exec -it 1491dbda35e5 /bin/bash 1.1.2. kibana docker安装es对应版本的kibanadocker pull kibana:7.3.0 启动kibana容器，使用es所使用的网卡docker run --name kibana --net esnet -e ELASTICSEARCH_URL=http://127.0.0.1:9200 -p 5601:5601 -d 8bcee4a4f79d 1.1.3. logstash docker安装对应版本的logstashdocker pull logstash:7.3.0 在宿主机新建文件夹，logstash/config,logstash/pipeline配置文件 在config内新建logstash.yml/pipelines.yml logstash.yml配置config: reload: automatic: true interval: 3s xpack: management.enabled: false monitoring.enabled: false pipelines.yml配置 - pipeline.id: test path.config: \"/usr/share/logstash/pipeline/logstash-test.conf\" 在pipeline内新建logstash-test.conf input { file { path => [\"/usr/share/logstash/pipeline/logs/test.log\"] start_position => \"beginning\" stat_interval => 1 } } filter { mutate { gsub => [\"message\", \"\\r\", \"\"] } dissect { mapping => {\"message\" => \"%{date} %{+date} [%{task} %{+task}] [%{type}] %{class} - %{info}\"} } } output { elasticsearch { hosts => [\"172.18.0.2:9200\"] } stdout { codec => rubydebug } } 编写logstash配置文件 ```shell input { beats { port => 5044 codec => \"json\" } } output { elasticsearch { hosts => [\"192.168.12.183:9200\"] } stdout { codec => rubydebug } } 4. 启动logstash ```shell docker run -it -d -p 5044:5044 --name logstash --net esnet -v E:/docker-logstash/pipeline/:/usr/share/logstash/pipeline/ -v E:/docker-logstash/config/:/usr/share/logstash/config -d container 启动之后，可以在logstash/pipeline/logs内写test.log文件即可验证 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/ElasticSearch/搜索引擎.html":{"url":"中间件/ElasticSearch/搜索引擎.html","title":"搜索引擎","keywords":"","body":"1. ES1.1. 分布式架构原理1.2. ES写入数据的流程1.2.1. 写数据过程1.2.2. 读数据过程1.2.3. es搜索数据过程1.2.4. 删除/更新数据底层原理1.2.5. 倒排索引1.3. 数十亿级别数据下如何提高查询效率1. ES 基于lucene的分布式搜索引擎 1.1. 分布式架构原理 创建的索引指定N个shard，支持横向扩展，提高性能；每个shard都有一个primary shard,负责写入数据，还有几个replica shard，primary shard写入数据后，会将数据同步到其它几个replica shard es集群多个节点，会自动选举一个节点为master节点，master节点负责维护索引元数据，切换primary shard和replica shard身份。要是master节点宕机，会重新选举一个节点为master 1.2. ES写入数据的流程 ES写入/查询流程 1.2.1. 写数据过程 客户端选择一个node发送请求过去，这个node是coordinating node(协调节点) ciirdinating node对document进行路由，将请求转发给对应的node(primary shard) 实际的node上primary shard处理请求，将数据同步到replica node coordinating node发现primary node和所有replica node都搞定后，返回响应结果给客户端 1.2.2. 读数据过程 通过doc id来查询，根据doc id进行hash，判断出来当时把doc id分配到哪个shard上去，从哪个shard去查询 客户端发送请求到任意一个node，成为coordinate node coordinate node对doc id进行哈希路由，请求转发到对应的node，使用round-robin随机轮询算法，在primary shard以及所有replica中随机选择一个，读请求负载均衡 接收请求的node返回document给coordinate node coordinate node返回document给客户端 1.2.3. es搜索数据过程 客户端发送请求到一个coordinate node 协调节点将搜索请求转发到所有的shard对应的primary shard或replica shard query phase:每个shard将自己的搜索结果返回给协调节点（doc id）,由协调节点进行数据合并、排序、分页等，产出最终结果 fetch phase:由协调节点根据doc id去各个节点上拉去实际的document数据，返回客户端 1.2.4. 删除/更新数据底层原理 删除操作：commit会生成一个.del文件，将某个doc标识为deleted状态 更新操作：原来的doc标识为deleted状态，然后新写入一条数据 1.2.5. 倒排索引 正向索引是通过key找value，反向索引则是通过value找key 倒排索引如下： 1.3. 数十亿级别数据下如何提高查询效率 主要是filesystem cache，让机器内存，至少可以容纳总数据量的一半 比如现在一行数据，id,name,age...30个字段。但是现在搜索只需要根据id,name,age3个字段来搜索，如果在es里写入了一行数据所有字段，则导致90%的数据不是用来搜索的，就会占据fileasystem cache的内存。 解决使用es+hbase,hbase适用于海量数据在线存储，但不要做复杂的搜索 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/Postgressql/初始Psql.html":{"url":"中间件/Postgressql/初始Psql.html","title":"初始Psql","keywords":"","body":"1.1. 表基础操作1.1.1. 创建一个新表1.1.2. 删除一张表1.1.3. 插入数据1.1. 表基础操作 1.1.1. 创建一个新表 CREATE TABLE weather ( city varchar(80), temp_lo int, -- 最低温度 temp_hi int, -- 最高温度 prcp real, -- 湿度 date date ); 说明：varchar(80)指定了一个可以存储最长 80 个字符的任意字符串的数据类型。int是普通的整数类型。real是一种用于存储单精度浮点数的类型。date类型时间类型。 PostgreSQL支持标准的SQL类型int、smallint、real、double precision、char(N)、varchar(N) 、date、time、timestamp和interval，还支持其他的通用功能的类型和丰富的几何类型。PostgreSQL中可以定制任意数量的用户定义数据类型。因而类型名并不是语法关键字，除了SQL标准要求支持的特例外 CREATE TABLE cities ( name varchar(80), location point ); point就是一种PostgreSQL特有数据类型 1.1.2. 删除一张表 DROP TABLE tablename; 1.1.3. 插入数据 INSERT INTO weather VALUES ('San Francisco', 46, 50, 0.25, '1994-11-27'); point类型要求一个座标对作为输入INSERT INTO cities VALUES ('San Francisco', '(-194.0, 53.0)'); Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/redis/":{"url":"中间件/redis/","title":"Redis","keywords":"","body":"1.1. 简介1.2. 设计使用当中的坑1.3. Redis全景图包括哪些1.4. redis核心基础篇1-11压缩图片1.1. 简介 设计一个单机千万级吞吐量的键值数据库。在这个过程中，可以深入、透彻地研究了 Redis，包括它的源代码、架构设计以及核心控制点 1.2. 设计使用当中的坑 总的来说分四个方面： CPU 使用上的“坑”，例如数据结构的复杂度、跨 CPU 核的访问； 内存使用上的“坑”，例如主从同步和 AOF 的内存竞争； 存储持久化上的“坑”，例如在 SSD 上做快照的性能抖动； 网络通信上的“坑”，例如多实例时的异常网络丢包。1.3. Redis全景图包括哪些 两大维度、三大主线 1.4. redis核心基础篇1-11压缩图片 推荐前往幕布查看原文档：https://www.mubucm.com/doc/1bMEbiZTaGl Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/redis/Redis实现延迟队列.html":{"url":"中间件/redis/Redis实现延迟队列.html","title":"Redis实现延迟队列","keywords":"","body":"1.1. Redis实现延迟队列1.1.1. redis失效监听事件1.1.2. 使用redisson实现延迟队列【wiki】1.1. Redis实现延迟队列 失效监听 redisson实现发布订阅延迟 1.1.1. redis失效监听事件 集成KeyExpirationEventMessageListener类实现redis失效监听事件 此种实现面临的问题 redis的失效监听事件会存在一定的时间差，并且当数据量越大时，误差会越大。 redis的失效监听事件会将所有key失效都会通知到onMessage,如果针对一个key，分布式业务的场景下，会出现重复消费的问题。（可以增加分布式锁的实现，但是redisson分布式锁提供了另一种延迟队列的实现方式） Redis 目前的订阅与发布功能采取的是发送即忘（fire and forget）策略,当订阅事件断线时，会丢失所有在断线期间分给它的事件。不能确保消息送达。 开发准备 redis需要在服务端开启配置，打开redis服务的配置文件 添加notify-keyspace-events Ex 相关参数如下： K：keyspace事件，事件以__keyspace@__为前缀进行发布； E：keyevent事件，事件以__keyevent@__为前缀进行发布； g：一般性的，非特定类型的命令，比如del，expire，rename等； $：字符串特定命令； l：列表特定命令； s：集合特定命令； h：哈希特定命令； z：有序集合特定命令； x：过期事件，当某个键过期并删除时会产生该事件； e：驱逐事件，当某个键因maxmemore策略而被删除时，产生该事件； A：g$lshzxe的别名，因此”AKE”意味着所有事件。 基础实现 加入依赖 org.springframework.boot spring-boot-starter-data-redis 可正常连接存取redis数据之后，创建监听类RedisKeyExpirationListener继承KeyExpirationEventMessageListener ，重写onMessage方法。（key失效之后，会发出onMessage方法，之呢个获取失效的key值，不能获取key对应的value值）。 import com.dadi01.scrm.service.member.api.common.MemberStatusEnum; import com.dadi01.scrm.service.member.provider.service.base.IBaseMemberService; import lombok.extern.slf4j.Slf4j; import org.springframework.data.redis.connection.Message; import org.springframework.data.redis.listener.KeyExpirationEventMessageListener; import org.springframework.data.redis.listener.RedisMessageListenerContainer; import org.springframework.stereotype.Component; /** * @author lviter */ @Component @Slf4j public class RedisKeyExpirationListener extends KeyExpirationEventMessageListener { private final IBaseMemberService baseMemberService; private final static String MEMBER_LOCK_ACCOUNT_SUFFIX = \".lock_account\"; private final static String MEMBER_LOCK_ACCOUNT_DOMAIN_SUFFIX = \"T\"; private final static String MEMBER_LOCK_ACCOUNT_MEMBER_SUFFIX = \"M\"; private final static String MEMBER_REDISSON_LOCK = \".member_lock_redisson\"; private final static int WAIT_TIME = 5; private final static int LEASE_TIME = 10; public RedisKeyExpirationListener(RedisMessageListenerContainer redisMessageListenerContainer, IBaseMemberService baseMemberService) { super(redisMessageListenerContainer); this.baseMemberService = baseMemberService; } @Override public void onMessage(Message message, byte[] pattern) { //获取失效的key String expiredKey = message.toString(); log.info(\"================================get on message:{}====================\", expiredKey); if (expiredKey.endsWith(MEMBER_LOCK_ACCOUNT_SUFFIX)) { log.info(\"================================on message:{}====================\", expiredKey); try { log.info(\"=======待解锁账号解锁======expiredKey:{}\", expiredKey); String tenantId = expiredKey.substring(expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_DOMAIN_SUFFIX) + 1, expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_MEMBER_SUFFIX)); String memberId = expiredKey.substring(expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_MEMBER_SUFFIX) + 1, expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_SUFFIX)); baseMemberService.updateAccount(Integer.parseInt(tenantId), Long.parseLong(memberId), MemberStatusEnum.NORMAL.getCode(), null); } catch (Exception exception) { log.info(\"auto unlock fail,expired key:{},exception:{}\", expiredKey, exception.getMessage()); } } } } 创建一个配置类RedisConfig /** * @author lviter */ @Configuration public class RedisConfig { @Value(\"${redis.dbIndex}\") private Integer dbIndex; private final String TOPIC = \"__keyevent@\" + dbIndex + \"__:expired\"; private final RedisConnectionFactory redisConnectionFactory; public RedisConfig(RedisConnectionFactory redisConnectionFactory) { this.redisConnectionFactory = redisConnectionFactory; } @Bean public RedisMessageListenerContainer redisMessageListenerContainer() { RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer(); redisMessageListenerContainer.setConnectionFactory(redisConnectionFactory); //keyevent事件，事件以__keyevent@__为前缀进行发布 //db为redis第几个库 db2... // redisMessageListenerContainer.addMessageListener(redisKeyExpirationListener, new PatternTopic(TOPIC)); return redisMessageListenerContainer; } } 1.1.2. 使用redisson实现延迟队列【wiki】 由于延时队列持久化在redis中，所以机器宕机数据不会异常丢失，机器重启后，会正常消费队列中积累的任务 redisson实现延迟队列的原理 使用redis的zset有序性，轮询zset中的每个元素，到点后将内容迁移至待消费的队列 Redisson延迟队列使用三个结构来存储，一个是queueName的list，值是添加的元素；一个是timeoutSetName的zset，值是添加的元素，score为timeout值；还有一个是getName()的blockingQueue，值是到期的元素。 将元素及延时信息入队，之后定时任务将到期的元素转移到阻塞队列。 使用HashedWheelTimer做定时，定时到期之后从zset中取头部100个到期元素，所以定时和转移到阻塞队列是解耦的，无论是哪个task触发的pushTask，最终都是先取zset的头部先到期的元素。 元素数据都是存在redis服务端的，客户端只是执行HashedWheelTimer任务，所以单个客户端挂了不影响服务端数据，做到分布式的高可用。 延迟队列配置 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import org.redisson.api.RBlockingQueue; import org.redisson.api.RDelayedQueue; import org.redisson.api.RedissonClient; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * @author lviter * redisson延迟队列 */ @Configuration public class RedissonQueueConfig { private final String queueName = \"queue\"; @Bean public RBlockingQueue rBlockingQueue(@Qualifier(\"redissonSingle\") RedissonClient redissonClient) { return redissonClient.getBlockingQueue(queueName); } @Bean(name = \"rDelayedQueue\") public RDelayedQueue rDelayedQueue(@Qualifier(\"redissonSingle\") RedissonClient redissonClient, @Qualifier(\"rBlockingQueue\") RBlockingQueue blockQueue) { return redissonClient.getDelayedQueue(blockQueue); } } 定义队列使用接口 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import java.util.concurrent.TimeUnit; /** * @author lviter */ public interface DelayQueue { /** * 发布 * * @param object * @return */ Boolean offer(Object object); /** * 带延迟功能的队列 * * @param object * @param time * @param timeUnit */ void offer(Object object, Long time, TimeUnit timeUnit); void offerAsync(Object object, Long time, TimeUnit timeUnit); Boolean offerAsync(Object object); } 延迟队列实现 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import org.redisson.api.RDelayedQueue; import org.redisson.api.RFuture; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; import javax.annotation.Resource; import java.util.concurrent.ExecutionException; import java.util.concurrent.TimeUnit; /** * @author lviter */ @Component public class RedissonDelayQueue implements DelayQueue { private static Logger log = LoggerFactory.getLogger(RedissonDelayQueue.class); @Resource(name = \"rDelayedQueue\") private RDelayedQueue rDelayedQueue; @Override public Boolean offer(Object object) { return rDelayedQueue.offer(object); } @Override public void offer(Object object, Long time, TimeUnit timeUnit) { rDelayedQueue.offer(object, time, timeUnit); } @Override public void offerAsync(Object object, Long time, TimeUnit timeUnit) { rDelayedQueue.offerAsync(object, time, timeUnit); } @Override public Boolean offerAsync(Object object) { boolean flag = false; RFuture rFuture = rDelayedQueue.offerAsync(object); try { flag = rFuture.get(); } catch (InterruptedException | ExecutionException e) { log.info(\"offerAsync exception:{}\", e.getMessage()); e.printStackTrace(); } return flag; } } 启动一个后台监控线程 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import org.redisson.api.RBlockingQueue; import org.springframework.stereotype.Component; import javax.annotation.PostConstruct; import javax.annotation.Resource; /** * @author lviter */ @Component public class RedissonTask { @Resource(name = \"rBlockingQueue\") private RBlockingQueue rBlockingQueue; @PostConstruct public void take() { new Thread(() -> { while (true) { try { System.out.println(\"=========================\" + rBlockingQueue.take()); } catch (InterruptedException e) { e.printStackTrace(); } } }).start(); } } 使用延迟队列发送 package com.dadi01.scrm.service.member.provider.impl; import org.junit.Test; import org.junit.runner.RunWith; import org.mybatis.spring.annotation.MapperScan; import org.redisson.api.RDelayedQueue; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.ActiveProfiles; import org.springframework.test.context.junit4.SpringRunner; import javax.annotation.Resource; import java.util.concurrent.TimeUnit; @RunWith(SpringRunner.class) @SpringBootTest @ActiveProfiles(value = \"llh\") @MapperScan(\"com.dadi01.scrm.service.member.provider.mapper\") public class RDelayQueueTests { @Resource(name = \"rDelayedQueue\") private RDelayedQueue rDelayedQueue; @Test public void offerAsync() { rDelayedQueue.offerAsync(\"llh send message\", 20, TimeUnit.SECONDS); } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/redis/Redis数据结构.html":{"url":"中间件/redis/Redis数据结构.html","title":"Redis数据结构","keywords":"","body":"1.1. redis有哪些数据结构（https://www.kancloud.cn/kancloud/redisbook/63832）1.1.1. redis对使用者暴露了五种value type,其底层实现的数据结构有8种，分别是：1.1.2. 键值采用什么结构组织1.1.3. string字符串1.1.4. hash哈希1.1.5. list列表1.1. redis有哪些数据结构（https://www.kancloud.cn/kancloud/redisbook/63832） 基础的数据结构有String、Hash、List、Set、ZSet五种，还有HyperLogLog、GEO、Pub\\Sub 1.1.1. redis对使用者暴露了五种value type,其底层实现的数据结构有8种，分别是： SDS - simple synamic string -支持自动动态扩容的字节数组（简单动态字符串--String类型） list - 链表（双向链表） dict - 使用双哈希表实现，支持平滑扩容的字典（哈希表） zskiplist - 附加了后向指针的跳表 intset - 用于存储整数数值集合的自有结构（整数数组） ziplist - 实现类似于TLV,但比TLV复杂，用于存储任意数据的有序序列的数据结构（压缩列表） quicklist - 以ziplist作为结点的双链表结构 zipmap - 小规模场合使用的轻量级字典结构 1.1.2. 键值采用什么结构组织 使用哈希表来保存所有键值对，其实就是一个数组，数组的每个元素称为一个哈希桶。哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。 如图，在哈希桶内，一个entry内存放了key,value的键值指针，分别指向了不同数据结构的值。 哈希表最大好处是可以使用O(1)的时间复杂度来快速查找到键值对==我们只需要计算键的哈希值，即可找到哈希桶的位置，然后可以访问到entry元素。 当写入大量数据时，操作可能会变慢，什么原因？ 哈希表冲突和rehash可能带来操作阻塞。 redis解决哈希冲突的方式，就是链式哈希。指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。 1.1.3. string字符串 字符串类型可以存放--简单的字符串、复杂的字符串（xml、json）、数字（整数、浮点数）、二进制（图片、音频、视频），最大不能超过512M 使用场景 缓存功能，大部分请求数据从redis获取，而不是持久层，支撑高并发访问对持久层造成的压力，缓存可以起到加速读写和降低后端压力的作用 计数器，快速计数，查询缓存，比如登录输入五次密码错误就锁定，可以统计输入密码次数。 共享token，redis引入解决最大的问题就是天然的分布式，一处写入token，写入用户信息，多处访问获取token内json数据 限速:用户获取手机短信验证码，为了短信接口不被频繁调用，限制用户每分钟获取验证码的频率 1.1.4. hash哈希 hash类型是指键本身又是一种键值对结构，如： 上面的例子是使用hash结构存储的Map /** * HashSet * * @param key 键 * @param map 对应多个键值 * @return true 成功 false 失败 */ public boolean hmset(String key, Map map) { try { redisTemplate.opsForHash().putAll(key, map); return true; } catch (Exception e) { e.printStackTrace(); return false; } } 1.1.5. list列表 列表类型用来储存多个有序的字符串，一个列表最多可以储存2的32次方-1个元素，可以通过索引下标获取某个或某个范围内的元素列表 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/redis/Redis核心1-11基础篇.html":{"url":"中间件/redis/Redis核心1-11基础篇.html","title":"Redis核心1-11基础篇","keywords":"","body":"1. Redis核心-初级篇1. Redis核心-初级篇 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/redis/Redis核心12-21高级篇.html":{"url":"中间件/redis/Redis核心12-21高级篇.html","title":"Redis核心12-21高级篇","keywords":"","body":"1. Redis核心-高级篇1. Redis核心-高级篇 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/消息队列/消息队列.html":{"url":"中间件/消息队列/消息队列.html","title":"消息队列","keywords":"","body":"1. 消息队列1.1. 为什么用消息队列？1.2. 如何保证消息队列的高可用1.3. 如何保证幂等性1.4. 保证消息的可靠性传输1.5. 保证消息的顺序性1.6. 解决消息队列的延时以及过期失效问题1.6.1. 大量消息在mq里积压了几个小时还没解决1.6.2. mq消息过期失效了1.7. 如何设计一个消息队列1. 消息队列 缺点：系统可用性降低；系统复杂度提高；数据一致性问题 1.1. 为什么用消息队列？ 解耦 通过MQ,Pub/Sub发布订阅消息模型，可以做到系统解耦 A系统产生数据，发送到MQ，哪个系统需要，自己去MQ消费即可 新增一个系统，如果需要，直接可以对接消费 如果哪个系统不需要数据，直接取消消费即可 异步 用户反问每个解耦请求必须在200ms以内完成，对用户无感知 异步处理，发送到队列效率快，不影响当前系统业务 削峰 在某个时间点，访问量突然暴增，全部打到数据库可能导致数据库不可用，系统挂掉 1.2. 如何保证消息队列的高可用 RabbitMq是基于主从做高可用的 单机模式 普通集群模式（无高可用，只是提高了吞吐量） 多台机器上启动多个RabbitMQ实例，每个机器启动一个 创建的queue，只会放在一个RabbitMq实例，但每个实例都同步queue的元数据（queue的配置信息），消费时，如果连接到一个实例，那实例会从queue所在实例上拉取数据 如果放queue的实例宕机，会导致其它实例无法拉取，如果开启了消息持久化，让RabbitMQ落地存储消息，不一定会丢，等实例恢复，可以继续从这个queue拉取数据 镜像集群模式（高可用性） 在镜像集群模式下，创建的queue，无论元数据还是queue里的消息都会存在于多个实例上，每个节点都有queue的一个完整镜像，每次写消息到queue的时候，会自动把消息同步到多个实例的queue上 好处：任何一个机器宕机，别的consumer都可以去其它节点上消费数据 缺点：性能开销大，消息需要同步到所有机器，导致网络带宽压力和消耗很重；没有扩展性，加机器，新增的机器也包含了queue的所有数据 1.3. 如何保证幂等性 一个数据，或者一个请求，重复多少次，确保对应的数据是不会改变的，不能出错就是幂等性 解决：比如写库操作，现根据主键查询，如果数据有了就不再插入；或者写入redis，去检验是否操作过；或者每条消息增加全剧唯一的id，先去查询再操作 1.4. 保证消息的可靠性传输 rabbitmq消息丢失的3种情况 消息在传入过程中丢失 解决：开启confirm模式，生产者那里设置开启confirm,每次写的消息都会分配一个唯一的id,如果写入了RabbitMQ，会回传一个ack消息。如果没能处理，会回调一个nack接口，通知消息接收失败。 RabbitMQ收到消息，暂存在内存中，还没消费，自己挂掉，内存中的数据搞丢 解决： 开启RabbitMQ的持久化，消息写入后持久化到磁盘，RabbitMq挂了恢复后会自动读取之前存储的数据，一般数据不会丢失 设置持久化步骤：（需要同时设置这两个持久化才行，RabbitMQ哪怕挂了再重启，也会从磁盘上重启恢复queue和数据） 创建queue的时候将其设置为持久化：可以保证持久化queue的元数据，但是不会持久化queue里的数据； 发送消息的时候将消息的deliveryMode设置为2:将消息设置持久化 结合confirm机制配合使用，只有消息被持久化到磁盘后，才会通知生产者ack 消费者收到消息，但还没来得及处理，就挂了 解决：RabbitMQ提供的ack机制，关闭自动ack,在业务逻辑处理完后调用一个手动ack的api 1.5. 保证消息的顺序性 拆分多个queue，一个queue一个consumer，一个queue对应一个consumer，然后这个consumer内部用内存队列做排队；mq本身是无序的，需要手动将多个queue排序或者在内存内排队处理 1.6. 解决消息队列的延时以及过期失效问题 1.6.1. 大量消息在mq里积压了几个小时还没解决 先修复consumer问题，确保恢复消费速度，然后停掉现有的consumer 写一个临时分发数据的consumer程序，这个程序部署去消费积压的数据，消费后不做耗时处理，直接均匀轮询写入临时建立好的10倍数量的queue 临时征用10倍机器来部署consumer，每一批consumer消费一个临时queue的数据 快速消费完积压数据后，恢复原先部署的架构，重新用原先的consumer机器来消费 1.6.2. mq消息过期失效了 批量重导，将丢失的数据，写临时程序，查询出来，重新灌入mq里 1.7. 如何设计一个消息队列 支持可伸缩，需要时快速扩容，增加吞吐量和容量 mq数据落磁盘，顺序写 可用性，挂了重新选举 支持数据0丢失 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/消息队列/Kafka/Kafka入门.html":{"url":"中间件/消息队列/Kafka/Kafka入门.html","title":"Kafka入门","keywords":"","body":"1. 文档1.1. 背景简介1.1.1. ABC火爆1.2. 基础结构1.3. 极简数据结构1.3.1. 最大化数据传输效率1. 文档 原文链接：https://www.springcloud.cc/apache-kafka-zhcn.html 0.11.0文档 1.1. 背景简介 kafka，分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域 1.1.1. ABC火爆 AI人工智能 BigData大数据 Cloud云计算 Kafka介绍 作用：能够有效隔离上下游业务，将上游突增的流量缓存起来，以平滑方式传导到下游子系统中，避免了流量的不规则冲击。甚至可以在业务中实现：消息引擎应用，应用程序集成，分布式存储构建，甚至流处理应用的开发与部署 解耦 容错：集群部署，能保证可用性，leader故障会重新选举副本中的follower为新的leader 缓冲：可以控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况 异步 削峰 1.2. 基础结构 Producer：生产者，向Kafka broker发消息的客户端 Consumer：消费者，向Kafka broker取消息的客户端 Consumer Group（CG）：消费者组，多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者之间互不影响，消费者组是逻辑上的一个订阅者 Broker：一台Kafka服务器就是一个broker。一个集群有多个broker组成，一个broker可以容纳多个topic Topic：队列，生产者和消费者面向的都是一个topic Partition：扩展性，一个大的topic可以分布到多个broker服务器上，一个topic可以分为多个partition，每个partition是一个有序的队列 Replica：副本，集群中某个节点发生故障时，该节点的partition数据不丢失，且kafka仍能继续工作，一个topic的每个分区都有若干个副本，一个leader和若干个follower leader：每个分区多个副本的主节点，生产者发送/消费者消费的数据对象都是leader follower：实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader 1.3. 极简数据结构 消息队列是以log文件的形式存储，消息生产者只能将消息添加到既有的文件尾部，没有任何ID信息用于消息的定位，完全依靠文件内的位移，因此消息的使用者只能依靠文件位移顺序读取消息，这样也就不需要维护复杂的支持随即读取的索引结构（所以可以保证，分区内部有序，但是全局并不有序。） 1.3.1. 最大化数据传输效率 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/消息队列/Kafka/Kafka消费超时导致重复消费.html":{"url":"中间件/消息队列/Kafka/Kafka消费超时导致重复消费.html","title":"Kafka消费超时导致重复消费","keywords":"","body":" Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"中间件/消息队列/RabbitMq/RabbitMQ安装及使用.html":{"url":"中间件/消息队列/RabbitMq/RabbitMQ安装及使用.html","title":"RabbitMQ安装及使用","keywords":"","body":"1. RabbitMQ安装及使用1.1.1. 一.Linux系统中安装RabbitMQ1.1.2. 二.RabbitMQ使用1. RabbitMQ安装及使用 1.1.1. 一.Linux系统中安装RabbitMQ 由于RabbitMQ依赖于Erlang,所以先要在机器上安装Erlang环境 单机版 1.安装GCC GCC-C++ Openssl等模块 yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel 2.安装ncurses yum -y install ncurses-devel 3.安装erlang环境 wget http://erlang.org/download/otp_src_18.2.1.tar.gz --这一步比较慢 tar xvfz otp_src_18.2.1.tar.gz 然后进入解压后的文件,执行下面命令 ./configure make install 经过上述步骤,就算完成Erlang环境的搭建了,接下来安装RabbitMQ 4.下载rabbitmq-server-3.6.9.tar.xz wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.9/rabbitmq-server-generic-unix-3.6.9.tar.xz 5.对于下载xz包进行解压，首先先下载xz压缩工具 yum install xz 6.对rabbitmq包进行解压 xz -d xz -d rabbitmq-server-generic-unix-3.6.9.tar.xz tar -xvf rabbitmq-server-generic-unix-3.6.9.tar 7.复制解压包到/usr/local/下,并改名rabbitmq cp -r rabbitmq_server-3.6.9 /usr/local/rabbitmq 8.这种下载的方式解压后直接可以使用，无需再编译安装 进入到rabbit文件内，其命令文件存在于sbin文件夹下，因此需要将sbin文件夹的路径添加到PATH中：修改/etc/profile export PATH=/usr/local/rabbitmq/sbin:$PATH 执行一下命令使得PATH路径更新，rabbitMQ安装成功 source /etc/profile 9.后台启动RabbitMQ设置 rabbitmq-plugins enable rabbitmq_management #启动后台管理 rabbitmq-server -detached #后台运行rabbitmq 10.设置端口号或者关闭防火墙,以便外部访问 iptables -I INPUT -p tcp --dport 15672 -j ACCEPT 或 service iptables stop 11.RabbitMQ页面默认是被禁止,如果需要访问,必须创建用户,并且设置权限,如下设置 添加用户:rabbitmqctl add_user admin admin 添加权限:rabbitmqctl set_permissions -p \"/\" admin \".*\" \".*\" \".*\" 修改用户角色:rabbitmqctl set_user_tags admin administrator 1.1.2. 二.RabbitMQ使用 1.订阅模式 当多个队列与交换机绑定时,消息的生产者生产消息并传递到交换机,交换机根据binding把消息传递给所有绑定的队列.如果队列下面有多个消息的消费者,则会轮询消费消息.队列采用的是匿名队列,匿名队列会随机产生一个队列名,并且消费者如果不存在以后,队列也会自动消失. /** * rabbitmq 配置类 * */ @Configuration public class RabbitMQConfig { //得到一个发布订阅的交换机 @Bean public FanoutExchange getFanoutExchange() { return new FanoutExchange(\"fanoutexchange\"); } //得到三个队列 @Bean public Queue createQueue1(){ return new AnonymousQueue(); } @Bean public Queue createQueue2() { return new AnonymousQueue(); } @Bean public Queue createQueue3() { return new AnonymousQueue(); } //绑定队列和交换机 @Bean public Binding getBinding1(Queue createQueue1,FanoutExchange fanoutExchange){ return BindingBuilder.bind(createQueue1).to(fanoutExchange); } @Bean public Binding getBinding2(Queue createQueue2, FanoutExchange fanoutExchange) { return BindingBuilder.bind(createQueue2).to(fanoutExchange); } @Bean public Binding getBinding(Queue createQueue3, FanoutExchange fanoutExchange) { return BindingBuilder.bind(createQueue3).to(fanoutExchange); } } /** * 消息的生产者 * */ @Component public class MessageProvider { @Autowired private AmqpTemplate amqpTemplate; @Autowired private FanoutExchange fanoutExchange; public void sendMessage(String message){ this.amqpTemplate.convertAndSend(fanoutExchange.getName(),null,message); } } @Component @RabbitListener(queues = \"#{createQueue1.name}\") public class MessageCustomer { @RabbitHandler public void getMessage(String message){ System.out.println(\"第一个信息接收者:\"+message); } } @Component @RabbitListener(queues = \"#{createQueue2.name}\") public class SecondMessageCustomer { @RabbitHandler public void getMessage(String message) { System.out.println(\"第二个信息接收者:\"+message); } } @Component @RabbitListener(queues = \"#{createQueue3.name}\") public class ThirdMessageCustomer { @RabbitHandler public void getMessage(String message) { System.out.println(\"第三个信息接收者:\"+message); } } /** * 测试类 * */ @RunWith(SpringRunner.class) @SpringBootTest public class RabbitMQTest { @Autowired private MessageProvider messageProvider; @Test public void messageTest() { messageProvider.sendMessage(\"hello\"); } } 2.点对点模式 队列和交换机绑定产生一个binding.当routing key与binding key完全匹配的时候,消息才会被投递 //点对点模式 @Bean(\"directexchange\") public DirectExchange getDirectExchange() { return new DirectExchange(\"directexchange\"); } @Bean(\"directqueue\") public Queue getDirectQueue() { return new Queue(\"directqueue\"); } @Bean public Binding getDirectBindingKey(@Qualifier(value = \"directqueue\") Queue queue,@Qualifier(value = \"directexchange\") Exchange exchange) { return BindingBuilder.bind(queue).to(exchange).with(\"direct_key\").noargs(); } //点对点模式 public void sendMessageFaceToFace(String message) { this.amqpTemplate.convertAndSend(directExchange.getName(),\"direct_key\",message); } 点对点拉取模式: 消费者不是和队列保持长连接状态,而是主动拉取消息 @Component public class AckMessageCustomer { public void processMessage2() throws IOException, TimeoutException { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"192.168.245.128\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); //pull方式获取信息 GetResponse response = channel.basicGet(\"directqueue\", false); channel.basicAck(response.getEnvelope().getDeliveryTag(),false); //channel.basicAck(tag, false);//确认收到信息.第二个参数如果为true时,表示tag小于当前tag的信息都会被一次性确认 connection.close(); } } 3.rabbitmq消息的确认机制 (1)消息生产者到交换机之间的消息确认: 通过实现ConfirmCallback 接口，消息发送到 Broker 后触发回调，确认消息是否到达 Broker 服务器 (2)交换机到队列之间的消息确认: 通过实现ReturnCallbac接口,启动消息失败返回,比如路由不到队列时触发回调 @Component public class RabbitTemplateConfig implements RabbitTemplate.ReturnCallback,RabbitTemplate.ConfirmCallback { @Autowired private RabbitTemplate rabbitTemplate; @PostConstruct public void init(){ rabbitTemplate.setConfirmCallback(this); } @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) { System.out.println(\"交换机确认信息:唯一标识\"+correlationData+\",确认结果:\"+ack+\",失败原因:\"+cause); } @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) { System.out.println(\"队列收到信息:\"+message+\",回复消息码:\"+replyCode+\",回复内容:\"+replyText+\",交换机:\"+exchange+\",路由键:\"+routingKey); } } (3)队列和消费者之间的消息确认: 默认情况是,消费者收到消息,队列就删除这条消息,无论消费者是否正确处理这条消息.为了保证数据不被丢失，RabbitMQ支持消息确认机制，即ack.当消费者正确处理完消息以后,才通知队列删除相应的消息,而不是一收到消息就立即通知队列删除消息. ack的模式有三种: AcknowledgeMode.NONE：自动确认 AcknowledgeMode.AUTO：根据情况确认 AcknowledgeMode.MANUAL：手动确认 @Component @RabbitListener(queues = \"directqueue\") public class AckMessageCustomer { @RabbitHandler public void processMessage2(String message, Channel channel, @Header(AmqpHeaders.DELIVERY_TAG) long tag) throws IOException { System.out.println(message); try { channel.basicAck(tag, false);//确认收到信息.第二个参数如果为true时,表示tag小于当前tag的信息都会被一次性确认 } catch (IOException e) { channel.basicNack(tag,false,true);//未收到信息.第三个参数如果为true时,表示要重复发送这条消息 channel.basicReject(tag,true);//丢弃信息 } } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/":{"url":"企业项目框架/","title":"企业项目框架","keywords":"","body":"1. 企业级项目框架1.1. 核心Spring1.2. SpringBoot1.3. SpringCloud1.4. Dubbo1. 企业级项目框架 1.1. 核心Spring JavaEE企业级核心框架 1.2. SpringBoot 1.3. SpringCloud 微服务架构 1.4. Dubbo 分布式架构 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/DDD领域/日常使用遇到的.html":{"url":"企业项目框架/DDD领域/日常使用遇到的.html","title":"日常使用遇到的","keywords":"","body":"1. DDD1.1. 充血模型与贫血模型1. DDD 1.1. 充血模型与贫血模型 充血模型对象如何获取类中的方法？ 答： 在DDD领域驱动中，充血模型是一种软件设计模式，它强调实体对象应该具有行为，而不是只包含数据。在充血模型中，实体对象应该能够通过方法来访问和修改其内部的数据。在DDD领域驱动中，可以通过以下方式获取实体对象的行为方法： 依赖注入：将实体对象注入到其他对象中，通过依赖注入来获取实体对象的行为方法。例如，在Spring框架中，可以使用@Autowired注解将实体对象注入到其他对象中，然后通过实体对象的方法来访问和修改其内部的数据。 工厂方法：使用工厂方法来创建实体对象，并返回实体对象的行为方法。例如，在Spring框架中，可以使用BeanFactory或ApplicationContext接口来创建实体对象，并通过实体对象的方法来访问和修改其内部的数据。 仓储模式：使用仓储模式来管理实体对象，并返回实体对象的行为方法。例如，在Spring Data框架中，可以使用Repository接口来管理实体对象，并通过实体对象的方法来访问和修改其内部的数据。 事件驱动：使用事件驱动来通知实体对象其内部的数据发生了变化，并返回实体对象的行为方法。例如，在Spring框架中，可以使用事件监听器来监听实体对象的事件，并通过实体对象的方法来访问和修改其内部的数据。 需要注意的是，在DDD领域驱动中，应该将实体对象和其行为方法分离开来，以便更好地封装和管理实体对象。同时，应该遵循单一职责原则和开闭原则，以便更好地扩展和维护实体对象。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/dubbo/":{"url":"企业项目框架/dubbo/","title":"Dubbo","keywords":"","body":"1.1. DUBBO1.1. DUBBO Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/dubbo/Dubbo.html":{"url":"企业项目框架/dubbo/Dubbo.html","title":"Dubbo","keywords":"","body":"1. 分布式服务框架1.1. dubbo的原理1. 分布式服务框架 dubbo官网 1.1. dubbo的原理 工作原理 service层：接口层，给服务提供者和消费者来实现 config层：配置层，主要对Dubbo进行各种配置 proxy层：服务代理层，consumer，provider,dubbo都会生成代理，代理之间进行网络通信 registry层：注册层，负责服务的注册与发现 cluster层；集群层，封装多个服务提供者的路由进行负载均衡，多个实例组合成一个服务 monitor层：监控层，对RPC接口的调用次数和调用时间进行监控 protocal层：远程调用层，疯转RPC调用 exchange层：信息交换层，封装请求响应模式，同步转异步 transport层：网络传输层，封装mina和netty为统一接口 serialize层：数据序列化层 工作流程 provider向注册中心去注册 consumer从注册中心订阅服务，注册中心通知consumer注册好的服务 consumer调用provider consumer和provide都异步通知监控中心 注册中心挂了可以继续通信吗？ 可以，初始化时，消费者会将provide提供的地址等信息拉取到本地，所以注册中心挂了仍然可以继续通信 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/dubbo/分布式服务框架.html":{"url":"企业项目框架/dubbo/分布式服务框架.html","title":"分布式服务框架","keywords":"","body":"1. 分布式服务框架Dubbo1.1. Dubbo原理1.2. Dubbo支持哪些序列化协议1.3. Dubbo的负载均衡策略和集群容错策略1.3.1. Dubbo的负载均衡策略1.3.2. 集群容错策略1.4. Dubbo的SPI思想1.4.1. spi概念1.4.2. java spi思想的体现1.4.3. dubbo的spi思想1.5. 基于Dubbo的服务治理1.5.1. 服务降级1.5.2. 失败重试/超时重试1.6. 分布式服务的幂等性如何设计1.6.1. 如何保证幂等性？1.7. 分布式服务接口请求顺序如何保证1.7.1. 如何保证顺序1.8. 自己设计一个类似Dubbo的RPC框架1.9. CAP理论1.10. 分布式事务1.10.1. 两阶段提交方案/XA方案1.10.2. TCC方案,try,confirm,cancel（金融场景选择方案）1.10.3. saga方案（长事务）1.10.4. 可靠消息最终一致性方案1.10.5. 最大努力通知方案1.10.6. 本地消息表1.11. 为什么要拆分分布式系统1.11.1. 如何进行拆分1.11.2. dubbo和服务拆分有什么关系1.11.3. zk的一些场景1.12. 基于Hystrix实现高可用1.12.1. 有哪些限流算法1.13. Dubbo如何在RPC调用时异步转同步？1. 分布式服务框架Dubbo 1.1. Dubbo原理 分布式服务框架 1.2. Dubbo支持哪些序列化协议 Dubbo支持哪些通信协议 dubbo://协议 默认协议，单一长连接，进行的NIO异步通信，基于hessian作为序列化协议 特性，传输数据量小(每次100k以下)；并发量高，适用多个消费者的情况 rmi://协议 采用JDK标准的Java.rmi.*实现 特性，多个短链接，适合消费者和提供者差不多的情况，适用文件传输 hessian://协议 http通信，采用servlet暴露服务，基于hessian序列化协议 多个短链接，适用于提供者数量多的情况，适用文件传输 http://协议 基于http表单的远程通用协议，走单表序列化 thrift://协议 在原生协议的基础上添加了一些额外的头信息，如service name，magic number等 webservice://协议 基于apache cxf的fronted-simple和transports-http实现，SOA文本序列化 memcached://协议 RPC协议 redis://协议 RPC协议 rest://协议 实现REST调用支持 grpc://协议 适用HTTP/2通信，想利用Stream、反压、Reactive变成能力的开发者适用 支持的序列化协议 支持hessian，java二进制序列化，SOAP文本序列化，json多种序列化协议 1.3. Dubbo的负载均衡策略和集群容错策略 4中负载均衡策略，6种集群容错策略 1.3.1. Dubbo的负载均衡策略 RandomLoadBalance，默认策略，随机调用；可以对provider设置不同的权重，按照权重来负载均衡。 算法思想：假设有一组服务器servers=[A,B,C],对应权重为weights=[5,3,2],总和为10.把权重平铺在一纬坐标值上，则[0,5]服务器A,[5,8]服务器B,[8,10] 属于服务器C.随机数生成器生成一个范围在[0,10]之间的随机数，然后计算会落在哪个区间的服务器，坐标轴上区间范围大的，随机数生成的数字就有更大概率落到此区间 RoundRobinLoadBalance，均匀分发 算法思想：均匀地将流量打到各个机器上，但是如果各个机器性能不同，容易导致性能差的机器负载过高。所以需要调整权重，让性能差的机器承载权重小一些，流量少一些 LeastActiveLoadBalance，最小活跃数负载均衡，活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多请求，此时请求会优先分配给该服务提供者 算法思想：每个provider对应一个活跃数active。初识情况，provider的active均为0.每收到一个请求，对应的provider的active会加1，处理完请求后，active会减1.所以，如果provider性能好，处理请求的效率就越高，active下降的越快。也引入了权重值，所以此算法是基于加权最小活跃数算法实现 ConsistenHashLoadBalance，一致性hash 算法思想：一致性Hash算法，相同参数请求一定分发到一个provider去。如果需要的不是随机负载均衡策略，要一类请求都到一个节点，那就走一致性Hash策略 1.3.2. 集群容错策略 Failover Cluster模式，默认这个，失败自动切换，自动重试其它机器，常见于读操作 Failfast Cluster模式，一次调用失败就立即失败，常见幂等性的写操作，如新增一条记录 Failsafe Cluster模式，出现异常时忽略，用于不重要的接口调用，如记录日志 Failback Cluster模式，失败了后台自动记录请求，然后定时重发，适合写消息队列失败 Forking Cluster模式，并行调用多个provider，只要一个成功就立即返回。用于实时性要求较高的读操作，但是会浪费更多服务资源 Broadcast Cluster模式，逐个调用所有的provider,任何一个出错则报错。用于通知所有提供者更新缓存或日志等本地资源信息 1.4. Dubbo的SPI思想 service provider interface 1.4.1. spi概念 一个接口，有三个实现类，在系统运行时这个接口该选择哪个实现类？就需要根据指定的配置或者是默认的配置，找对应的实现类加载进来，用这个实现类的实例对象。插件扩展的场景 1.4.2. java spi思想的体现 经典思想JDBC。java定义了一套jdbc的接口，并没有提供jdbc的实现类，所以可以使用mysql-jdbc-connector.jar引入或者oracle-jdbc-connector.jar 1.4.3. dubbo的spi思想 Protocol 接口，在系统运行的时候，，dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用，它会去找一个你配置的 Protocol，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了 Protocol protocol=ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); dubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol http=com.alibaba.dubbo.rpc.protocol.http.HttpProtocol hessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol 1.5. 基于Dubbo的服务治理 服务治理：调用链路自动生成；服务访问压力以及时长统计 1.5.1. 服务降级 接口+Mock后缀，如： 降级逻辑 public class HelloServiceMock implements HelloService { public void sayHello() { // 降级逻辑 }} 1.5.2. 失败重试/超时重试 timeOut:一般设置为200ms，retries:设置重试次数 1.6. 分布式服务的幂等性如何设计 场景：假如有个服务部署在5台机器上，有个接口是付款接口，用户在前端操作，一个订单不小心发起了两次支付请求，然后分散在了这个服务部署的不同机器上 1.6.1. 如何保证幂等性？ 对于每个请求必须有一个唯一的标识，如：订单支付请求，订单id需要唯一 每次处理完请求后，必须有一个记录标识这个请求处理过了，如在mysql中记录个状态 每次接收请求需要进行判断，判断之前是否处理过。唯一键约束 1.7. 分布式服务接口请求顺序如何保证 场景：服务A调用服务B，先插入再删除。结果俩请求过去，落在不同机器上，可能因为插入请求因为某些原因执行慢了，导致删除请求先执行了 1.7.1. 如何保证顺序 尽量合并成一个操作，避免此问题产生 可以使用Dubbo的一致性hash负载均衡策略，将比如某一个订单id对应的请求都分发到某个机器上，然后可以将某个订单id对应的请求放到一个内存队列中，强制排队，来确保顺序性 1.8. 自己设计一个类似Dubbo的RPC框架 注册中心服务注册，可以用zk 消费者需要去注册中心拿对应服务信息，而且每个服务可能存在于多台机器上 发起一次请求，基于动态代理，面向接口获取到一个动态代理，然后这个代理找到服务对应的机器地址 使用简单的负载均衡策略确定向哪个机器发送请求 使用netty,nio方式，使用hessian序列化协议 服务器侧，针对自己的服务生成一个动态代理，监听某个网络端口 1.9. CAP理论 CAP，Consistency一致性，Availability可用性，Partition tolerance分区容错性 C:所有节点访问同一份最新的数据副本 A:每次请求都能获取到正常响应，不保证获取的数据为最新数据 P:分区相当于对通信的时限要求。系统不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择 常用的CAP框架 eureka(AP)：保证可用，实现最终一致性。使用内置轮询负载均衡器去注册，有一个检测间隔时间，如果在一定时间没有收到心跳，才会移除该节点注册信息。eurekaAP的特性和请求间隔同步机制 zookeeper(CP)：强一致性。在选举leader时会停止服务，只有成功选举后才能提供服务 1.10. 分布式事务 这里列举五种事务方案，具体得看本身业务 1.10.1. 两阶段提交方案/XA方案 有一个事务管理器，负责协调多个数据库的事务，事务管理器先问各个数据库你准备好了吗？如果都回复ok，就正式提交事务，在各个数据库上执行操作；如果任何其中一个数据库回答不ok，那么就回滚 场景应用：常见单应用实例内跨多个库的分布式场景 （缺点是严重依赖数据库层面来搞定复杂的事务，效率很低，不适合高并发场景） 1.10.2. TCC方案,try,confirm,cancel（金融场景选择方案） try阶段，各个服务的资源做检测以及对资源进行锁定或者预留 confirm阶段，在各个服务中执行实际的操作 cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，已经执行成功的业务逻辑回滚 对于一致性要求高、短流程、并发高的场景，会考虑TCC方案 1.10.3. saga方案（长事务） 业务流程中每个参与者都提交本地事务，若某一个参与者失败，则补偿前面已经成功的参与者 适用场景：业务流程长、业务流程多；参与者包含其它公司或遗留系统服务，无法提供TCC模式要求的三个接口 优势： 一阶段提交本地事务，无锁高性能 参与者可异步，高吞吐 补偿服务易于实现，因为一个更新操作的反向操作易于理解 缺点：不保证事务的隔离性 1.10.4. 可靠消息最终一致性方案 直接使用MQ来实现事务，如阿里的RocketMQ支持事务 1.10.5. 最大努力通知方案 A本地事务执行完成后，发送消息到MQ 有专门消费MQ的最大努力通知服务，这个服务消费MQ然后写入数据库中记录，或者放入内存队列，接着调用系统B接口 B执行成功就ok，要是执行失败，最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃 1.10.6. 本地消息表 严重依赖数据库的消息来管理事务，高并发场景并不适合 A系统在自己本地一个事务里操作同时，插入一条数据到消息表 A系统将这个消息发送到MQ B系统收到消息后，在一个事务里，往自己的本地消息表插入一条数据，同时执行其它业务操作，如果消息被处理过了，那此时这个事务回滚，保证不会重复处理消息 B系统执行成功后，更新自己本地消息表状态以及A系统消息表状态 B如果处理失败，那不更新消息表状态，此时A系统定时扫描自己的消息表，如果有未处理消息，再次发送到MQ，让B再次处理 保证了最终一致性，B失败，A会不断发消息，直至B成功 1.11. 为什么要拆分分布式系统 拆分服务，减少代码改动冲突，影响范围变小，开发效率高 减少服务发布影响范围，减少代码测试范围 1.11.1. 如何进行拆分 多轮拆分，根据业务性质拆分，如订单系统，商品系统等等 1.11.2. dubbo和服务拆分有什么关系 可以不用dubbo,纯http通信，但是就要考虑负载等问题，所以dubbo其实就是一个rpc框架，本地调用接口，dubbo会代理这个调用请求，跟远程机器网络通信，处理掉负载均衡、服务实例上下线自动感知、超时重试等 1.11.3. zk的一些场景 分布式协调，A系统发送了个消息到mq里去，然后B系统消费处理，那B系统消费处理后A如何知道结果？ 解决：用ZK实现分布式系统之间的协调 A系统发送后在Zk上对某个节点的值注册个监听器，一旦B处理完就修改ZK那个节点的值，A系统立马就可以收到通知 分布式锁 1.12. 基于Hystrix实现高可用 Hystrix让我们在分布式系统中对服务间调用进行控制，加入调用延迟或者依赖故障的容错机制；还提供故障时的fallback降级机制 1.12.1. 有哪些限流算法 计数器 控制单位时间内的请求数量。 劣势：设每分钟请求数量60个，每秒处理1个请求，用户在 00:59 发送 60 个请求，在 01:00 发送 60 个请求 此时 2 秒钟有 120 个请求(每秒 60 个请求)，远远大于了每秒钟处理数量的阈值。（突刺现象） import java.util.concurrent.atomic.AtomicInteger; public class Counter { /** * 最大访问数量 */ private final int limit = 10; /** * 访问时间差 */ private final long timeout = 1000; /** * 请求时间 */ private long time; /** * 当前计数器 */ private AtomicInteger reqCount = new AtomicInteger(0); public boolean limit() { long now = System.currentTimeMillis(); if (now 滑动窗口 对计数器的一个改进，增加一个时间粒度的度量单位，一分钟分为若干份（如6份，没10秒一份），在每份上设置独立计数器， leaky bucket漏桶 规定固定容量的桶，进入的水无法管控数量、速度，但是对于流出的水我们可以控制速度 劣势：无法应对短时间突发流量（桶满了就丢弃） public class LeakBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * 水流出速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.max(0, (nowSize - (now - time) * rate)); time = now; if ((nowSize + 1) Token Bucket令牌桶 规定固定容量的桶，token 以固定速度往桶内填充， 当桶满时 token 不会被继续放入， 每过来一个请求把 token 从桶中移除， 如果桶中没有 token 不能请求。 可以准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。 public class TokenBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * 水流出速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.min(total, (nowSize + (now - time) * rate)); time = now; if (nowSize 1.13. Dubbo如何在RPC调用时异步转同步？ TCP协议本身异步，发送完RPC请求后，线程不会等待RPC的响应结果 等待-通知机制，在dubbo之间RPC发起调用时，线程进入Timed_WAITING状态，阻塞调用线程；当rpc返回结果后，唤醒等待线程 // 创建锁与条件变量 private final Lock lock=new ReentrantLock(); private final Condition done=lock.newCondition(); // 调用方通过该方法等待结果 Object get(int timeout){ long start=System.nanoTime(); lock.lock(); try{ while(!isDone()){ done.await(timeout); long cur=System.nanoTime(); if(isDone()|| cur-start>timeout){ break; } } }finally{ lock.unlock(); } if(!isDone()){ throw new TimeoutException(); } return returnFromResponse(); } // RPC结果是否已经返回 boolean isDone(){ return response!=null; } // RPC结果返回时调用该方法 private void doReceived(Response res){ lock.lock(); try{ response=res; if(done!=null){ done.signal(); } }finally{ lock.unlock(); } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/ORM框架/":{"url":"企业项目框架/ORM框架/","title":"ORM框架","keywords":"","body":"1. ORM框架1.1. 接口层1.2. 数据处理层：1.3. 框架支撑层：1.4. mybatis核心部件1. ORM框架 Mybatis 1.1. 接口层 MyBatis 和数据库的交互有两种方式： 使用传统的 MyBatis 提供的 API 使用 Mapper 接口 1.2. 数据处理层： 通过传入参数构建动态 SQL 语句；（MyBatis 通过传入的参数值，使 用 Ognl 来动态地构造 SQL 语句，使得 MyBatis 有很强的灵活性和扩展性） SQL 语句的执行以及封装查询结果集成 List 1.3. 框架支撑层： 事务管理；连接池管理；缓存等 1.4. mybatis核心部件 SqlSession 作为 MyBatis 工作的主要顶层 API，表示和数据库交互的会话，完成 必要数据库增删改查功能 Executor MyBatis 执行器，是 MyBatis 调度的核心，负责 SQL 语句的生成和 查询缓存的维护 StatementHandler 封装了 JDBC Statement 操作，负责对 JDBC statement 的操作，如设置参数、将 Statement 结果集转换成 List 集合。 ParameterHandler 负责对用户传递的参数转换成 JDBC Statement 所需要的参数， ResultSetHandler 负责将 JDBC 返回的 ResultSet 结果集对象转换成 List 类型的集合； TypeHandler 负责 java 数据类型和 jdbc 数据类型之间的映射和转换 MappedStatement MappedStatement 维护了一条节点的封装， SqlSource 负责根据用户传递的 parameterObject，动态地生成 SQL 语句，将信 息封装到 BoundSql 对象中，并返回 BoundSql 表示动态生成的 SQL 语句以及相应的参数信息 Configuration MyBatis 所有的配置信息都维持在 Configuration 对象之中。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/ORM框架/Mybatis详解.html":{"url":"企业项目框架/ORM框架/Mybatis详解.html","title":"Mybatis详解","keywords":"","body":"1. Mybatis详解1.1. 初始化过程1.1.1. SQLSessionFactory1.2. 数据源与连接池1.2.1. DataSource创建1.2.2. Connection创建1. Mybatis详解 1.1. 初始化过程 创建 Configuration 对象的过程 基于XML配置文件：MyBatis 加载 XML 配置文件，将配置文信息组装成内部的 Configuration 对象 基于 Java API：在java代码中，手动创建 Configuration 对象，然后将配置参数 set 进入 Configuration 对象中。 1.1.1. SQLSessionFactory mybatis 初始化 -->创建 SqlSession -->执行 SQL 语句 SqlSessionFactoryBuilder 根据传入的数据流生成 Configuration 对象，然后根据 Configuration 对象创建默认的 SqlSessionFactory 实例 调用 SqlSessionFactoryBuilder 对象的 build(inputStream)方法； SqlSessionFactoryBuilder 会根据输入流 inputStream 等信息创建 XMLConfigBuilder 对象; SqlSessionFactoryBuilder 调用 XMLConfigBuilder 对象的 parse()方法； XMLConfigBuilder 对象返回 Configuration 对象； SqlSessionFactoryBuilder 根据 Configuration 对象创建一个 DefaultSessionFactory 对象； SqlSessionFactoryBuilder 返回 DefaultSessionFactory 对象给 Client，供 Client 使用。 源码解析 public SqlSessionFactory build(InputStream inputStream,String environment,Properties properties){ SqlSessionFactory var5; try{ //根据输入流创建XMLConfigBuilder对象 XMLConfigBuilder parser=new XMLConfigBuilder(inputStream,environment,properties); var5=this.build(parser.parse()); }catch(Exception var14){ throw ExceptionFactory.wrapException(\"Error building SqlSession.\",var14); }finally{ ErrorContext.instance().reset(); try{ inputStream.close(); }catch(IOException var13){ } } return var5; } //根据 Configuration 对象创建一个 DefaultSessionFactory 对象 public SqlSessionFactory build(Configuration config){return new DefaultSqlSessionFactory(config);} 1.2. 数据源与连接池 1.2.1. DataSource创建 MyBatis 把数据源 DataSource 分为三种： UNPOOLED 不使用连接池的数据源 ; POOLED 使用连接池的数据源 ; JNDI 使用 JNDI 实现的数据源 有源码可以看出使用了工厂模式 MyBatis 是通过工厂模式来创建数据源 DataSource 对象的，MyBatis 定义了抽象的工厂接口,通过其 getDataSource()方法返回数据源 DataSource。 1.2.2. Connection创建 当需要创建 SqlSession 对象并需要执行 SQL 语句时，这时候 MyBatis 才会去调用 dataSource 对象来创建 java.sql.Connection 对象 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/":{"url":"企业项目框架/Spring/","title":"Spring","keywords":"","body":"1. Spring概览1.1. 有哪些模块1.2. 框架中用了哪些设计模式1.3. SpringIOC/AOP1.4. Spring如何处理线程并发问题1.5. Spring如何解决循环依赖1. Spring概览 1.1. 有哪些模块 1.2. 框架中用了哪些设计模式 1.3. SpringIOC/AOP 1.4. Spring如何处理线程并发问题 1.5. Spring如何解决循环依赖 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/InitillizingBean.html":{"url":"企业项目框架/Spring/InitillizingBean.html","title":"Initillizing Bean","keywords":"","body":"1. 简介1.1. 扩展1.2. 源码解读1.3. JDK动态代理与CGLIB1. 简介 InitializingBean是spring为bean的初始化提供了一种新的方式，里面只有一个方法afterPropertiesSet，作用就是实现这个接口或者实现了继承InitializingBean的方法的bean都要执行这个方法。 1.1. 扩展 构造方法、注解postConstruct，实现InitializingBean方法afterPropertiesSet，bean初始化init方法执行顺序。 @Component public class MyInitializingBean implements InitializingBean { public MyInitializingBean() { System.out.println(\"我是MyInitializingBean构造方法执行...\"); } @Override public void afterPropertiesSet() throws Exception { System.out.println(\"我是afterPropertiesSet方法执行...\"); } @PostConstruct public void postConstruct() { System.out.println(\"我是postConstruct方法执行...\"); } public void init() { System.out.println(\"我是init方法执行...\"); } @Bean(initMethod = \"init\") public MyInitializingBean test() { return new MyInitializingBean(); } } 执行顺序优先级：构造方法>postConstruct>afterPropertiesSet>init方法 1.2. 源码解读 Spring加载bean的源码类AbstractAutowiredCapableBeanFactory可以看出其中的奥妙，AbstractAutowiredCapableBeanFactory类中的invokeInitMethods protected void invokeInitMethods(String beanName,final Object bean,RootBeanDefinition mbd)throws Throwable{ //判断该bean是否实现了实现了InitializingBean接口，如果实现了InitializingBean接口，则只掉调用bean的afterPropertiesSet方法 boolean isInitializingBean=(bean instanceof InitializingBean); if(isInitializingBean&&(mbd==null||!mbd.isExternallyManagedInitMethod(\"afterPropertiesSet\"))){ if(logger.isDebugEnabled()){ logger.debug(\"Invoking afterPropertiesSet() on bean with name '\"+beanName+\"'\"); } if(System.getSecurityManager()!=null){ try{ AccessController.doPrivileged(new PrivilegedExceptionAction(){ public Object run()throws Exception{ //直接调用afterPropertiesSet ((InitializingBean)bean).afterPropertiesSet(); return null; } },getAccessControlContext()); }catch(PrivilegedActionException pae){ throw pae.getException(); } } else{ //直接调用afterPropertiesSet ((InitializingBean)bean).afterPropertiesSet(); } } if(mbd!=null){ String initMethodName=mbd.getInitMethodName(); //判断是否指定了init-method方法，如果指定了init-method方法，则再调用制定的init-method if(initMethodName!=null&&!(isInitializingBean&&\"afterPropertiesSet\".equals(initMethodName))&& !mbd.isExternallyManagedInitMethod(initMethodName)){ //进一步查看该方法的源码，可以发现init-method方法中指定的方法是通过反射实现 invokeCustomInitMethod(beanName,bean,mbd); } } } 1.3. JDK动态代理与CGLIB JDK 动态代理： 其代理对象必须是某个接口的实现，它是通过在运行时创建一个接口的实现类来完成对目标对象的代理 CGLIB 代理：在运行时生成的代理对象是针对目标类扩展的子类。 CGLIB 是高效的代码生产包，底层是依靠 ASM 操作字节码实现的，性能比JDK强。相关标签 true 表示使用 CGLIB 代理 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/SpringAOP.html":{"url":"企业项目框架/Spring/SpringAOP.html","title":"Spring AOP","keywords":"","body":"1. SpringAOP面向切面编程1.1. AOP概述1.2. AspectJ是什么1. SpringAOP面向切面编程 SpringAOP个Aspect4J的关系 Spring框架如何实现AOP的，代理技术？静态代理，动态代理 https://pdai.tech/md/spring/spring-x-framework-aop.html 1.1. AOP概述 面向切面，更加解耦，将分散再各个业务模块重复的代码提取横向切割出来 1.2. AspectJ是什么 java实现的AOP框架，能够对java代码进行AOP编译，是目前实现AOP框架中最成熟，功能最丰富的框架。 AspectJ应用到java代码的过程（这个过程称为织入），对于织入这个概念，可以简单理解为aspect(切面)应用到目标函数( 类)的过程。 动态织入的方式是在运行时动态将要增强的代码织入到目标类中，这样往往是通过动态代理技术完成的，如Java JDK的动态代理(Proxy，底层通过反射实现)或者CGLIB的动态代理(底层通过继承实现)，Spring AOP采用的就是基于运行时增强的代理技术 ApectJ采用的就是静态织入的方式。ApectJ主要采用的是编译期织入，在这个期间使用AspectJ的acj编译器(类似javac) 把aspect类编译成class字节码后，在java目标类编译时织入，即先编译aspect类再编译目标类。 SpringAOP AspectJ 在纯 Java 中实现 使用 Java 编程语言的扩展实现 不需要单独的编译过程 除非设置 LTW，否则需要 AspectJ 编译器 (ajc) 只能使用运行时织入 运行时织入不可用。支持编译时、编译后和加载时织入 功能不强-仅支持方法级编织 更强大 - 可以编织字段、方法、构造函数、静态初始值设定项、最终类/方法等. 只能在由 Spring 容器管理的 bean 上实现 可以在所有域对象上实现 仅支持方法执行切入点 支持所有切入点 代理是由目标对象创建的, 并且切面应用在这些代理上 在执行应用程序之前 (在运行时) 前, 各方面直接在代码中进行织入 比 AspectJ 慢 更好的性能 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/Spring事务.html":{"url":"企业项目框架/Spring/Spring事务.html","title":"Spring事务","keywords":"","body":"1. Spring @Transactional1.1. Spring事务1.2. Spring事务需要解决的问题1.3. 传播机制--7种事务传播机制1.4. Spring事务失效的11种场景1. Spring @Transactional 概念：用户的一系列数据库操作，增删改查，这些操作可视为一个完整的逻辑处理工作单元，要么全部执行，要么全部不执行，是不可分割的工作单元。 1.1. Spring事务 编程式事务：类似于JDBC编程实现事务管理。管理使用TransactionTemplate或者直接使用底层的PlatformTransactionManager。对于编程式事务管理，Spring推荐使用TransactionTemplate 声明式事务：管理建立在 AOP 之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务 @Transactional注解：只能标注共有方法，可以加在方法以及类上，类上增加的整个类的所有公共方法都会支持事务。 rollbackFor，遇到异常即回滚 noRollbackFor，遇到指定异常不回滚 timeout单位为秒，超时属性，事务在强制回滚之前可以保持多久，可以防止长期运行的事务占用资源 readOnly，只读属性，表示这个事务只读取数据但不更新数据，这样可以帮助数据库引擎优化事务 1.2. Spring事务需要解决的问题 serviceA方法调用了serviceB方法，两个方法都有事务，这个时候serviceB方法异常，是serviceB方法提交，还是两个一起回滚 serviceA方法调用了serviceB方法，但是只有serviceA方法有事务，是否把serviceB也加入serviceA的事务，如果serviceB异常，是否回滚serviceA serviceA方法调用了serviceB方法，两者都有事务，serviceB方法已经正常执行完，但serviceA异常，是否需要回滚serviceB 1.3. 传播机制--7种事务传播机制 spring是用AOP来代理事务控制，是针对接口或类的，所以同一个service类中两个方法的调用，传播机制是不生效的。 原因：在spring中，当一个方法开启事务时，spring创建这个方法的类的bean对象，则创建该对象的代理对象。spring中调用bean对象的方法才会去判断方法上的注解。在代理bean对象中，一个方法调用本身的另一个方法，实则调用的代理对象的原始对象（不属于 spring bean）的方法，调用方法时不会去判断方法上的注解。这就是传播机制不生效的原因 解决：获取到当前service的代理类即可实现调用自己类的方法：自身类注入自己；AopContext.currentProxy来获取，但是此方法需要再启动类开启exposeProxy注释（@EnableAspectJAutoProxy( exposeProxy = true)） PROPAGATION_REQUIRED spring的默认事务传播类型required:如果当前没有事务，则新建事务； 如果已经存在事务，则加入当前事务，合并成一个事务 REQUIRES_NEW 新建事务，如果存在当前事务，则把当前事务挂起； 这个方法独立事务，不受调用者影响，调用者异常也不会影响当前事务提交 NESTED 当前没有事务，会新建事务 有事务，会作为父级事务的一个子事务，方法结束后并没有提交，等父事务提交它才提交 如果它异常，父级可以捕获它的异常而不进行回滚，正常提交 如果父级异常，它必然回滚 SUPPORTS 如果当前存在事务，就加入当前事务 如果不存在事务，则已无事务方式运行，和不写没区别 NOT_SUPPORTED 非事务运行 如果当前存在事务，将当前事务挂起 MANDATORY 如果当前有事务，则运行在当前事务中 如果当前没有事务，则抛出异常，即父方法必须有事务 NEVER 以非事务方法运行，如果当前有事务即抛异常；不允许父方法有事务 1.4. Spring事务失效的11种场景 访问权限问题，spring要求被代理方法必须是public的。spring源码种，如果目标方法不是public，则TransactionAttribute返回null，不支持事务 方法用final修饰，也会导致失效，因为spring事务底层是aop，用了jdk的动态代理或者cglib的动态代理，会生成代理类，在代理类中实现事务功能（static修饰同样失效） 直接调用内部方法，解决方案应该注入自己调用或者使用AopContext.currentProxy来获取 未被Spring管理，当然无法使用spring事务 多线程调用，重新new一个线程调用带事务的方法，因为线程不同，获取到数据库连接不一样，从而是两个不同的事务。spring事务时通过数据库连接实现的，当前线程保存了一个map,key是数据源，value是数据库连接 表不支持事务，如myisam引擎 事务没有开启，如果springboot项目，事务默认是开启的，但spring项目，需要xml配置 事务传播特性，只有PROPAGATION_REQUIRED、REQUIRES_NEW、NESTED这三种才会创建新事务 try...catch自己捕获了异常，导致事务不回滚 手动抛了spring事务不支持的异常，也不会回滚。spring事务，默认情况下只回滚RuntimeException运行时异常和Error错误，对于普通的非运行时异常，不会回滚 指定了rollbackFor异常回滚，但是并不是报的此类异常，也不会捕获回滚 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/TransactionSynchronizationManager.html":{"url":"企业项目框架/Spring/TransactionSynchronizationManager.html","title":"Transaction Synchronization Manager","keywords":"","body":"1. TransactionSynchronizationManager1. TransactionSynchronizationManager spring事务提供的注册回调接口的方法 代码示例： TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronizationAdapter() { @Override public void afterCommit() { AService.invoke(); } }); 源码：TransactionSynchronizationAdapter.java public interface TransactionSynchronization extends Flushable { /** 事务提交状态 **/ int STATUS_COMMITTED = 0; /** 事务回滚状态 **/ int STATUS_ROLLED_BACK = 1; /** 事务状态未知 **/ int STATUS_UNKNOWN = 2; /** 事务挂起 **/ void suspend(); /** 事务恢复 **/ void resume(); /** 将基础会话刷新到数据存储区（如Hibernate JPA的session） **/ void flush(); /** 在事务提交前触发，如果此处发生异常，会导致回滚 **/ void beforeCommit(boolean var1); /** 在beforeCommit之后，Commit/RoolBack之前，即使发生异常，也不会回滚 **/ void beforeCompletion(); /** 事务提交后执行 **/ void afterCommit(); /** 事务提交/回滚执行 **/ void afterCompletion(int var1); } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/大事务处理优化.html":{"url":"企业项目框架/Spring/大事务处理优化.html","title":"大事务处理优化","keywords":"","body":"1. 大事务1.1. 产生原因1.2. 影响1.3. 优化方案1.3.1. 编程式事务1.3.2. 将查询（select）放到事务外1. 大事务 耗时比较久的事务 1.1. 产生原因 操作数据多 调用rpc超时 有其他耗时操作 锁竞争 1.2. 影响 并发情况，连接池容易爆 锁定数据多，大量阻塞和锁超时 执行时间长，容易造成主从延迟 undo log 日志膨胀，增加了存储，降低查询性能 如果有异常，事务回滚耗时 1.3. 优化方案 @Transactional是声明式事务，操作不熟练，容易发生事务失效，问题不好排查。 声明式事务增加再方法尚，粒度较粗，嵌套业务比较多，不好控制事务范围 1.3.1. 编程式事务 TransactionTemplate @Resource private TransactionTemplate transactionTemplate; //减少事务颗粒度 transactionTemplate.execute((status)->{ insertBatch(costItemInfos); //插入记录表内容 insertCostItemChanged(); return Boolean.TRUE; }); 减少事务颗粒度，可控 1.3.2. 将查询（select）放到事务外 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/很老的东西.html":{"url":"企业项目框架/Spring/很老的东西.html","title":"很老的东西","keywords":"","body":"1. 一些存在很老的东西，还有在应用的应该没有了1. 一些存在很老的东西，还有在应用的应该没有了 Spring和SpringMVC相关 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/Spring/循环依赖.html":{"url":"企业项目框架/Spring/循环依赖.html","title":"循环依赖","keywords":"","body":"1. 循环依赖1.1. 源码解读1.2. 通过构造器注入构成的循环依赖，无法解决，只能抛出异常：BeanCurrentlyInCreationException1. 循环依赖 1.1. 源码解读 类：DefaultSingletonBeanRegistry 核心集合：singletonObjects、singletonFactories、earlySingletonObjects、registeredSingletons /** Cache of singleton objects: bean name to bean instance. */ private final Map singletonObjects=new ConcurrentHashMap<>(256); /** Cache of singleton factories: bean name to ObjectFactory. */ private final Map>singletonFactories=new HashMap<>(16); /** Cache of early singleton objects: bean name to bean instance. */ private final Map earlySingletonObjects=new ConcurrentHashMap<>(16); /** Names of beans that are currently in creation. */ private final Set singletonsCurrentlyInCreation= Collections.newSetFromMap(new ConcurrentHashMap<>(16)); 流程： 检测当前 bean 是否在 singletonObjects 中，在则直接返回缓存好的 bean；不在则检测 是否在 singletonFactories 中，在，则调用其 getObject 方法，返回，并从 singletonFactories 中移除，加入到 earlySingletonObjects 中。 正常创建，beforeSingletonCreation:检测当前 bean 是否在 singletonsCurrentlyInCreation， 如果存在，抛出异常。表示存在构造器循环依赖。如果不存在，则将当前 bean 加入。 bean 初始化，分为构造方法初始化、工厂方法初始化和简单初始化。如果是构造方法初 始化，那么递归地获取参数 bean。其他情况不会递归获取 bean。 addSingletonFactory:如果当前 bean 不在 singletonObjects 中，则将当前 bean 加入到 singletonFactories 中，并从 earlySingletonObjects 中移除。 调用用户初始化方法，比如 BeanPostProcesser、InitializingBean、init-method，有可能 返回代理后的 bean。 检测循环依赖，如果当前 bean 在 singletonObjects 中，则判断当前 bean(current bean) 与 singletonObjects 中的 bean(cached bean)是否是同一个，如果不是，那么说明当前 bean 是被代理过的，由于依赖当前 bean 的 bean 持有的是对 cached bean 的引用，这是不被允许的，所以会抛出 BeanCurrentlyInCreationException 异常。 afterSingletonCreation:将当前 bean 从 singletonsCurrentlyInCreation 中删除 addSingleton: 将当前bean加入到 singletonObjects，然后从singletonFactories, earlySingletonObjects中移除，结束 1.2. 通过构造器注入构成的循环依赖，无法解决，只能抛出异常：BeanCurrentlyInCreationException Spring 容器创建单例“A” Bean，首先检测 singletonFactories 是否包含 A，发现没有，于是正常创建，然后检测 A 是否包含在 singletonsCurrentlyInCreation中，没有，则将 A 放入。 构造方法初始化时需要 B 实例（A 尚未放入到 singletonFactories 中），于是调用了 getBean(B) 方法、 Spring 容器创建单例“B” Bean，首先检测 singletonFactories 是否包含 B，发现没有，于是正常创建，然后检测 B 是否包含在 singletonsCurrentlyInCreation中，没有，则将 B 放入。 构造方法初始化时需要 C 实例（B 尚未放入到 singletonFactories 中），于是调用了 getBean(C) 方法、 Spring 容器创建单例“C” Bean，首先检测 singletonFactories 是否包含 C，发现没有，于是正常创建，然后检测 C 是否包含在 singletonsCurrentlyInCreation中，没有，则将 C 放入。 构造方法初始化时需要 A 实例（C 尚未放入到 singletonFactories 中），于是调用了 getBean(A) 方法、 Spring 容器创建单例“A” Bean，首先检测 singletonFactories 是否包含 A，发现没有于是正常创建 ，然后检测A是否包含在 singletonsCurrentlyInCreation 中 ， 有 ， 抛 出 BeanCurrentlyInCreationException 异常。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringBoot/SpringApplication解析.html":{"url":"企业项目框架/SpringBoot/SpringApplication解析.html","title":"SpringApplication解析","keywords":"","body":"1. SpringApplication1.1. 详解1.1.1. 无参构造方法调用initialize()方法做初始化动作1.1.2. 启动方法run()1.1.3. SpringApplicationRunListeners 使用1. SpringApplication SpringBoot启动流程及各组件相互调用关系如图 1.1. 详解 1.1.1. 无参构造方法调用initialize()方法做初始化动作 /** * Create a new SpringApplication instance. The application context will load beans from the specified sources (see class-level documentation for details. The instance can be customized before calling run(String...). Params: sources – the bean sources See Also: run(Object, String[]), SpringApplication(ResourceLoader, Object...) */ @SuppressWarnings({\"unchecked\", \"rawtypes\"}) private void initialize(Object[]sources){ if(sources!=null&&sources.length>0){ this.sources.addAll(Arrays.asList(sources)); } //推断当前程序类型 this.webEnvironment=deduceWebEnvironment(); //使用SpringFactoriesLoader 实例化所有可用的初始器 setInitializers((Collection)getSpringFactoriesInstances(ApplicationContextInitializer.class)); //使用SpringFactoriesLoader 实例化所有可用的监听器 setListeners((Collection)getSpringFactoriesInstances(ApplicationListener.class)); //配置应用主方法所在类 this.mainApplicationClass=deduceMainApplicationClass(); } 1.1.2. 启动方法run() run方法完成了spring整个启动过程 /** * Run the Spring application, creating and refreshing a new * {@link ApplicationContext}. * @param args the application arguments (usually passed from a Java main method) * @return a running {@link ApplicationContext} */ public ConfigurableApplicationContext run(String... args) { //开启时钟计时 StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; FailureAnalyzers analyzers = null; //开启设置，让系统模拟不存在io设备 configureHeadlessProperty(); //初始化SpringApplicationRunListener 监听器，并进行封装 SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); //环境准备 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); //打印banner Banner printedBanner = printBanner(environment); context = createApplicationContext(); analyzers = new FailureAnalyzers(context); //容器初始化 prepareContext(context, environment, listeners, applicationArguments, printedBanner); //刷新上下文容器 refreshContext(context); afterRefresh(context, applicationArguments); listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } return context; } catch (Throwable ex) { handleRunFailure(context, listeners, analyzers, ex); throw new IllegalStateException(ex); } } 1.1.3. SpringApplicationRunListeners 使用 通过getSpringFactoriesInstances 获取到所有实现SpringApplicationRunListener 接口的实例，默认情况下该接口的实现类只有 EventPublishingRunListener 他的主要作用是作为springboot 的一个广播器 private SpringApplicationRunListeners getRunListeners(String[] args) { Class[] types = new Class[] { SpringApplication.class, String[].class }; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances( SpringApplicationRunListener.class, types, this, args)); } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringBoot/SpringBoot-Arthas.html":{"url":"企业项目框架/SpringBoot/SpringBoot-Arthas.html","title":"Spring Boot Arthas","keywords":"","body":"在项目中通过引入 [arthas-spring-boot-starter](https://search.maven.org/search?q=arthas-spring-boot-starter) 进行Java诊断 使用步骤 在项目的pom.xml引入 com.taobao.arthas arthas-spring-boot-starter ${arthas.version} 配置连接信息 2.1 配置arthas tunnel server实现远程管理 通过Arthas Tunnel Server/Client 来远程管理/连接多个Agent，因此我们首先要先安装Arthas Tunnel Server/Client。 a、下载部署arthas tunnel server b、启动arthas-tunnel-server java -jar arthas-tunnel-server-3.5.4-fatjar.jar c、在项目的yml做如下配置 arthas: agent-id: ${ARTHAS_AGENT_ID:hsehdfsfghhwertyfad} app-name: ${spring.application.name} tunnel-server: ${ARTHAS_TUNNEL_SERVER:ws://localhost:7777/ws} PS：agentId要保持唯一，否则会在tunnel server上冲突，不能正常工作 d、效果演示 2.2 在spring配置文件进行配置 arthas: # 通过http访问的端口 http-port: 8563 # 通过telnet访问的端口 telnet-port: 3658 session-timeout: 1800 # 绑定的ip ip: 0.0.0.0 PS： 如果配置 arthas.telnetPort为 -1 ，则不监听 telnet端口，如果配置 arthas.telnetPort为 0 ，则随机telnet端口。arthas.httpPort也同理。 效果演示： 命令文档 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringCloud/Eureka.html":{"url":"企业项目框架/SpringCloud/Eureka.html","title":"Eureka","keywords":"","body":"1.1. Eureka1.1.1. Eureka概念1.1.2. Eureka怎么实现高可用1.1.3. Eureka注册流程1.1.4. Eureka心跳机制1.1.5. Eureka和zk区别1.1. Eureka 微服务注册发现中心 1.1.1. Eureka概念 微服务系统中的其他服务使用Eureka客户端将其连接到Eureka Service中，并且维持心跳，可以通过Eureka Service来监控各个微服务是否运行正常1.1.2. Eureka怎么实现高可用 集群部署，注册多台互相注册 集群部署时，注册中心收到注册信息会判断是否是其他注册中心同步的信息，如果是客户端注册，那么他将会该客户端信息同步到其他注册中心去，否则收到信息不作任何操作（避免集群中信息同步的死循环）1.1.3. Eureka注册流程 客户端启动后，定时向Eureka服务端注册自己的服务信息(服务名、IP、端口等) 客户端启动时，首先创建一个心跳的定时任务，定时向服务端发送心跳信息，服务端对客户端心跳作出响应，如果响应状态为404，表示服务端没有该客户端的服务信息，此时客户端向服务端发送注册请求 服务端保存客户端信息：客户端通过Jersey框架（http框架）将服务实例信息发送到服务端，服务端将客户端信息放在一个ConcurrentHashMap对象中 客户端启动后，定时拉去服务端已保存的服务注册信息 客户端拉取服务端服务信息是通过一个定时任务定时拉取，每次拉取后刷新本地已保存的信息，需要时直接从本地获取 拉取服务端保存的服务注册信息后，就可以调用消费其他服务提供者提供的服务1.1.4. Eureka心跳机制 心跳机制：客户端启动后，会启动一个定时任务，定时向服务端发送心跳数据，告知服务端自己还存活，默认心跳时间间隔是30秒 服务剔除机制： ```text 如果开启了自我保护机制，那么所有服务，包括已过期的服务都不会被剔除 如果未开启自我保护机制，那么将判断最后一分钟收到的心跳数与一分钟收到心跳数临界值比较 一旦服务剔除机制开启，则eureka服务端不回直接剔除所有已过期的服务，而是通过随机数的方式进行剔除，避免自我保护开启之前将所有的服务给剔除 ```1.1.5. Eureka和zk区别 zk节点服务挂了要选举，选举期间注册服务瘫痪不可用，因为zk集群必须有一台主其他都是从 eureka各个节点平等，只要有一台eureka可用就可保证服务可用，数据都是最新的 eureka本质是一个工程，而zk只是一个进程 zk保证的是CP（一致性与分区容错性），eureka保证的是AP（可用性以及分区容错性） Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringCloud/Feign.html":{"url":"企业项目框架/SpringCloud/Feign.html","title":"Feign","keywords":"","body":"1.1. Feign1.1.1. 什么是Feign1.1.2. Feign组件详解1.1. Feign FeignClient 1.1.1. 什么是Feign 一个声明式的Web服务客户端，编写Web服务客户端只需要创建一个接口，然后添加注解即可 1.1.2. Feign组件详解 如图： contract契约组件 feign中通过定义API接口方式调用远程的Http API，调用Client时候增加一些注解描述这个调用API的基本信息，如请求是GET/POST Encoder编码组件 将请求信息采用指定的编码方式进行编码后传输 Decoder解码组件 将响应数据解码成对象 Logger日志记录 feign中记录日志，可以指定Logger级别以及自定义日志输出 Client请求执行组件 负责Http请求执行的组件，默认Client是通过JDK的HttpURLConnection发送请求，每次发送请求都会创建新的HttpURLConnection，所以feign的性能会很差。所以可以扩展使用Apache HttpClient基于连接池的高性能HTTP客户端 Retryer重试组件 内置重试器，HTTP请求出现IO异常，会限定一个最大重试次数来进行重试操作 InvocationHandlerFactory代理 采用JDK的动态代理生成代理对象 RequestInterceptor请求拦截器 可以为Feign添加多个拦截器，请求执行前设置一些扩展的参数信息 QueryMapEncoder参数查询 针对实体类参数查询的编码器 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringCloud/Hystrix.html":{"url":"企业项目框架/SpringCloud/Hystrix.html","title":"Hystrix","keywords":"","body":"1.1. Hystrix1.1.1. 什么是Hystrix1.1.2. 如何限流1.1. Hystrix 断路器 1.1.1. 什么是Hystrix 防雪崩利器，具有服务降级、服务熔断、服务隔离、监控等一些防雪崩的技术 防雪崩方式 服务降级：接口调用失败就调用本地方法返回一个空 服务熔断：接口调用失败进入调用接口提前定义好的一个熔断方法，返回错误信息 服务隔离：隔离服务之间相互影响 服务监控：服务发生调用时，将每秒请求数、成功请求数等运行指标记录1.1.2. 如何限流 限流算法： 计数器：控制单位时间内的请求数量 劣势：设每分钟请求数量60个，每秒处理1个请求，用户在 00:59 发送 60 个请求，在 01:00 发送 60 个请求 此时 2 秒钟有 120 个请求(每秒 60 个请求)，远远大于了每秒钟处理数量的阈值。（突刺现象） leaky bucket（漏桶）：规定固定容量的桶，进入的水无法管控数量、速度，但是对于流出的水我们可以控制速度 劣势：无法应对短时间突发流量（桶满了就丢弃） Token bucket令牌桶：可以准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringCloud/微服务SpringCloud.html":{"url":"企业项目框架/SpringCloud/微服务SpringCloud.html","title":"微服务SpringCloud","keywords":"","body":"1.1. SpringCloud1.1.1. SpringCloud是什么？1.1.2. SpringCloud优缺点1.1.3. SpingCloud与SpringBoot对应关系1.1.4. SpringCloud都有哪些组件1.1.5. SpringCloud与一些框架的区别1.1. SpringCloud 微服务架构 1.1.1. SpringCloud是什么？ 概念：一系列框架的有序集合，利用SpringBoot脚手架简化了分布式系统基础设施的开发，如：服务发现注册中心、配置中心、消息总线、负载均衡、数据监控等 将比较成熟的服务框架组合，通过SpringBoot风格再封装简化配置和实现原理1.1.2. SpringCloud优缺点 优点： ```text 耦合度低 并行开发，互不影响 配置简单化，注解支持 跨平台，可以不同模块使用不同开发语言 数据库独立 前后端分离，通过组件进行服务通信 支持单模块横向扩容，针对性提升访问效率与节省没必要的资源浪费 缺点：text 部署麻烦，模块多 针对数据管理麻烦，因为数据库进行拆分了 性能监控麻烦 ```1.1.3. SpingCloud与SpringBoot对应关系 1.1.4. SpringCloud都有哪些组件 很多，常用的 Eureka:服务注册发现 zuul:网关 feign：web服务客户端，调用使用http hystrix:断路器，有限流算法（计数器、漏桶、令牌桶） config:分布式统一配置管理 1.1.5. SpringCloud与一些框架的区别 SpringCloud和SpringBoot区别 SpringBoot相当于个脚手架，SpringCloud是使用SpringBoot脚手架开发的多个组件的服务框架 SpringCloud关注全局微服务协调整理治理，将SpringBoot开发的一个个单体服务合并管理 SpringCloud提供了丰富的组件，如服务发现注册、配置中心、路由网关、数据监控等SpringCloud和Dubbo区别 服务调用方式：Duubo是RPC调用，SpringCloud是Rest api 注册中心：dubbo是zookeeper（保证CP），SpringCloud是eureka（保证AP）,也可以是zk 网关：dubbo本身是一个rpc框架，并没有网关，springcloud有zuul Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringFramework/":{"url":"企业项目框架/SpringFramework/","title":"Spring Framework","keywords":"","body":"1. SpringFramework1.1. 阅读顺序1. SpringFramework v5.2.5-REALEASE 1.1. 阅读顺序 源码解析之ApplicationContext,介绍Spring上下文初始化一些重要步骤 源码解析之@Component注解的扫描，内容是@Component、@Service等注解的扫描过程 源码解析之@Configuration注解解析，@Configuration、@Bean、@Scope、@ComponentScan、@Import 源码解析之bean的创建/销毁，介绍bean的创建过程和bean的生命周期，BeanPostProcessor、Aware、InitalizingBean调用过程 SpringAOP源码解析aop:aspectj-autoproxy标签解析 SpringAOP代理创建，CGLIB/JDK动态代理分别什么时候使用 Spring事务源码解析，@Transactional注解的解析，如何匹配应该添加事务的接口和方法 Spring事务执行、提交、回滚过程分析 源码解析之整合Mybatis SpringBoot启动流程解析 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"企业项目框架/SpringFramework/源码解析之ApplicationContext.html":{"url":"企业项目框架/SpringFramework/源码解析之ApplicationContext.html","title":"源码解析之ApplicationContext","keywords":"","body":"1. 源码解析之ApplicationContext1.1. ClassPathXmlApplicationContext1. 源码解析之ApplicationContext 主入口：ApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"spring-config.xml\"); 1.1. ClassPathXmlApplicationContext ClassPathXmlApplicationContext public ClassPathXmlApplicationContext(String[] paths, Class clazz, @Nullable ApplicationContext parent) throws BeansException { super(parent); Assert.notNull(paths, \"Path array must not be null\"); Assert.notNull(clazz, \"Class argument must not be null\"); this.configResources = new Resource[paths.length]; for (int i = 0; i 刷新环境方法refresh Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"偏门但研究过的东西/SpringSecurity.html":{"url":"偏门但研究过的东西/SpringSecurity.html","title":"Spring Security","keywords":"","body":"1. SpringSecurity1. SpringSecurity 个人总结 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/Groovy/Groovy基础语法.html":{"url":"大数据组件/Groovy/Groovy基础语法.html","title":"Groovy基础语法","keywords":"","body":"1. Groovy1.1. 代码示例1. Groovy 使用java jvm虚拟机运行(jdk类库都可引入使用) 1.1. 代码示例 import java.time.LocalDate import java.time.LocalDateTime import java.time.format.DateTimeFormatter def main() { def params = \"{llv}\" println(\"1231\" + params) def startDate = \"2023-01-01\" def endDate = \"2023-01-31\" monthDemo(startDate, endDate) } def monthDemo(def startDate, def endDate) { def df = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\") def dtf = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\") def now = LocalDate.now() if (LocalDate.parse(endDate).isAfter(now)) { endDate = now.format(df) } def startDate1 = now.withDayOfMonth(1).format(df) def endDate1 = now.format(df) def startDate2 = LocalDate.parse(startDate).withDayOfMonth(1).format(df) def endDate2 = LocalDate.parse(startDate).withDayOfMonth(1).plusMonths(1).minusDays(1).format(df) //如果满足查询当前月份 if (startDate1.compareTo(startDate) == 0 && endDate1.compareTo(endDate) == 0) { isFullMonth = true startLastDate = LocalDate.parse(startDate).minusMonths(1).format(df) endLastDate = LocalDate.parse(endDate).minusMonths(1).format(df) } else if (startDate2.compareTo(startDate) == 0 && endDate2.compareTo(endDate) == 0) { //如果满足查询满月 isFullMonth = true startLastDate = LocalDate.parse(startDate).minusMonths(1).withDayOfMonth(1).format(df) endLastDate = LocalDate.parse(startDate).withDayOfMonth(1).minusDays(1).format(df) } else { isFullMonth = false } def startTime = LocalDateTime.parse(startDate + \" 00:00:00\", dtf).format(dtf) def endTime = LocalDateTime.parse(endDate + \" 23:59:59\", dtf).format(dtf) println(\"start: \" + startDate + \" end: \" + endDate) println(\"startTime: \" + startTime + \" endTime: \" + endTime) println(\"是否查询满月: \" + isFullMonth) } return main() Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/hadoop/初识.html":{"url":"大数据组件/hadoop/初识.html","title":"初识","keywords":"","body":"1. Hadoop概述1.1. Hadoop优点1.2. 组件组成1.3. HDFS架构概述1. Hadoop概述 1.1. Hadoop优点 高可靠：维护多个数据副本，即使Hadoop某个计算元素或存储出现故障也不会导致数据流失 高扩展：在集群分配任务数据，可方便地扩展节点 高效：在MapReduce的思想下，Hadoop是并行工作的，加快任务处理速度 高容错：自动将失败的任务重新分配 1.2. 组件组成 MapReduce(计算) Yarn(资源调度) HDFS(数据存储) Common(辅助工具) 1.3. HDFS架构概述 概述：Hadoop Distributed File System分布式文件系统，内置三种组件（） Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/hive/hive基础语法DDL.html":{"url":"大数据组件/hive/hive基础语法DDL.html","title":"hive基础语法DDL","keywords":"","body":"1. Hive1.1. Create/Drop/Alter/Use Database1.1.1. Create Database1.1.2. Drop Database1.1.3. Alter Database1. Hive https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/LanguageManual_DDL.html 1.1. Create/Drop/Alter/Use Database SCHEMA 和 DATABASE 的用法是相同的 1.1.1. Create Database 默认情况下，Hive 创建托管表，其中文件，元数据和统计信息由内部 Hive 进程 Management CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [MANAGEDLOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; 1.1.2. Drop Database DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; 1.1.3. Alter Database ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0) ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0) ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later) ALTER (DATABASE|SCHEMA) database_name SET MANAGEDLOCATION hdfs_path; -- (Note: Hive 4.0.0 and later) Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/hive/hive语法.html":{"url":"大数据组件/hive/hive语法.html","title":"hive语法","keywords":"","body":"1. Hive1.1. 遇到的语法1.1.1. insert into 和insert overwrite区别1.1.2. if语法1.1.3. order by id DESC nulls last 排序字段1.1.4. with as语法1.1.5. row_number over(partition by,order by)用法1.1.6. least/greatest函数1.1.7. 小数取整函数（floor，ceil，round函数）1.1.8. 内置时间函数datediff/date_add1. Hive https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/ 1.1. 遇到的语法 1.1.1. insert into 和insert overwrite区别 区别一 insert into ：其实是将数据追加到表的末尾，注意：不是覆盖，是追加。 insert overwrite:其实是将重写表（或分区）中的内容，即将原来的hive表（或分区）中的数据删除掉，再进行插入数据操作。（如果hive 表示分区表的话，insert overwrite 操作只是会重写当前分区的数据，是不会重写其他分区的数据的。） 区别二 hive > insert into dwd_user select * from ods_user; insert into 是可以省略table关键字 hive > insert overwrite table dwd_user select * from ods_user; 覆盖之前的数据，table关键字不可省略 1.1.2. if语法 if和case差不多，都是处理单个列的查询结果 表达式: if(boolean testCondition, T valueTrue, T valueFalseOrNull) 当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull （if中的等于条件用“=”或“==”均可） 1.1.3. order by id DESC nulls last 排序字段 order by后面可以有多列进行排序，默认按字典排序。 order by为全局排序。 order by需要reduce操作，且只有一个reduce，无法配置(因为多个reduce无法完成全局排序) 1.1.4. with as语法 with as就类似于一个视图或临时表，可以用来存储一部分的sql语句作为别名，不同的是with as 属于一次性的，而且必须要和其他sql一起使用才可以 with dwd_test as ( select user_id , user_name as name , trade_number as tradeNumber , business_type as businessType , win_bid_date as bidDate , loading_date as loadingDate , demand_order_code as demandOrderCode , user_mobile as userMobile from dwd_test.test001 where loading_date between '${startDate}' and '${endDate}' /*and user_id > 0*/ and user_type = ${userType} and business_type = ${businessType} and ${bdDataScope} ) select dt.* from dwd_test dt 1.1.5. row_number over(partition by,order by)用法 row_number() over(partition by 分组列 order by 排序列 desc) 在使用 row_number() over()函数的时候，over()里面的分组以及排序的执行晚于 where、group by、order by 的执行 partition：按照month分成区块 order by ：排序是在partition分成的区块中分别进行。 row_number()：对各个分区分别添加编号，类似于rownum的递增序列 实例：取分组内的排第二的数据可用此种方式 1.1.6. least/greatest函数 least函数 取多列最小值select least(-99, 0, 73) -- -99 存在null或者字符串，有null取null，有字符串取null，select least(-99, 0, 73, null) --null ;select least(-99, 0, 73, 'string') --null 存在日期，取最小日期 select least('2022-01-01','2022-06-01','2022-06-09') -- 2022-01-01 greatest函数 取多列最大值 select greatest(-99, 0, 73) --73 存在null取到null 存在日期，取最大日期 select greatest('2022-01-01','2022-06-01','2022-06-09') --2022-06-09（如果不确定日期有无空，可以设置空值默认时间，再用函数） 1.1.7. 小数取整函数（floor，ceil，round函数） floor()向下取整函数 select floor(1.4) # 结果是：1 ceil()向上取整 select ceil(1.4) #结果是：2 round()四舍五入 select round(1.455, 2) #结果是：1.46，即四舍五入到十分位 1.1.8. 内置时间函数datediff/date_add datediff函数 日期比较函数，返回结束日期减去开始日期的天数（两个日期必须是'yyyy-MM-dd'的格式，否则执行结果会是NULL） datediff(string enddate,string startdate) 例如：select datediff('2022-12-31','2022-12-20');执行结果11 date_add日期增加函数 返回开始日期startdate增加days天后的日期。 date_add(string startdate, intdays) 例如：select date_add('2022-12-20',11);执行结果:2022-12-31 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/member/会员大数据相关.html":{"url":"大数据组件/member/会员大数据相关.html","title":"会员大数据相关","keywords":"","body":"1.1. 会员大数据架构图1.1. 会员大数据架构图 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/Presto/PrestoSQL原理分析.html":{"url":"大数据组件/Presto/PrestoSQL原理分析.html","title":"PrestoSQL原理分析","keywords":"","body":"1. Presto1.1. Presto架构1.1.1. 组成部分1.1.2. 应用场景1.1.3. 遇到的语法问题1. Presto 官网 Presto is an open source SQL query engine that's fast, reliable, and efficient at scale. Use Presto to run interactive/ad hoc queries at sub-second performance for your high volume apps. 本身不提供存储，不是数据库，通过connector取连接对应的数据库，完成数据查询和计算。基于内存计算，适用交互式分析查询 1.1. Presto架构 master-slave架构，由一个Coordinator节点，一个Discovery 节点，和多个Worker节点组成 1.1.1. 组成部分 Coordinator(协调器): 负责解析SQL语句，生成执行计划，分发执行任务给Worker节点执行。 Discovery(发现服务): 通常内嵌于Coordinator节点中。 Worker(工作节点): 负责实际执行查询任务，从对应数据库中读取数据；Worker启动后向； Discovery Server服务注册，Coordinator从Discovery Server获得可以正常工作的Worker节点。 1.1.2. 应用场景 1.1.3. 遇到的语法问题 set session hive.insert_existing_partitions_behavior = 'overwrite'; 解析：presto不支持 insert overwrite，所以提供了三种模式： set session hive.insert_existing_partitions_behavior = 'overwrite'; set session hive.insert_existing_partitions_behavior = 'append'; set session hive.insert_existing_partitions_behavior = 'error'; Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/StarRocks-DorisDB/":{"url":"大数据组件/StarRocks-DorisDB/","title":"Star Rocks Doris DB","keywords":"","body":"1. StarRocks1. StarRocks 官方文档 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/StarRocks-DorisDB/DorisDB了解.html":{"url":"大数据组件/StarRocks-DorisDB/DorisDB了解.html","title":"DorisDB了解","keywords":"","body":"1. DorisDB1.1. 特性1.2. 分区/分桶/表1.2.1. 一定要设置分统，可以不设置分区1.2.2. 分区创建规则(要注意，发生过错误)1. DorisDB 极速SQL查询，向量化执行引擎，亚秒级查询延时。DorisDB采用关系模型， 使用严格的数据类型， 使用列式存储引擎， 通过编码和压缩技术， 降低读写放大. 使用向量化执行方式， 充分挖掘多核CPU的并行计算能力， 从而显著提升查询性能 1.1. 特性 DorisDB采用分布式架构，存储容量和计算能力可近似线性水平扩展。DorisDB集群的规模可扩展到数百节点，支持的数据规模可达到10PB级别 自治系统，管理简单 标准SQL，DorisDB支持标准的SQL语法，包括聚合，JOIN，排序，窗口函数，自定义函数等功能，用户可以通过标准的SQL对数据进行灵活的分析运算 流批导入，DorisDB支持实时和批量两种数据导入方式， 支持的数据源有Kafka， HDFS， 本地文件. 支持的数据格式有ORC， Parquet和CSV等. DorisDB可以实时消费Kafka数据来完成数据导入，保证数据不丢不重（exactly once）。DorisDB也可以从本地或者远程（HDFS）批量导入数据 1.2. 分区/分桶/表 1.2.1. 一定要设置分统，可以不设置分区 用户数据被水平划分为若干个数据分片（Tablet，也称作数据分桶）。每个 Tablet 包含若干数据行。各个 Tablet 之间的数据没有交集，并且在物理上是独立存储的。 多个 Tablet 在逻辑上归属于不同的分区（Partition）。一个 Tablet 只属于一个 Partition。而一个 Partition 包含若干个 Tablet。因为 Tablet 在物理上是独立存储的，所以可以视为 Partition 在物理上也是独立。Tablet 是数据移动、复制等操作的最小物理存储单元。 若干个 Partition 组成一个 Table。Partition 可以视为是逻辑上最小的管理单元。数据的导入与删除，都可以或仅能针对一个 Partition 进行 partition partition列可以指定一列或者多列，分区列必须为key列，当不使用partition by建表的时候，系统会自动生成一个和表名同名的，全值范围的partition，该partition对用户不可见，并且不可修改 range分区，分区列通常为时间列，以方便管理新旧数据，分区的删除不会改变已存在分区的范围。删除分区可能出现空洞。通过 VALUES LESS THAN 语句增加分区时，分区的下界紧接上一个分区的上界。 list分区，分区列支持 BOOLEAN, TINYINT, SMALLINT, INT, BIGINT, LARGEINT, DATE, DATETIME, CHAR, VARCHAR 数据类型，分区值为枚举值。只有当数据为目标分区枚举值其中之一时，才可以命中分区 bucket，分桶列的选择，是在 查询吞吐 和 查询并发 之间的一种权衡 1.2.2. 分区创建规则(要注意，发生过错误) 分区名称仅支持字母开头，由字母、数字和下划线组成。 仅支持以下类型的列作为 Range 分区列：TINYINT, SMALLINT, INT, BIGINT, LARGEINT, DATE, DATETIME。 分区为左闭右开区间，首个分区的左边界为最小值。 NULL 值只会存放在包含 最小值 的分区中。当包含最小值的分区被删除后，NULL 值将无法导入。 可以指定一列或多列作为分区列。如果分区值缺省，则会默认填充最小值。 本人就因为分区使用2023-03中划线导致在shell脚本中创建临时分区时报错 大数据Hadoop之——DorisDB介绍与环境部署（StarRocks） Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/StarRocks-DorisDB/DorisDB语法.html":{"url":"大数据组件/StarRocks-DorisDB/DorisDB语法.html","title":"DorisDB语法","keywords":"","body":"1. 语法1. 语法 查看表结构 DESC table; 查询分区信息 SHOW TEMPORARY PARTITIONS FROM table_name; //临时分区 SHOW PARTITIONS FROM table_name; 查看前端节点 SHOW PROC '/frontends' 查看后端节点 SHOW PROC '/backends' 查看表数据大小 SHOW DATA # 查看所有表大小 SHOW DATA FROM org_project_data_2 # 查看指定表大小 查看表分区 SHOW PARTITIONS FROM new_table_name 查看load数据的lable的任务执行情况 SHOW LOAD WHERE label=\"my_label1\" 重命名表 ALTER TABLE test_table RENAME new_table_name Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/调度/AzKaban/多任务流并行.html":{"url":"大数据组件/调度/AzKaban/多任务流并行.html","title":"多任务流并行","keywords":"","body":"1. 多任务流并行，一条失败影响另一条1.1. 失败任务，也可以设置重试1. 多任务流并行，一条失败影响另一条 azkaban在使用默认的设置下。当azkaban有并行的两条任务线时，其中一个节点失败，其他未依赖此节点的任务也会被取消。例如C任务失败，另一条线的D任务也被取消了 1.1. 失败任务，也可以设置重试 retries=5 # 重试次数 retry.backoff=3000 # 重试间隔时间 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/ClickHouse集群安装.html":{"url":"大数据组件/ClickHouse集群安装.html","title":"ClickHouse集群安装","keywords":"","body":"1.1.1. Clickhouse 集群安装1.1.2. 二、生成 /etc/metrika.xml 配置文件1.1.3. 三、启动和验证1.1.1. Clickhouse 集群安装 前期准备 1.设置主机名和hosts映射 2.关闭防火墙和SELinux 3.配置ntp时间同步 4.安装zookeeper和JDK环境 一、安装单机 (3台都操作,以CentOS7为例) 1.1 在线安装 从官方仓库安装 (使用root用户在所有节点操作) #使用脚本安装yum源 curl -s https://packagecloud.io/install/repositories/altinity/clickhouse/script.rpm.sh | sudo bash ​ #yum 安装 server 以及 client yum install -y clickhouse-server clickhouse-client ​ #查看是否安装完成 sudo yum list installed 'clickhouse*' 离线安装 如果网络无法连接就采用离线方式安装，上传rpm包到/apps/source/ [root@ddyw-data-center-dev01 ~]# cd /apps/source/ [root@ddyw-data-center-dev01 source]# ll -rw-r--r--. 1 root root 6384 7月 29 10:09 clickhouse-client-20.8.3.18-1.el7.x86_64.rpm -rw-r--r--. 1 root root 69093220 7月 29 10:09 clickhouse-common-static-20.8.3.18-1.el7.x86_64.rpm -rw-r--r--. 1 root root 36772044 7月 29 10:09 clickhouse-server-20.8.3.18-1.el7.x86_64.rpm -rw-r--r--. 1 root root 14472 7月 29 10:09 clickhouse-server-common-20.8.3.18-1.el7.x86_64.rpm [root@ddyw-data-center-dev01 source]# yum install -y clickhouse-* 1.2 修改配置参数 修改配置文件config.xml (在node01节点操作) 开起远程访问模式,配置数据目录修改，由于默认端口9000被占用，修改为9002 # vim /etc/clickhouse-server/config.xml 8123 9002 //更改默认的9000端口 9004 :: //注释打开 ​ /apps/clickhouse/data/ //设置数据存储目录 /apps/clickhouse/tmp/ //设置缓存目录 ​ # 添加ClickHouse分布式DDL记录自动清理配置 /clickhouse/task_queue/ddl 60 //搜索ddl 增加以下3行 86400 200 default --> ​ cleanup_delay_period：检查DDL记录清理的间隔，单位为秒，默认60秒。 task_max_lifetime：分布式DDL记录可以保留的最大时长，单位为秒，默认保留7天。 max_tasks_in_queue：分布式DDL队列中可以保留的最大记录数，默认为1000条。 1.1.2. 二、生成 /etc/metrika.xml 配置文件 2.1 新建metrika.xml文件, 配置集群信息 (在node01节点操作) # vim /etc/metrika.xml ​ ddyw-data-center-dev01 9002 ddyw-data-center-dev02 9002 ddyw-data-center-dev03 9002 ​ ​ ddyw-data-center-dev01 2181 ​ ddyw-data-center-dev02 2181 ddyw-data-center-dev03 2181 ​ ddyw-data-center-dev02 ​ ​ ::/0 ​ ​ 10000000000 0.01 lz4 ​ 2.2 修改users.xml配置文件,添加密码 (在node01节点操作) 修改默认用户密码以及添加新用户的密码，密码由下面随机生成 # 生成随机密码 [root@sky-node04 ~]# PASSWORD=$(base64 和之间插入自定义密码配置 [root@sky-node04 ~]# vim /etc/clickhouse-server/users.xml e4e727cd493ec6e316258dc7b68a56de401cce62a393dceb99f2a082695584ca ::/0 default default guest01 ::/0 default readonly 2.3 同步node01节点的配置到其他两台节点 (在node01节点操作) [root@sky-node04 ~]# scp /etc/clickhouse-server/config.xml root@sky-node05:/etc/clickhouse-server/ [root@sky-node04 ~]# scp /etc/clickhouse-server/config.xml root@sky-node06:/etc/clickhouse-server/ ​ [root@sky-node04 ~]# scp /etc/metrika.xml root@sky-node05:/etc/ [root@sky-node04 ~]# scp /etc/metrika.xml root@sky-node06:/etc/ ​ [root@sky-node04 ~]# scp /etc/clickhouse-server/users.xml root@sky-node05:/etc/clickhouse-server/ [root@sky-node04 ~]# scp /etc/clickhouse-server/users.xml root@sky-node06:/etc/clickhouse-server/ 1.1.3. 三、启动和验证 3.1 启动ClickServer (所有节点操作) 首先保证开启Zookeeper正常，每一个节点开启ck 创建相关目录及权限 mkdir -p /apps/clickhouse chown -R clickhouse:clickhouse /apps/clickhouse ​ #开机启动clickhouse-server systemctl enable clickhouse-server ​ #启动clickhouse-server systemctl start clickhouse-server ​ #查看clickhouse-server运行状态 systemctl status clickhouse-server ​ #关闭clickhouse-server systemctl stop clickhouse-server ​ #查看日志 tail -f /var/log/clickhouse-server/clickhouse-server.log tail -f /var/log/clickhouse-server/clickhouse-server.err.log ​ #开启Debug调试模式 clickhouse-server start 3.2 连接数据库 # clickhouse-client进入操作数据库 clickhouse-client --host= --port= --user= --password= ​ 例如： clickhouse-client --host=sky-node04 --port=9002 --user=dd01test --password='' ​ [root@sky-node04 ~]# clickhouse-client --host=sky-node04 --port=9002 --user=dd01test --password='' ClickHouse client version 20.8.3.18. Connecting to sky-node04:9002 as user dd01test. Connected to ClickHouse server version 20.8.3 revision 54438. ​ sky-node04 :) show databases //查看数据库 ​ SHOW DATABASES ​ ┌─name───────────────────────────┐ │ _temporary_and_external_tables │ │ default │ │ system │ └────────────────────────────────┘ ​ 3 rows in set. Elapsed: 0.001 sec. ​ sky-node04 :) select * from system.clusters; //查看系统表 sky-node04 :) select cluster,shard_num,replica_num,host_name,port,user from system.clusters; //或者使用指定字段的查询语句，方便观察，如下所示 ​ SELECT cluster, shard_num, replica_num, host_name, port, user FROM system.clusters ​ ┌─cluster───────────────────────────┬─shard_num─┬─replica_num─┬─host_name──┬─port─┬─user────┐ │ ddyw_ck_japan_test │ 1 │ 1 │ sky-node04 │ 9002 │ default │ │ ddyw_ck_japan_test │ 2 │ 1 │ sky-node05 │ 9002 │ default │ │ ddyw_ck_japan_test │ 3 │ 1 │ sky-node06 │ 9002 │ default │ │ test_cluster_two_shards │ 1 │ 1 │ 127.0.0.1 │ 9000 │ default │ │ test_cluster_two_shards │ 2 │ 1 │ 127.0.0.2 │ 9000 │ default │ │ test_cluster_two_shards_localhost │ 1 │ 1 │ localhost │ 9000 │ default │ │ test_cluster_two_shards_localhost │ 2 │ 1 │ localhost │ 9000 │ default │ │ test_shard_localhost │ 1 │ 1 │ localhost │ 9000 │ default │ │ test_shard_localhost_secure │ 1 │ 1 │ localhost │ 9440 │ default │ │ test_unavailable_shard │ 1 │ 1 │ localhost │ 9000 │ default │ │ test_unavailable_shard │ 2 │ 1 │ localhost │ 1 │ default │ └───────────────────────────────────┴───────────┴─────────────┴────────────┴──────┴─────────┘ ​ 11 rows in set. Elapsed: 0.002 sec. ​ sky-node04 :) 参考文档 初识ClickHouse——安装与入门 https://segmentfault.com/a/1190000038991230ClickHouse实战-ClickHouse安装部署 https://developer.aliyun.com/article/780045Clickhouse集群安装部署 https://www.cnblogs.com/biehongli/p/14462096.htmlClickhouse 集群安装(完整版) https://blog.csdn.net/weixin_42003671/article/details/112849897 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"大数据组件/数据研发流程.html":{"url":"大数据组件/数据研发流程.html","title":"数据研发流程","keywords":"","body":"1. 入门1.1. Hive/Doris1.1.1. hive是什么1.2. 数据仓库分层1.2.1. ODS1.2.2. DWD1.2.3. DWS1.2.4. ADS1. 入门 安装Groovy，新建Groovy项目 Groovy基本语法：https://blog.csdn.net/weixin_52851967/article/details/125390597 基础语法如下 package com.llh.test class GroovyTest { static void main(String[] args) { println \"hello world\" println(geBigDecimal()) System.out.println(\"这种方式也支持\") def def001 = getDef() println(def001.isBigDecimal()) //GString类型 def testName = \"call me\" //GString类型，Groovy特有 def text = \"Hello : ${testName}\" println testName.toString() println text.toString() //多行字符串保留格式 def mail = '''\\ abc efg ''' println(mail) //填充 def str = \"groovy hello\" println str.center(14, \"R\") println str.padLeft(14, \"L\") println str.padRight(14, \"0\") def str2 = \"G\" def str3 = \"g\" //比较unicode码 println str3 > str2 //比较字符串 println str3 str2 //字符串相减 println str - str3 // switch...case支持各种类型 switch (\"abc\") { case \"ll\": println \"ll\" break case \"ABC\": println \"123\" break default: println(\"456\") } //范围循环 def sum = 0 for (i in 0..9) { sum += i println(sum) } for (i in [\"张三\": 23, \"lucy\": 19, \"Bob\": 30]) { print(i.key) sum += i.value } } static def getDef() { return \"中文\" } /** * 获取字符串 * @return */ static String getString() { return \"1231231\" } static Integer getInteger() { return 123123 } static Long getLong() { //注释 /* 注释 */ return 123123123123L } static Double getDouble() { return 0.123123123123 } static Float getFloat() { return 0.123123123123 } static BigDecimal geBigDecimal() { return 123 } } 1.1. Hive/Doris 1.1.1. hive是什么 hive基于hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，提供类sql查询功能 基本语法 show create table vts_llh_dwd.first_test; show create table vts_llh_dwd.first_test; use vts_llh_dwd; create database if not exists vts_llh_dwd; use vts_llh_dwd; drop table if exists first_test; create table if not exists first_test ( id int comment '编号', name string comment '姓名' ) comment '第一张测试表' TBLPROPERTIES ( 'author' = 'llh', 'desc' = '第一个测试表', 'remark' = '备注信息' ); alter table vts_llh_dwd.first_test add columns (etl_update_time string comment '更新时间'); show tables; select * from vts_llh_dwd.first_test limit 10; 直接使用hadoop所面临的问题 人员学习成本太高 项目周期要求太短 MapReduce实现复杂查询逻辑开发难度太大 1.2. 数据仓库分层 数仓的分层的每一层都有自己的职责，同时都是基于下一层或者下面多层做数据处理之后的结果 1.2.1. ODS 贴源层，接近数据源的一层，数据最原始，最真实未经太多处理 还起到一个数据备份作用，特殊行业，ODS层数据甚至会保留一年甚至多年 日志数据估算，也尤为重要，磁盘一般需要保留30%缓冲空间，数据本身可以做压缩，数仓还要做分层，数据本身存储还会有备份机制（HDFS/Kafka等框架） ODS层也并不意味着一定就是未经任何处理的数据，企业级项目，因为真实环境可能存在采集错误，bug，网络等问题，会造成原始数据存在问题 字段缺失 数据字段不统一 格式错误 数据来源混乱 数据类型不易，如json,xml,text等 一般企业级ODS会对原始数据，做一些基本处理 数据来源区分 按时间分区存储，按天，年，月，日 基本数据处理，格式错误丢弃，关键信息丢失的过滤 当然，DWD层也是可以做这一层处理优化，关键看公司技术规范 ODS层建表时，如果时hive进行处理，一般建外部表 hive的外部表，相对的是业务表 外部表存放数据文件可以不是在hive的hdfs默认位置，hive对应表删除时，对应数据文件并不会被删除，这样可以防止误操作 业务表则相反，数据文件存放在hive对应的默认位置，表删除对应文件也会被删除 大数据开发，使用hive时，一般都是用外部表 1.2.2. DWD DWD又叫数据明细表，很多时候存储的是事实表为主 在DWD层，会有ETL（extract transform load） 提取转换加载处理，逻辑较复杂，用hive一般无法满足要求，这些逻辑一般是编写代码实现，使用脚本进行周期性调用 筛选字段，去除废弃，格式错误，丢失关键字段的信息 数据规范化，可能不同业务不同来源的数据类型空值不同，这时候会在DWD层做抹平。如：boolean类型有true/fals,1/0的；字符串代表空的有\"\"/null的；日期格式差异更大的 DWD存储数据，一般就是维度表，事实表，实体表等数据 维度表，一些维度信息，一般直接存储维度信息，表一般不会很大 事实表，就是表述一些事实信息，如订单，收藏，购物车这些，数据量大，一般存储维度主键，具体维度值在后续处理分析时再临时关联 实体表，类似javabean，用来描述信息，如优惠券表，促销表，内部就是描述信息，大部分时全量导入 事实表中数据，一般不是所有维度都按照维度主键做信息存储 数仓理论中，去除数据冗余的思想，一般会将维度信息单独存放，其他表要用时，记录对应维度的id。 即使维度数据变化，也不会影响到多个表，降低了数据冗余 做数据映射 如将GPS经纬度转换为省市区详细地址 ip地址也转换位省市区详细地址 将时间转换为年，月，日甚至周，季度维度信息 1.2.3. DWS 数据服务层，数据聚合层，为更上层的ADS层或者直接面向需求方服务 DWS建模，一般用主题建模，维度建模等方式 主题建模，围绕某一个业务主体进行数据建模，将相关数据抽离提取 流量绘画按照天，月聚合 每日新用户进行聚合 每日活跃用户聚合 2.维度建模，根据业务需要，提前将后续数据查询处理需要的维度数据抽离出来，方便后续查询用 运营位维度数据聚合 渠道拉新维度数据聚合 1.2.4. ADS 应用服务层，直接对接OLAP分析，或者业务层数据调用接口 最顶层，一般是结果类型数据，可以直接展示的数据，数据抽离分析成都最高的一层数据 需求最明确的一层，接口相对最固化 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"工具类/加解密类/加解密操作.html":{"url":"工具类/加解密类/加解密操作.html","title":"加解密操作","keywords":"","body":"1. 加解密操作1. 加解密操作 import javax.crypto.Cipher; import javax.crypto.SecretKey; import javax.crypto.spec.SecretKeySpec; public class Test { /** * 随意定一个私钥（长度必须为24位） */ public static final String SECRET_KEY = \"ABCDEFGHIJKLMN1234567890\"; /** * 加密 * * @param inStr 需要加密的内容 * @param secretKey 密钥 * @return 加密后的数据 */ public static String encrypt(String inStr, String secretKey) { SecretKey deskey = new SecretKeySpec(secretKey.getBytes(), \"DESede\"); Cipher cipher; String outStr = null; try { cipher = Cipher.getInstance(\"DESede\"); cipher.init(Cipher.ENCRYPT_MODE, deskey); outStr = byte2hex(cipher.doFinal(inStr.getBytes())); } catch (Exception e) { System.out.println(\"异常：\" + e); } return outStr; } /** * 解密 * * @param inStr 需要解密的内容 * @param secretKey 密钥 * @return 解密后的数据 */ public static String decrypt(String inStr, String secretKey) { SecretKey deskey = new SecretKeySpec(secretKey.getBytes(), \"DESede\"); Cipher cipher; String outStr = null; try { cipher = Cipher.getInstance(\"DESede\"); cipher.init(Cipher.DECRYPT_MODE, deskey); outStr = new String(cipher.doFinal(hex2byte(inStr))); } catch (Exception e) { System.out.println(\"异常：\" + e); } return outStr; } /** * 转化为16进制字符串方法 * * @param digest 需要转换的字节组 * @return 转换后的字符串 */ private static String byte2hex(byte[] digest) { StringBuffer hs = new StringBuffer(); String stmp = \"\"; for (int n = 0; n Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/":{"url":"数据库/","title":"数据库","keywords":"","body":"1. 数据库专区1.1. 分库分表框架ShardingSphere1. 数据库专区 mysql 1.1. 分库分表框架ShardingSphere Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/mysql/ACID与事务隔离级别.html":{"url":"数据库/mysql/ACID与事务隔离级别.html","title":"ACID与事务隔离级别","keywords":"","body":"1.1. ACID原理1.1.1. 事务并发问题1.1. ACID原理 （Atomicity）原子性，（Consistency）一致性，（Isolation）隔离性，（Durability）持久性 原子性：事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生 一致性：事务必须使数据库从一个一致性状态变换到另外一个一致性状态。 隔离性：事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。 持久性：持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响 1.1.1. 事务并发问题 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那A读取到的是脏数据 不可重复读：一个事务范围内，多次查询某个数据，得到不同结果。 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/mysql/MVCC.html":{"url":"数据库/mysql/MVCC.html","title":"MVCC","keywords":"","body":"1. MVCC1.1. 当前读1.2. 快照读1.3. 目的1.4. 数据库事务1.5. 实例1.6. MVCC解决了什么问题1.6.1. MVCC实现三大要素1. MVCC 概念：multi version concurrency control多版本并发控制 。MVCC 只在 REPEATABLE READ（可重复读） 和 READ COMMITTED（已读提交）这俩种隔离级别下适用 1.1. 当前读 给读操作加上共享锁、排它锁，DML 操作加上排它锁，这些操作就是当前读 读取的数据记录，都是最新的版本，会对当前读取的数据进行加锁，防止其他事务修改数据，是悲观锁的一种实现 如下操作都是当前读： select lock in share mode（共享锁） select for update（排他锁） update （排他锁） insert （排他锁） delete （排他锁） 串行化事务隔离级别 1.2. 快照读 普通的select id name user where id = 1;都属于快照读 MVCC实现 快照读读到的数据不一定是最新版本的数据 1.3. 目的 提供数据库读写性能，快照读，非当前读 读写时无须竞争锁来提高性能 1.4. 数据库事务 原子性由undo log实现 undolog 记录的是回滚日志，回滚指针指向了前一个是数据，根据回滚指针可以追溯到未变更前的所有数据形成了一个版本链 持久性是由reod log实现(WAL写前日志) 隔离性通过加索和MVCC实现 写写操作，通过行锁/表锁实现 写读操作，通过MVCC实现 ReadView介绍 作用：在select时可以知道在版本链中选用哪条记录 数据结构： 如何判断版本链中哪个版本可用？ MVCC如何实现RC(读已提交）和RR(可重复读) 读已提交生成ReadView的时机是每一次select语句时生成，所以访问的一定是最新的版本的，所以不能保证可重复读 RR生成ReadView的时机是以一个事务为单位的，RR可重复读会造成幻读，innodb是使用间隙锁锁住了区间，如查询>2的数据，则>2的数据都不会再插入来解决幻读 1.5. 实例 案例：需要支付一笔钱，两个人都登录进去了，这时候问题发生了 悲观并发控制PCC悲观锁（写多读少的并发环境下使用）： 假设A正在访问并且发起了支付，那么A这条数据是已经锁定了的，悲观的以为一定会发生锁竞争，所以直接加锁。这就导致了B查询不到此条数据，需要等待到A访问完毕释放锁。 悲观锁保证在同一时间只能有一个线程访问，默认数据在访问的时候会产生冲突，然后在整个过程都加上了锁 乐观并发控制OCC乐观锁： 在A访问并且支付的时候，B也可以访问到数据。乐观锁认为在并发场景下，也不会产生冲突，所以也不会加锁。而是在数据提交时检测，如果有冲突则返回冲突信息 总结：Innodb 的 MVCC 机制就是乐观锁的一种体现，读不加锁，读写不冲突，在不加锁的情况下能让多个事务进行并发读写，并且解决读写冲突问题，极大的提高系统的并发性。 1.6. MVCC解决了什么问题 事务并发情况遇到的问题 点这详解 脏读：读取到其它事务未提交的数据 不可重复读：一个事务读取一条数据时，由于另一个事务修改了这条数据，导致读取前后数据不一致 幻读：一个事务读取了某个范围数据的数量，另外的事务新增了这个范围的数据，再次读取发现得到结果不一致 MVCC 可以解决脏读，不可重复读，MVCC 使用快照读解决了部分幻读问题，但是在修改时还是使用的当前读，所以还是存在幻读问题，幻读的问题最终就是使用间隙锁解决。 1.6.1. MVCC实现三大要素 两个隐式字段、undo 日志、Read view 参考Bilibili 参考博客 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/mysql/Mysql实战-基础篇.html":{"url":"数据库/mysql/Mysql实战-基础篇.html","title":"Mysql实战-基础篇","keywords":"","body":"1.1. Mysql实战-基础篇1.1.1. 一条sql语句是怎么执行的1.1.2. 日志：一条sql更新语句是怎么执行的1.1.3. 事务隔离：为什么更改了还看不见？1.1.4. 深入浅出索引1.1.5. 全局锁和表级行锁1.1. Mysql实战-基础篇 Mysql核心 1.1.1. 一条sql语句是怎么执行的 mysql逻辑架构图如图 server层：包含了连接器、查询缓存、分析器、优化器、执行器等，所有的内置函数，所有跨存储引擎的功能都在这实现，如存储过程，触发器，视图等 存储引擎层：负责数据的存储和提取，是插件式的，有InnoDB,MyIsam等 执行步骤： 建立连接 连接器：mysql -h -p 建立连接过程复杂，建议使用长链接，长连接指建立一个连接成功后，如果客户端持续有请求，则一直使用同一个连接 如果全部使用长连接，会导致Mysql占用内存非常多--解决：定期断开长连接；可以在每个执行比较大的操作后，执行mysql_reset_connection重置连接 查询缓存 mysql8.0此功能删除，因为查询缓存功能弊大于利 分析器 词法分析，检测输入的sql语句格式，如果错误，会收到‘You have an error in your sql syntax’的提示 优化器 在表里有多个索引时，判断使用哪个索引；在一个语句有多个join连接的时候，判断连接顺序 执行器 开始执行前，会判断是否有执行此表的权限：无权限责任提示错误；有权限则根据引擎定义，开始执行引擎提供的接口 1.1.2. 日志：一条sql更新语句是怎么执行的 流程：一样走上面的逻辑流程，分析器检测sql语句判断是更新语句，会清空查询缓存内的信息 redo log重做日志，InnoDB特有的日志，保证了事务的持久性 WAL（write-ahead-logging）技术,先写日志，再写磁盘（区别AOF，写后日志，一个为了持久性数据不丢失；一个为了单线程操作不阻塞） 当有记录更新时，InnoDB引擎会把日志先写入redo log里，并更新内存，这时候就算更新完成，InnoDB会在适当时候，将记录更新到磁盘内 InnoDB的redo log是固定大小的 如图的write pos当前记录的位置；check point是当前要擦除的点（如果write pos追上check point表示固定大小的redo log满了） 有了redo log，InnoDB就能保证数据库发生异常重启后数据不丢失，这个能力称为crash-safe bin log归档日志（server层实现） 和redo log区别： redolog是InnoDB特有，bin log是mysql的server层实现，所有引擎都可以使用 redo log是物理日志，记录的是在某个数据页上做了什么修改，而binlog是逻辑日志，记录的是这个字段的原始逻辑，如：给ID=2这行的c字段加1 redo log是循环写，空间固定会用完，binlog是追加写入，在写入到一定大小会切换到下一个，并不会覆盖以前的日志 两阶段提交 如下sql语句执行流程：update T set c=c+1 where ID = 2 执行器先找引擎渠道ID=2这一行，ID是主键，引擎直接使用主键索引找到这一行，如果ID=2这行所在的数据页本来就在内存中，就直接返回给执行器，否则，需要先从磁盘读入内存，然后再返回 执行器拿到引擎给的行数据，把这个值加上1，得到新的一行数据，再调用引擎接口写入这行新数据 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态，然后告知执行器执行完成，随时可以提交事务 执行器生成这个操作的bin log，并把bin log写入磁盘 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交状态，更新完成 1.1.3. 事务隔离：为什么更改了还看不见？ 保证一组数据库操作，要么都成功，要么都失败 隔离性与隔离级别 四种事务隔离级别 read uncommited读未提交，一个事务未提交时，它的数据就能被别的事务看到（造成脏读） read commited读已提交，一个事务提交成功后，它的数据变更才能被别的事务看到（造成幻读） repeatable read可重复读，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。此事务隔离级别下，未提交变更对其他事务也是不可见的（mysql默认的事务隔离级别） serializable串行化，对于同行记录，写加写锁，读加读锁。并且事务执行有顺序 事务隔离的实现(详细) 每条记录在更新时，会同时记录一条回滚操作-undo log 回滚日志删除时机：系统内没有比这个回滚日志更早的read-view的时候 事务的启动方式 显示启动：begin或start transaction，提交commit，回滚rollback set autocommit=0,这个命令会将这个线程的自动提交关闭，直至主动执行commit或rollback，或者断开连接 1.1.4. 深入浅出索引 索引出现的目的是为了提高数据查询效率 索引常见模型 哈希表 以k-v存储数据的结构 值放在数组里，用一个哈希函数把key换算成一个确定的位置，把value放在数组这个位置，出现同一个值的情况：拉出一个链表 缺点：无序，哈希索引做区间查询速度很慢 适用场景：等值查询，如Memcached及其他一些NoSql引擎 有序数组 二分查找，查询速度快，时间复杂度O(log(N)) 缺点：如果往中间插入一个记录就必须挪动后面所有记录，成本太高 场景：适用于静态存储引擎 搜索树 二叉，多叉，树高度问题，高度越高，每个数据库寻址速度会累加 InnoDB的索引模型 InnoDB中，表根据主键顺序以索引的形式存放，这种存储方式表称为索引组织表 每个索引在InnoDB中对应一棵B+树 假设sqlmysql> create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 对应的索引树如图 根据叶子节点内容，索引类型分为：主键索引（聚簇索引），非主键索引 主键索引叶子节点存整行数据，非主键索引的叶子节点内容是主键的值 主键索引和普通索引区别：主键索引查询，只需要搜索ID这棵B+树；如果普通索引查询方式，需要先搜索普通索引树，得到ID值，再搜索ID索引树，这个过程称为--回表 索引维护 页分裂/页合并：B+树为了保证有序性，插入新值时需要挪动位置，如果数据页满了，再插入当前页中一个新值，则需要将部分数据挪动到新申请的一个数据页中，这个过程称为页分裂；当相邻页由于删除数据，利用率很低后会做页合并 自增主键有序，插入时向后插入，不会导致页分裂的产生，所以一般使用自增主键 查询索引树上包含的值无须回表的被称作覆盖索引，比如select ID from。。。可以直接查询到值 最左前缀原则：最左侧第一个数据有序，当第一个数据相等时，判断第二个数据，第二个数据在第一个数据相等时有序，所以联合索引如：（a,b,c）,查询为a=1 and b=3 and c=1，则可以使用到索引，如果是a > 1 and b =3 and c = 1则使用不到索引 索引下推：减少回表次数，多个查询条件时，会优先在普通用索引树内做过滤，再去回表过滤 1.1.5. 全局锁和表级行锁 全局锁：全局锁对整个数据库实例加锁，全局读锁方式：Flush tables with read lock.此时整个库处于只读状态，其他线程会被阻塞。（全库逻辑备份使用） 表级锁 表锁：lock tables ... read/write会限制别的线程的读写，也限定了本线程接下来的操作对象。除非执行unlock tables 元数据锁：无须显示使用，访问一个表时自动加上。对一个表操作增删改查操作时，加元数据读锁；对表数据结构变更操作时，加元数据写锁 读锁之间不互斥；读写锁之间，写锁之间互斥，保证变更表结构操作的安全性，如果由两个线程同时给一个表加字段，其中一个要等另一个执行完才能开始执行 给一个小表加字段，如 如上图，A,B都可以正常申请到读锁，不互斥；当alter添加表字段时增加写锁，D申请不到读锁。当业务有多个重试，连接可能很快爆满导致挂掉---解决：alter table 内设定等待时间，拿不到先放弃ALTER TABLE tbl_name WAIT N add column ... online DDL：优化解决了写锁的问题 拿元数据写锁 降级成元数据读锁 真正做DDL 升级成元数据写锁 实放元数据锁 行锁，引擎层自己实现 两阶段锁协议：在InnoDB事务中，行锁是在需要的时候加上的，并不是不需要了就立刻释放，而是要等到事务结束才释放 死锁和死锁检测：并发系统中不同线程出现循环资源依赖，涉及的线程都在等别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁 死锁两种策略：一直进入等待，直到超时，innodb_lock_wait_timeout设置，默认50s;发起死锁检测，发现死锁后，主动回滚某一个事务，让其他事务继续执行，innodb_deadlock_detect设置为on( 死锁检测会耗费大量CPU资源) 详细如图 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/mysql/Mysql实战-实践篇.html":{"url":"数据库/mysql/Mysql实战-实践篇.html","title":"Mysql实战-实践篇","keywords":"","body":"1. Mysql实践1.1. 普通索引和唯一索引，该怎么选1.1.1. 问题：从性能的角度考虑，你选择唯一索引还是普通索引？1.1.2. change buffer1.1.3. change buffer和redo log1.2. 给字符串字段加索引1.2.1. 使用查询语句，在两种索引结构内分别时怎么执行的？1.3. 为什么数据库会“抖”一下1.3.1. 脏页1.3.2. InnoDb刷脏页的控制策略1.4. 表数据删掉一半，表文件大小不变？1.4.1. 删除表中数据时的流程1.4.2. 页空洞1.4.3. 经过大量增删改的表，都可能存在空洞，如果去掉空洞，可以达到收缩表空间的目的1.4.4. 分布式ID（雪花算法生成，ID越来越大，但不是递增）生成的索引会比自增长的ID性能低吗？1.5. count(*)这么慢1.5.1. 实现1.5.2. 为什么InnoDB不能将数量存储？1.5.3. InnoDB在count(*)上做了什么优化？1.5.4. count是函数操作，作用在server层1. Mysql实践 实际应用，实际问题 1.1. 普通索引和唯一索引，该怎么选 场景:如一个市民系统，每个人都需一个唯一身份证号，而业务代码无法保证不会写入两个重复的身份证号 分析：身份证号肯定不建议做主键，因为字段比较大；给id_card字段创建唯一索引；创建普通索引。 1.1.1. 问题：从性能的角度考虑，你选择唯一索引还是普通索引？ 使用普通索引，因为唯一索引在更新过程中的时候，如果要更新的记录的目标数据页不在内存中，需要将数据页读入内存，这个过程会增加使用成本 查询过程 假设：select id from T where k=5（k有一棵主键索引树，叶子节点存储的是主键id） 通过B+树树根开始，按层搜索到叶子节点，将数据页加载到内存中，开始查找 普通索引：查找到满足条件的第一个记录（5，500）后，继续找下个记录，直到碰到第一个不满足k=5条件的记录 唯一索引，由于索引定义了唯一性，找到第一个满足条件的记录后，会停止继续检索 性能差距，微乎其微 更新过程 要插入的记录的目标也在内存中 InnoDB对于唯一索引来说，找到位置，判断没有冲突，插入这个值，语句执行结束 对于普通索引来说，找到位置，插入这个值，语句执行结束 要插入的记录的目标页不在内存中 唯一索引，需要将数据页读入内存，判断没有冲突，插入值，语句执行结束 普通索引，将更新记录在change buffer里，语句执行结束 1.1.2. change buffer 当需要更新一个数据页时，如果数据页在内存中直接更新，如果不再内存中，不影响数据一致性的前提下，InnoDB将这些更新操作缓存在channge buffer中，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作 将change buffer中的操作应用到数据页，得到最新结果的过程称为merge 访问这个数据页会触发merge 系统有后台线程会定期merge 数据库正常关闭过程中，也会merge 什么条件下使用 普通索引：merge时真正进行更新的时刻，所以merge之前，change buffer记录的变更越多，收益就越大；因此写多读少的业务场景，使用效果最好（账单，日志类） 唯一索引不能使用的原因是，所有更新操作都要先判断是否违反唯一性约束，判断方式是需要将数据页读入内存判断。数据页已进入内存，就没必要使用change buffer了 注意 change buffer使用的是buffer pool里的内存，不能无限增大。设置innodb_change_buffer_max_size动态设置 1.1.3. change buffer和redo log 示例：假设要执行如下插入语句：insert into t(id,k) values(id1,k1),(id2,k2); 假设当前k索引树的状态，查到位置后，k1所在的数据页在内存（InnoDB buffer pool）中，k2的数据页不在内存中 分析，涉及四个部分（内存，redo log,系统表空间-t.ibd，系统表空间-ibdata1） Page1在内存中，直接更新内存 Page2不在内存中，就在内存的change buffer区域，记录下“我要往Page2插入一行”这个信息 将上述两个动作记入redo log 执行这条更新语句：写了两处内存，写了一处磁盘，顺序写的 之后的读请求，如：select * from where k in (k1,k2) 如果发生在更新语句后不久，内存中数据在，此时读操作就与系统表空间(ibdata1)和redo log无关 读Page1时，直接从内存返回 读Page2时，要从磁盘读入内存，然后应用change buffer里的操作日志，生成一个正确的版本并返回结果 1.2. 给字符串字段加索引 场景：一个支持邮箱登录的系统，用户表定义： mysql > create table SUser ( ID bigint unsigned primary key, email varchar(64), . . . )engine=innodb; email字段两种索引方式 alter table SUser add index index1(email); 整个字符串建索引 alter table SUser add index index2(email(6)); 前缀索引，只取前6个字节 1.2.1. 使用查询语句，在两种索引结构内分别时怎么执行的？ select id,name,email from SUser where email='zhangssxyz@xxx.com' 如果使用的是email整个字符串的索引结构，执行顺序如： 从index1索引树找到满足索引值符合条件的这条记录，取得ID2的值 到主键索引上查到主键值是ID2的行，判断email值是否正确，将这行记录加入结果集 取index1索引树上刚刚查到的位置的下一条记录，发现不满足，循环结束 如果使用的是email(6)索引结构，执行顺序如： 从index2索引树找到满足条件的记录，找到第一个是ID1 到主键上查到主键值是ID1的行，判断出email的值不是完整匹配的email数据，这行记录丢弃 在index2上取下一条符合条件的记录，取出ID2，再到主键索引树上取值判断，这次对，将这行记录加入结果集 重复上一部，直到在index2上取到的值不是‘zhangs’时，循环结束 对比：如果使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本；使用前缀索引用不上覆盖索引对查询性能的优化。 1.3. 为什么数据库会“抖”一下 flush刷新脏页数据过程会导致“”抖 1.3.1. 脏页 InnoDb在处理更新语句时，只做了写日志redo log这一步操作，更新内存写完redolog后，返回给客户端。此时，当内存数据页跟磁盘数据页内容不一致时，这个内存页为“脏页” redo log 写满了，要flush脏页（应该尽量避免，出现这种情况，整个系统不能再接受更新，所有更新堵住） 内存不够用，需要先将脏页写到磁盘（常态，InnoDB用缓冲池管理内存） 空闲时操作，系统无压力 正常关闭mysql时，会把内存的脏页都flush到磁盘上 1.3.2. InnoDb刷脏页的控制策略 innodb_io_capacity参数，让InnoDb直到磁盘能力，可以设置为IOPS 多关注脏页比例，不要让它经常接近75% innodb_flush_neighbors参数1为连坐机制，比如刷一个脏页时会将旁边的一起刷掉，并且会一直蔓延（mysql8.0默认值为0--只刷自己的） 1.4. 表数据删掉一半，表文件大小不变？ drop table命令回收表空间，大小会变化 1.4.1. 删除表中数据时的流程 InnoDb引擎：索引结构如图，删除R4的数据时，引擎只会把R4这个记录标记为删除，如果之后再插入一个ID在300-600之间的记录，可能会复用这个位置 如果整个数据页的数据被删除呢？ 数据页可以被复用：PageA会被标记为可复用，如果要插入一条ID=50的记录需要使用新页的时候，PageA可以被复用 如果相邻的两个数据页利用率很小，系统会把两个页上数据合到一个页上，另一个数据页被标记为可复用 解答问题：如果用delete命令将整个表数据删除，所有的数据页都会被标记为可复用，磁盘上，文件不会变小 1.4.2. 页空洞 delete删除数据，页被标记为可复用，没有被使用的空间，就是页空洞 插入数据也会，如：按照索引顺序插入，索引紧凑；如果是随机插入，可能造成索引的数据页分裂 更新索引上的值，可以理解为删除一个旧值，再插入一个新值，也会造成空洞 1.4.3. 经过大量增删改的表，都可能存在空洞，如果去掉空洞，可以达到收缩表空间的目的 重建表（推荐使用gh-ost） alter table A engine = InnoDB命令（mysql会自动完成转存数据、交换表名、删除旧表的操作）锁表，不能有别的操作 思路：新建一个与表A结构相同的表B,然后按主键ID递增顺序，将数据一行行从表A读出插入到表B online DDL（开始复制临时数据时，会将元数据写锁降级为读锁） 建立临时文件，扫描A主键所有数据页 用数据页中A的记录生成B+树，存储到临时文件 生成临时文件的过程中，将所有对A的操作记录在一个日志文件（row log）中 临时文件生成后，将日志文件的操作应用到临时文件，得到一个逻辑数据上与表A相同的数据文件 用临时文件替换表A的数据文件 mysql5.5版本之后，加全文索引并不是online 1.4.4. 分布式ID（雪花算法生成，ID越来越大，但不是递增）生成的索引会比自增长的ID性能低吗？ 性能一样，没有一定要连续，只要是递增 1.5. count(*)这么慢 1.5.1. 实现 MyISAM引擎：一个表的总行数存在了磁盘上，执行count(*)时会直接返回这个数，效率很高 InnoDB引擎：执行count(*)，需要将数据一行行从引擎里读取出来，累积计数 1.5.2. 为什么InnoDB不能将数量存储？ 因为即使同一个时刻的多个查询，由于MVCC的原因，返回多少行也是不确定的（事务隔离级别有关） 1.5.3. InnoDB在count(*)上做了什么优化？ 保证逻辑正确的前提下，尽量减少扫描的数据量，数据库系统涉及的通用法则之一 （主键索引树比普通索引树大很多） show table status命令查询到的TABLE_ROWS也是采样估算的，误差可能达到40%-50% 1.5.4. count是函数操作，作用在server层 针对返回的结果集，一行行判断，count的字段值不是null,累计值+1 count(字段)标识返回满足条件的数据行里面，不为NULL的总个数 按照效率排序：count(字段) 因为count(*)不会把所有字段取出来，直接按行累加；别的字段的，需要先取出来，拷贝字段，再判断不为空，再累加 详细如图 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/mysql/常用SQL聚合.html":{"url":"数据库/mysql/常用SQL聚合.html","title":"常用SQL聚合","keywords":"","body":"1. SQL1.1. 查询1. SQL 1.1. 查询 统计表中是否有重复数据 select user_id, contract_code, count(contract_code) from vts_finance.deposit_contract_summary group by contract_code having count(contract_code) > 1; sql2 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/mysql/系统学习Mysql.html":{"url":"数据库/mysql/系统学习Mysql.html","title":"系统学习Mysql","keywords":"","body":"1. Mysql1.1. 索引简介1.1.1. 为什么维护逻辑删除字段1.1.2. 索引分类1.1.3. 什么时候需要创建索引1.1.4. 索引优/劣势1.2. 性能分析Explain1.2.1. Explain1.3. 索引优化1.4. 锁机制1. Mysql 数据库（粗粒度学习） 1.1. 索引简介 定义：索引是帮助mysql高效获取数据的--数据结构（自己理解：排好序的快速查找的数据结构） 索引并非存储在内存中，一般以文件形式存储在磁盘上 1.1.1. 为什么维护逻辑删除字段 为了数据分析 为了索引树，经常物理删除数据容易造成页分裂 1.1.2. 索引分类 单值索引:一个索引包含单个列 唯一索引:索引列的值必须唯一，允许有空值 复合索引:一个索引包含多列 1.1.3. 什么时候需要创建索引 需要创建 主键自动创建唯一索引 频繁作为查询条件的字段 排序字段 查询中统计或分组字段 无须创建 表记录太少 经常增删改的表 数据重复且分布平均的字段，比如类别、性格无须创建 1.1.4. 索引优/劣势 优势 提高数据检索效率，降低数据库的IO 通过所以列对数据进行排序，降低数据排序成本，降低CPU消耗 劣势 影响插入/修改/删除性能 索引列要占用空间 1.2. 性能分析Explain mysql常见瓶颈 CPU负担重，数据在装入内存或者从磁盘读取数据时 IO负担重，装入数据远大于内存容量时 服务器硬件性能瓶颈 1.2.1. Explain 查询执行计划，再执行sql之前增加explain id select查询的序列号，表示查询中执行select子句或操作表的顺序 值三种情况 相同，执行顺序由上至下 id不同，如果是自查询，id序号递增，id值越大优先级越高，越先被执行 id相同不同，id值越大，优先级越高，越先执行；相同，从上往下顺序执行 select_type 查询类型，如普通查询，子查询，联合查询等等 SIMPLE PRIMARY 主键查询 SUBQUERY DERIVED(临时表，衍生表) UNION 联合查询 UNION RESULT type(关键参数) 访问类型,现实查询使用了何种类型，从最好到最差排序为：system>const>eq_ref>ref>range>index>ALL.（优化至少达到range级别，最好能达到ref.） system：表只有一行记录，等于系统表，const类型的特例 const：通过索引一次就找到。会转换为常量 eq_ref：唯一性索引扫描，每个索引键，表中只有一条记录与之匹配；主键索引 ref：非唯一性索引扫描，匹配某个单独值的所有行。 range：只检索给定范围的行，使用一个索引来选择行，一般是where语句中出现了between,,in等查询 index：full index scan,index与all区别为index类型只遍历索引树，index也是读全表，但是读的是索引树，而All是从硬盘中读全表 ALL：从硬盘中读全表 possible_keys 显示可能应用在这张表中的索引，一个或多个，查询涉及到的字段上若存在索引，则该索引被列出，不一定被查询实际使用 key 实际使用到的索引，如果为NULL,则没有使用索引 若查询中使用了覆盖索引，则该索引仅出现在key列表中 key_len 表示索引中使用的字节数，通过该列计算查询中使用的索引的长度，不损失精度性的情况下，长度越短越好 值为索引字段的最大可能长度，并非实际使用长度，根据表定义计算而得，不是通过表内检索出 ref 显示索引的哪一列被使用。哪些列或常量被用于查找索引列上的值 rows 根据表统计信息及索引选用情况，大致估算出找到所需记录需要读取的行数 extra 包含不再其他列中，但十分重要的额外信息 using filesort：mysql会对数据使用一个外部的索引排序，无法利用索引完成的排序操作称为“文件排序”。性能极差 using temporary：需要建立临时表来暂存中间结果，性能也较差 using index：sql所需要的数据均在一棵索引树上，无需访问实际的行记录。此类sql往往查询性能较好 using where：sql使用了where条件过滤数据 using join buffer：需要进行嵌套循环计算，也往往较差，需要优化 impossible where select tables optimized away 1.3. 索引优化 索引失效场景 全值匹配 未遵循最佳左前缀原则 不再索引上有函数操作 存储引擎不能使用索引中范围条件（>, 减少select *,按需取数据 使用不等于（!=或者> is null,is not null也无法使用索引 like通配符开头（%abc...）会导致索引失效；（使用覆盖索引可以解决此问题，如age加索引，select age from table where name like '%qwe%'可以使用索引） 字符串不加引号导致索引失效（mysql做类型转换，如int转换为字符串） 少用or,用or连接也会导致索引失效 1.4. 锁机制 从数据操作的类型分类（读/写） 读锁：共享锁，针对同一份数据，多个读操作可以同时进行而不会相互影响 写锁：排他锁，当前写锁没有完成前，它会阻塞其他写锁和读锁 从对数据操作的粒度分类（表锁，行锁） 表锁：偏读，偏向MyISAM存储引擎，开销小，加锁快，无死锁 行锁：偏写 页锁 更详细可以查看 Mysql基础篇 Mysql实践篇 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据库/ShardingSphere/ShardingSphere简介及使用.html":{"url":"数据库/ShardingSphere/ShardingSphere简介及使用.html","title":"ShardingSphere简介及使用","keywords":"","body":"1.1. ShardingSphere​1.1.1. 简介1.1.2. 功能列表1.1.3. 安装使用1.1.4. 性能测试1.1.5. 问题点1.1. ShardingSphere​ Apache ShardingSphere 是一套开源的分布式数据库中间件解决方案组成的生态圈，它由 JDBC、Proxy 和 Sidecar（规划中）这 3 款相互独立，却又能够混合部署配合使用的产品组成。 它们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如 Java 同构、异构语言、云原生等各种多样化的应用场景。 Apache ShardingSphere 定位为关系型数据库中间件，旨在充分合理地在分布式的场景下利用关系型数据库的计算和存储能力，而并非实现一个全新的关系型数据库。 它通过关注不变，进而抓住事物本质。关系型数据库当今依然占有巨大市场，是各个公司核心业务的基石，未来也难于撼动，我们目前阶段更加关注在原有基础上的增量，而非颠覆。 Apache ShardingSphere 5.x 版本开始致力于可插拔架构，项目的功能组件能够灵活的以可插拔的方式进行扩展。 目前，数据分片、读写分离、数据加密、影子库压测等功能，以及 MySQL、PostgreSQL、SQLServer、Oracle 等 SQL 与协议的支持，均通过插件的方式织入项目。 1.1.1. 简介 ShardingSphere-JDBC 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。 适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC。 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等。 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库。 ShardingSphere-Proxy 定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 MySQL 和 PostgreSQL 版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端(如：MySQL Command Client, MySQL Workbench, Navicat 等)操作数据，对 DBA 更加友好。 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用。 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端。 ShardingSphere-Sidecar(TODO) 定位为 Kubernetes 的云原生数据库代理，以 Sidecar 的形式代理所有对数据库的访问。 通过无中心、零侵入的方案提供与数据库交互的的啮合层，即 Database Mesh，又可称数据库网格。 Database Mesh 的关注重点在于如何将分布式的数据访问应用与数据库有机串联起来，它更加关注的是交互，是将杂乱无章的应用与数据库之间的交互进行有效地梳理。 使用 Database Mesh，访问数据库的应用和数据库终将形成一个巨大的网格体系，应用和数据库只需在网格体系中对号入座即可，它们都是被啮合层所治理的对象。 ShardingSphere-JDBC ShardingSphere-Proxy ShardingSphere-Sidecar 数据库 任意 MySQL/PostgreSQL MySQL/PostgreSQL 连接消耗数 高 低 高 异构语言 仅 Java 任意 任意 性能 损耗低 损耗略高 损耗低 无中心化 是 否 是 静态入口 无 有 无 混合架构 ShardingSphere-JDBC 采用无中心化架构，适用于 Java 开发的高性能的轻量级 OLTP 应用；ShardingSphere-Proxy 提供静态入口以及异构语言的支持，适用于 OLAP 应用以及对分片数据库进行管理和运维的场景。 Apache ShardingSphere 是多接入端共同组成的生态圈。 通过混合使用 ShardingSphere-JDBC 和 ShardingSphere-Proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，使得架构师更加自由地调整适合与当前业务的最佳系统架构。 1.1.2. 功能列表 数据分片 背景 传统的将数据集中存储至单一数据节点的解决方案，在性能、可用性和运维成本这三方面已经难于满足互联网的海量数据场景。 从性能方面来说，由于关系型数据库大多采用 B+ 树类型的索引，在数据量超过阈值的情况下，索引深度的增加也将使得磁盘访问的 IO 次数增加，进而导致查询性能的下降；同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈。 从可用性的方面来讲，服务化的无状态型，能够达到较小成本的随意扩容，这必然导致系统的最终压力都落在数据库之上。而单一的数据节点，或者简单的主从架构，已经越来越难以承担。数据库的可用性，已成为整个系统的关键。 从运维成本方面考虑，当一个数据库实例中的数据达到阈值以上，对于 DBA 的运维压力就会增大。数据备份和恢复的时间成本都将随着数据量的大小而愈发不可控。一般来讲，单一数据库实例的数据的阈值在 1TB 之内，是比较合理的范围。 在传统的关系型数据库无法满足互联网场景需要的情况下，将数据存储至原生支持分布式的 NoSQL 的尝试越来越多。 但 NoSQL 对 SQL 的不兼容性以及生态圈的不完善，使得它们在与关系型数据库的博弈中始终无法完成致命一击，而关系型数据库的地位却依然不可撼动。 数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中以达到提升性能瓶颈以及可用性的效果。 数据分片的有效手段是对关系型数据库进行分库和分表。分库和分表均可以有效的避免由数据量超过可承受阈值而产生的查询瓶颈。 除此之外，分库还能够用于有效的分散对数据库单点的访问量；分表虽然无法缓解数据库压力，但却能够提供尽量将分布式事务转化为本地事务的可能，一旦涉及到跨库的更新操作，分布式事务往往会使问题变得复杂。 使用多主多从的分片方式，可以有效的避免数据单点，从而提升数据架构的可用性。 通过分库和分表进行数据的拆分来使得各个表的数据量保持在阈值以下，以及对流量进行疏导应对高访问量，是应对高并发和海量数据系统的有效手段。 数据分片的拆分方式又分为垂直分片和水平分片。 垂直分片 按照业务拆分的方式称为垂直分片，又称为纵向拆分，它的核心理念是专库专用。 在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库。 下图展示了根据业务需要，将用户表和订单表垂直分片到不同的数据库的方案。 水平分片 水平分片又称为横向拆分。 相对于垂直分片，它不再将数据根据业务逻辑分类，而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分。 例如：根据主键分片，偶数主键的记录放入 0 库（或表），奇数主键的记录放入 1 库（或表），如下图所示。 分片架构 数据分片 核心概念 SQL 逻辑表 水平拆分的数据库（表）的相同逻辑和数据结构表的总称。例：订单数据根据主键尾数拆分为 10 张表，分别是 t_order_0 到 t_order_9，他们的逻辑表名为 t_order。 真实表 在分片的数据库中真实存在的物理表。即上个示例中的 t_order_0 到 t_order_9 数据节点 数据分片的最小单元。由数据源名称和数据表组成，例：ds_0.t_order_0 绑定表 指分片规则一致的主表和子表。例如：t_order 表和 t_order_item 表，均按照 order_id 分片，则此两张表互为绑定表关系。绑定表之间的多表关联查询不会出现笛卡尔积关联，关联查询效率将大大提升。举例说明，如果 SQL 为： SELECT i.* FROM t_order o JOIN t_order_item i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); 在不配置绑定表关系时，假设分片键 order_id 将数值 10 路由至第 0 片，将数值 11 路由至第 1 片，那么路由后的 SQL 应该为 4 条，它们呈现为笛卡尔积: SELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); 在配置绑定表关系后，路由的 SQL 应该为 2 条： SELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id = i.order_id WHERE o.order_id in (10, 11); 其中 t_order 在 FROM 的最左侧，ShardingSphere 将会以它作为整个绑定表的主表。 所有路由计算将会只使用主表的策略，那么 t_order_item 表的分片计算将会使用 t_order 的条件。故绑定表之间的分区键要完全相同。 广播表 指所有的分片数据源中都存在的表，表结构和表中的数据在每个数据库中均完全一致。适用于数据量不大且需要与海量数据的表进行关联查询的场景，例如：字典表。 分片 分片键 用于分片的数据库字段，是将数据库（表）水平拆分的关键字段。例：将订单表中的订单主键的尾数取模分片，则订单主键为分片字段。 SQL 中如果无分片字段，将执行全路由，性能较差。 除了对单分片字段的支持，Apache ShardingSphere 也支持根据多个字段进行分片。 分片算法 通过分片算法将数据分片，支持通过 =、>=、、>、、BETWEEN 和 IN 分片。 分片算法需要应用方开发者自行实现，可实现的灵活度非常高。 目前提供4种分片算法。 由于分片算法和业务实现紧密相关，因此并未提供内置分片算法，而是通过分片策略将各种场景提炼出来，提供更高层级的抽象，并提供接口让应用开发者自行实现分片算法。 标准分片算法 对应 StandardShardingAlgorithm，用于处理使用单一键作为分片键的 =、IN、BETWEEN AND、>、、>=、进行分片的场景。需要配合 StandardShardingStrategy 使用。 复合分片算法 对应 ComplexKeysShardingAlgorithm，用于处理使用多键作为分片键进行分片的场景，包含多个分片键的逻辑较复杂，需要应用开发者自行处理其中的复杂度。需要配合 ComplexShardingStrategy 使用。 Hint分片算法 对应 HintShardingAlgorithm，用于处理使用 Hint 行分片的场景。需要配合 HintShardingStrategy 使用。 分片策略 包含分片键和分片算法，由于分片算法的独立性，将其独立抽离。真正可用于分片操作的是分片键 + 分片算法，也就是分片策略。目前提供 5 种分片策略。 标准分片策略 对应 StandardShardingStrategy。提供对 SQ L语句中的 =, >, , >=, , IN 和 BETWEEN AND 的分片操作支持。 StandardShardingStrategy 只支持单分片键，提供 PreciseShardingAlgorithm 和 RangeShardingAlgorithm 两个分片算法。 PreciseShardingAlgorithm 是必选的，用于处理 = 和 IN 的分片。 RangeShardingAlgorithm 是可选的，用于处理 BETWEEN AND, >, , >=, 分片，如果不配置 RangeShardingAlgorithm，SQL 中的 BETWEEN AND 将按照全库路由处理。 复合分片策略 对应 ComplexShardingStrategy。复合分片策略。提供对 SQL 语句中的 =, >, , >=, , IN 和 BETWEEN AND 的分片操作支持。 ComplexShardingStrategy 支持多分片键，由于多分片键之间的关系复杂，因此并未进行过多的封装，而是直接将分片键值组合以及分片操作符透传至分片算法，完全由应用开发者实现，提供最大的灵活度。 Hint分片策略 对应 HintShardingStrategy。通过 Hint 指定分片值而非从 SQL 中提取分片值的方式进行分片的策略。 不分片策略 对应 NoneShardingStrategy。不分片的策略。 SQLHint 对于分片字段非 SQL 决定，而由其他外置条件决定的场景，可使用 SQL Hint 灵活的注入分片字段。 例：内部系统，按照员工登录主键分库，而数据库中并无此字段。SQL Hint 支持通过 Java API 和 SQL 注释（待实现）两种方式使用。 详情请参见强制分片路由 配置 分片规则 分片规则配置的总入口。包含数据源配置、表配置、绑定表配置以及读写分离配置等。 数据源配置 真实数据源列表。 表配置 逻辑表名称、数据节点与分表规则的配置。 数据节点配置 用于配置逻辑表与真实表的映射关系。可分为均匀分布和自定义分布两种形式。 均匀分布 指数据表在每个数据源内呈现均匀分布的态势，例如： db0 ├── t_order0 └── t_order1 db1 ├── t_order0 └── t_order1 那么数据节点配置如下： db0.t_order0, db0.t_order1, db1.t_order0, db1.t_order1 自定义分布 指数据表呈现有特定规则的分布，例如： db0 ├── t_order0 └── t_order1 db1 ├── t_order2 ├── t_order3 └── t_order4 那么数据节点的配置如下： db0.t_order0, db0.t_order1, db1.t_order2, db1.t_order3, db1.t_order4 分片策略配置 对于分片策略存有数据源分片策略和表分片策略两种维度。 数据源分片策略 对应于 DatabaseShardingStrategy。用于配置数据被分配的目标数据源。 表分片策略 对应于 TableShardingStrategy。用于配置数据被分配的目标表，该目标表存在与该数据的目标数据源内。故表分片策略是依赖与数据源分片策略的结果的。 两种策略的 API 完全相同。 自增主键生成策略 通过在客户端生成自增主键替换以数据库原生自增主键的方式，做到分布式主键无重复。 行表达式 语法说明 行表达式的使用非常直观，只需要在配置中使用 ${ expression } 或 $->{ expression } 标识行表达式即可。 目前支持数据节点和分片算法这两个部分的配置。行表达式的内容使用的是 Groovy 的语法，Groovy 能够支持的所有操作，行表达式均能够支持。例如： ${begin..end} 表示范围区间 ${[unit1, unit2, unit_x]} 表示枚举值 行表达式中如果出现连续多个 ${ expression } 或 $->{ expression } 表达式，整个表达式最终的结果将会根据每个子表达式的结果进行笛卡尔组合。 例如，以下行表达式： ${['online', 'offline']}_table${1..3} 配置数据节点 对于均匀分布的数据节点，如果数据结构如下： db0 ├── t_order0 └── t_order1 db1 ├── t_order0 └── t_order1 用行表达式可以简化为： db${0..1}.t_order${0..1} 或者 db$->{0..1}.t_order$->{0..1} 对于自定义的数据节点，如果数据结构如下： db0 ├── t_order0 └── t_order1 db1 ├── t_order2 ├── t_order3 └── t_order4 用行表达式可以简化为： db0.t_order${0..1},db1.t_order${2..4} 或者 db0.t_order$->{0..1},db1.t_order$->{2..4} 对于有前缀的数据节点，也可以通过行表达式灵活配置，如果数据结构如下： db0 ├── t_order_00 ├── t_order_01 ├── t_order_02 ├── t_order_03 ├── t_order_04 ├── t_order_05 ├── t_order_06 ├── t_order_07 ├── t_order_08 ├── t_order_09 ├── t_order_10 ├── t_order_11 ├── t_order_12 ├── t_order_13 ├── t_order_14 ├── t_order_15 ├── t_order_16 ├── t_order_17 ├── t_order_18 ├── t_order_19 └── t_order_20 db1 ├── t_order_00 ├── t_order_01 ├── t_order_02 ├── t_order_03 ├── t_order_04 ├── t_order_05 ├── t_order_06 ├── t_order_07 ├── t_order_08 ├── t_order_09 ├── t_order_10 ├── t_order_11 ├── t_order_12 ├── t_order_13 ├── t_order_14 ├── t_order_15 ├── t_order_16 ├── t_order_17 ├── t_order_18 ├── t_order_19 └── t_order_20 可以使用分开配置的方式，先配置包含前缀的数据节点，再配置不含前缀的数据节点，再利用行表达式笛卡尔积的特性，自动组合即可。 上面的示例，用行表达式可以简化为： db${0..1}.t_order_0${0..9}, db${0..1}.t_order_${10..20} 或者 db$->{0..1}.t_order_0$->{0..9}, db$->{0..1}.t_order_$->{10..20} 配置分片算法 对于只有一个分片键的使用 = 和 IN 进行分片的 SQL，可以使用行表达式代替编码方式配置。 行表达式内部的表达式本质上是一段 Groovy 代码，可以根据分片键进行计算的方式，返回相应的真实数据源或真实表名称。 例如：分为 10 个库，尾数为 0 的路由到后缀为 0 的数据源， 尾数为 1 的路由到后缀为 1 的数据源，以此类推。用于表示分片算法的行表达式为： ds${id % 10} 或者 ds$->{id % 10} 内核剖析 解析引擎 抽象语法树 解析过程分为词法解析和语法解析。 词法解析器用于将 SQL 拆解为不可再分的原子符号，称为 Token。并根据不同数据库方言所提供的字典，将其归类为关键字，表达式，字面量和操作符。 再使用语法解析器将 词法解析器的输出 转换为抽象语法树。 例如，以下 SQL： SELECT id, name FROM t_user WHERE status = 'ACTIVE' AND age > 18 解析之后的为抽象语法树见下图。 为了便于理解，抽象语法树中的关键字的 Token 用绿色表示，变量的 Token 用红色表示，灰色表示需要进一步拆分。 最后，通过visitor对抽象语法树遍历构造域模型，通过域模型(SQLStatement)去提炼分片所需的上下文，并标记有可能需要改写的位置。 供分片使用的解析上下文包含查询选择项（Select Items）、表信息（Table）、分片条件（Sharding Condition）、自增主键信息（Auto increment Primary Key）、排序信息（Order By）、分组信息（Group By）以及分页信息（Limit、Rownum、Top）。 SQL 的一次解析过程是不可逆的，一个个 Token 按 SQL 原本的顺序依次进行解析，性能很高。 考虑到各种数据库 SQL 方言的异同，在解析模块提供了各类数据库的 SQL 方言字典。 SQL 解析引擎 功能点 提供独立的SQL解析功能 可以非常方便的对语法规则进行扩充和修改(使用了ANTLR) 支持多种方言的SQL解析 API使用 # 引入maven依赖 org.apache.shardingsphere shardingsphere-sql-parser-engine ${project.version} // 根据需要引入指定方言的解析模块（以MySQL为例）,可以添加所有支持的方言，也可以只添加使用到的 org.apache.shardingsphere shardingsphere-sql-parser-mysql ${project.version} 获取语法树 /** * databaseType type:String 可能值 MySQL,Oracle，PostgreSQL，SQL92，SQLServer * sql type:String 解析的SQL * useCache type:boolean 是否使用缓存 * @return parse tree */ ParseTree tree=new SQLParserEngine(databaseType).parse(sql,useCache); 获取SQLStatement /** * databaseType type:String 可能值 MySQL,Oracle，PostgreSQL，SQL92，SQLServer * useCache type:boolean 是否使用缓存 * @return SQLStatement */ ParseTree tree=new SQLParserEngine(databaseType).parse(sql,useCache); SQLVisitorEngine sqlVisitorEngine=new SQLVisitorEngine(databaseType,\"STATEMENT\"); SQLStatement sqlStatement=sqlVisitorEngine.visit(tree); SQL格式化 /** * databaseType type:String 可能值 MySQL * useCache type:boolean 是否使用缓存 * @return String */ ParseTree tree=new SQLParserEngine(databaseType).parse(sql,useCache); SQLVisitorEngine sqlVisitorEngine=new SQLVisitorEngine(databaseType,\"FORMAT\"); String formatedSql=sqlVisitorEngine.visit(tree); 路由引擎 根据解析上下文匹配数据库和表的分片策略，并生成路由路径。 对于携带分片键的 SQL，根据分片键的不同可以划分为单片路由(分片键的操作符是等号)、多片路由(分片键的操作符是 IN)和范围路由(分片键的操作符是 BETWEEN)。 不携带分片键的 SQL 则采用广播路由。 分片路由 用于根据分片键进行路由的场景，又细分为直接路由、标准路由和笛卡尔积路由这 3 种类型 直接路由 满足直接路由的条件相对苛刻，它需要通过 Hint（使用 HintAPI 直接指定路由至库表）方式分片，并且是只分库不分表的前提下，则可以避免 SQL 解析和之后的结果归并。 因此它的兼容性最好，可以执行包括子查询、自定义函数等复杂情况的任意 SQL。直接路由还可以用于分片键不在 SQL 中的场景。例如，设置用于数据库分片的键为 3 hintManager.setDatabaseShardingValue(3); 假如路由算法为 value % 2，当一个逻辑库 t_order 对应 2 个真实库 t_order_0 和 t_order_1 时，路由后 SQL 将在 t_order_1 上执行。下方是使用 API 的代码样例： String sql=\"SELECT * FROM t_order\"; try( HintManager hintManager=HintManager.getInstance(); Connection conn=dataSource.getConnection(); PreparedStatement pstmt=conn.prepareStatement(sql)){ hintManager.setDatabaseShardingValue(3); try(ResultSet rs=pstmt.executeQuery()){ while(rs.next()){ //... } } } 标准路由 标准路由是 ShardingSphere 最为推荐使用的分片方式，它的适用范围是不包含关联查询或仅包含绑定表之间关联查询的 SQL。 当分片运算符是等于号时，路由结果将落入单库（表），当分片运算符是 BETWEEN 或 IN 时，则路由结果不一定落入唯一的库（表），因此一条逻辑 SQL 最终可能被拆分为多条用于执行的真实 SQL。 举例说明，如果按照 order_id 的奇数和偶数进行数据分片，一个单表查询的 SQL 如下： SELECT*FROM t_order WHERE order_id IN(1,2); 那么路由的结果为： SELECT * FROM t_order_0 WHERE order_id IN (1, 2); SELECT * FROM t_order_1 WHERE order_id IN (1, 2); 绑定表的关联查询与单表查询复杂度和性能相当。举例说明，如果一个包含绑定表的关联查询的 SQL 如下： SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id = i.order_id WHERE order_id IN (1, 2); SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id = i.order_id WHERE order_id IN (1, 2); 笛卡尔路由 笛卡尔路由是最复杂的情况，它无法根据绑定表的关系定位分片规则，因此非绑定表之间的关联查询需要拆解为笛卡尔积组合执行。 如果上个示例中的 SQL 并未配置绑定表关系，那么路由的结果应为： SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id = i.order_id WHERE order_id IN (1, 2); SELECT * FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id = i.order_id WHERE order_id IN (1, 2); SELECT * FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id = i.order_id WHERE order_id IN (1, 2); SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id = i.order_id WHERE order_id IN (1, 2); 广播路由 对于不携带分片键的 SQL，则采取广播路由的方式。根据 SQL 类型又可以划分为全库表路由、全库路由、全实例路由、单播路由和阻断路由这 5 种类型。 全库表路由 全库表路由用于处理对数据库中与其逻辑表相关的所有真实表的操作，主要包括不带分片键的 DQL 和 DML，以及 DDL 等。例如： SELECT * FROM t_order WHERE good_prority IN (1, 10); 则会遍历所有数据库中的所有表，逐一匹配逻辑表和真实表名，能够匹配得上则执行。路由后成为 SELECT * FROM t_order_0 WHERE good_prority IN (1, 10); SELECT * FROM t_order_1 WHERE good_prority IN (1, 10); SELECT * FROM t_order_2 WHERE good_prority IN (1, 10); SELECT * FROM t_order_3 WHERE good_prority IN (1, 10); 全库路由 全库路由用于处理对数据库的操作，包括用于库设置的 SET 类型的数据库管理命令，以及 TCL 这样的事务控制语句。 在这种情况下，会根据逻辑库的名字遍历所有符合名字匹配的真实库，并在真实库中执行该命令，例如 SET autocommit=0; 在 t_order 中执行，t_order 有 2 个真实库。则实际会在 t_order_0 和 t_order_1 上都执行这个命令 全实例路由 全实例路由用于 DCL 操作，授权语句针对的是数据库的实例。无论一个实例中包含多少个 Schema，每个数据库的实例只执行一次。例如： CREATE USER customer@127.0.0.1 identified BY '123'; 这个命令将在所有的真实数据库实例中执行，以确保 customer 用户可以访问每一个实例。 单波路由 单播路由用于获取某一真实表信息的场景，它仅需要从任意库中的任意真实表中获取数据即可。例如： DESCRIBE t_order; t_order 的两个真实表 t_order_0，t_order_1 的描述结构相同，所以这个命令在任意真实表上选择执行一次。 阻断路由 阻断路由用于屏蔽 SQL 对数据库的操作，例如： USE order_db; 这个命令不会在真实数据库中执行，因为 ShardingSphere 采用的是逻辑 Schema 的方式，无需将切换数据库 Schema 的命令发送至数据库中。 路由引擎的整体结构划分如下图。 分布式事务 数据库事务需要满足 ACID（原子性、一致性、隔离性、持久性）四个特性。 原子性（Atomicity）指事务作为整体来执行，要么全部执行，要么全不执行。 一致性（Consistency）指事务应确保数据从一个一致的状态转变为另一个一致的状态。 隔离性（Isolation）指多个事务并发执行时，一个事务的执行不应影响其他事务的执行。 持久性（Durability）指已提交的事务修改数据会被持久保存。 在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为本地事务。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。 但是在基于微服务的分布式应用环境下，越来越多的应用场景要求对多个服务的访问及其相对应的多个数据库资源能纳入到同一个事务当中，分布式事务应运而生。 关系型数据库虽然对本地事务提供了完美的 ACID 原生支持。 但在分布式的场景下，它却成为系统性能的桎梏。如何让数据库在分布式场景下满足 ACID 的特性或找寻相应的替代方案，是分布式事务的重点工作。 本地事务 在不开启任何分布式事务管理器的前提下，让每个数据节点各自管理自己的事务。 它们之间没有协调以及通信的能力，也并不互相知晓其他数据节点事务的成功与否。 本地事务在性能方面无任何损耗，但在强一致性以及最终一致性方面则力不从心。 两阶段提交 XA协议最早的分布式事务模型是由 X/Open 国际联盟提出的 X/Open Distributed Transaction Processing (DTP) 模型，简称 XA 协议。 基于XA协议实现的分布式事务对业务侵入很小。 它最大的优势就是对使用方透明，用户可以像使用本地事务一样使用基于XA协议的分布式事务。 XA协议能够严格保障事务 ACID 特性。 严格保障事务 ACID 特性是一把双刃剑。 事务执行在过程中需要将所需资源全部锁定，它更加适用于执行时间确定的短事务。 对于长事务来说，整个事务进行期间对数据的独占，将导致对热点数据依赖的业务系统并发性能衰退明显。 因此，在高并发的性能至上场景中，基于XA协议的分布式事务并不是最佳选择。 柔性事务 如果将实现了 ACID 的事务要素的事务称为刚性事务的话，那么基于 BASE 事务要素的事务则称为柔性事务。 BASE 是基本可用、柔性状态和最终一致性这三个要素的缩写。 基本可用（Basically Available）保证分布式事务参与方不一定同时在线。 柔性状态（Soft state）则允许系统状态更新有一定的延时，这个延时对客户来说不一定能够察觉。 而最终一致性（Eventually consistent）通常是通过消息传递的方式保证系统的最终一致性。 在 ACID 事务中对隔离性的要求很高，在事务执行过程中，必须将所有的资源锁定。 柔性事务的理念则是通过业务逻辑将互斥锁操作从资源层面上移至业务层面。通过放宽对强一致性要求，来换取系统吞吐量的提升。 基于 ACID 的强一致性事务和基于 BASE 的最终一致性事务都不是银弹，只有在最适合的场景中才能发挥它们的最大长处。 可通过下表详细对比它们之间的区别，以帮助开发者进行技术选型。 本地事务 两（三）阶段事务 柔性事务 业务改造 无 无 实现相关接口 一致性 不支持 支持 最终一致 隔离性 不支持 支持 业务方保证 并发性能 无影响 严重衰退 略微衰退 适合场景 业务方处理不一致 短事务 & 低并发 长事务 & 高并发 核心概念 XA两阶段事务 两阶段事务提交采用的是 X/OPEN 组织所定义的DTP模型所抽象的 AP（应用程序）, TM（事务管理器）和 RM（资源管理器） 概念来保证分布式事务的强一致性。 其中 TM 与 RM 间采用 XA 的协议进行双向通信。 与传统的本地事务相比，XA 事务增加了准备阶段，数据库除了被动接受提交指令外，还可以反向通知调用方事务是否可以被提交。 TM 可以收集所有分支事务的准备结果，并于最后进行原子提交，以保证事务的强一致性。 Java 通过定义 JTA 接口实现了 XA 模型，JTA 接口中的 ResourceManager 需要数据库厂商提供 XA 驱动实现， TransactionManager 则需要事务管理器的厂商实现，传统的事务管理器需要同应用服务器绑定，因此使用的成本很高。 而嵌入式的事务管器可以以 jar 包的形式提供服务，同 Apache ShardingSphere 集成后，可保证分片后跨库事务强一致性。 通常，只有使用了事务管理器厂商所提供的 XA 事务连接池，才能支持 XA 的事务。Apache ShardingSphere 在整合 XA 事务时，采用分离 XA 事务管理和连接池管理的方式，做到对应用程序的零侵入。 SEATA 柔性事务 Seata是阿里集团和蚂蚁金服联合打造的分布式事务框架。 其 AT 事务的目标是在微服务架构下，提供增量的事务 ACID 语意，让开发者像使用本地事务一样，使用分布式事务，核心理念同 Apache ShardingSphere 一脉相承。 Seata AT 事务模型包含TM (事务管理器)，RM (资源管理器) 和 TC (事务协调器)。 TC 是一个独立部署的服务，TM 和 RM 以 jar 包的方式同业务应用一同部署，它们同 TC 建立长连接，在整个事务生命周期内，保持远程通信。 TM 是全局事务的发起方，负责全局事务的开启，提交和回滚。 RM 是全局事务的参与者，负责分支事务的执行结果上报，并且通过 TC 的协调进行分支事务的提交和回滚。 Seata 管理的分布式事务的典型生命周期： TM 要求 TC 开始一个全新的全局事务。TC 生成一个代表该全局事务的 XID。 XID 贯穿于微服务的整个调用链。 作为该 XID 对应到的 TC 下的全局事务的一部分，RM 注册本地事务。 TM 要求 TC 提交或回滚 XID 对应的全局事务。 TC 驱动 XID 对应的全局事务下的所有分支事务完成提交或回滚。 实现原理 XA两阶段事务 XAShardingTransactionManager 为Apache ShardingSphere 的分布式事务的XA实现类。 它主要负责对多数据源进行管理和适配，并且将相应事务的开启、提交和回滚操作委托给具体的 XA 事务管理器。 开启全局事务 收到接入端的 set autoCommit=0 时，XAShardingTransactionManager 将调用具体的 XA 事务管理器开启 XA 全局事务，以 XID 的形式进行标记。 执行真实分片SQL XAShardingTransactionManager将数据库连接所对应的 XAResource 注册到当前 XA 事务中之后，事务管理器会在此阶段发送 XAResource.start 命令至数据库。 数据库在收到 XAResource.end 命令之前的所有 SQL 操作，会被标记为 XA 事务。 XAResource1.start ## Enlist阶段执行 statement.execute(\"sql1\"); ## 模拟执行一个分片SQL1 statement.execute(\"sql2\"); ## 模拟执行一个分片SQL2 XAResource1.end ## 提交阶段执行 提交或回滚事务 XAShardingTransactionManager 在接收到接入端的提交命令后，会委托实际的 XA 事务管理进行提交动作， 事务管理器将收集到的当前线程中所有注册的 XAResource，并发送 XAResource.end 指令，用以标记此 XA 事务边界。 接着会依次发送 prepare 指令，收集所有参与 XAResource 投票。 若所有 XAResource 的反馈结果均为正确，则调用 commit 指令进行最终提交； 若有任意 XAResource 的反馈结果不正确，则调用 rollback 指令进行回滚。 在事务管理器发出提交指令后，任何 XAResource 产生的异常都会通过恢复日志进行重试，以保证提交阶段的操作原子性，和数据强一致性。 XAResource1.prepare ## ack: yes XAResource2.prepare ## ack: yes XAResource1.commit XAResource2.commit XAResource1.prepare ## ack: yes XAResource2.prepare ## ack: no XAResource1.rollback XAResource2.rollback Seata 柔性事务 整合 Seata AT 事务时，需要将 TM，RM 和 TC 的模型融入 Apache ShardingSphere 的分布式事务生态中。 在数据库资源上，Seata 通过对接 DataSource 接口，让 JDBC 操作可以同 TC 进行远程通信。 同样，Apache ShardingSphere 也是面向 DataSource 接口，对用户配置的数据源进行聚合。 因此，将 DataSource 封装为 基于Seata 的 DataSource 后，就可以将 Seata AT 事务融入到 Apache ShardingSphere的分片生态中。 引擎初始化 包含 Seata 柔性事务的应用启动时，用户配置的数据源会根据 seata.conf 的配置，适配为 Seata 事务所需的 DataSourceProxy，并且注册至 RM 中。 开启全局事务 TM 控制全局事务的边界，TM 通过向 TC 发送 Begin 指令，获取全局事务 ID，所有分支事务通过此全局事务 ID，参与到全局事务中；全局事务 ID 的上下文存放在当前线程变量中。 执行真实分片SQL 处于 Seata 全局事务中的分片 SQL 通过 RM 生成 undo 快照，并且发送 participate 指令至 TC，加入到全局事务中。 由于 Apache ShardingSphere 的分片物理 SQL 采取多线程方式执行，因此整合 Seata AT 事务时，需要在主线程和子线程间进行全局事务 ID 的上下文传递。 提交或回滚事务 提交 Seata 事务时，TM 会向 TC 发送全局事务的提交或回滚指令，TC 根据全局事务 ID 协调所有分支事务进行提交或回滚。 分布式治理 治理 配置中心 配置中心数据结构 配置中心在定义的命名空间下，以 YAML 格式存储，包括数据源信息，规则信息、权限配置和属性配置，可通过修改节点来实现对于配置的动态管理。 namespace ├──authentication # 权限配置 ├──props # 属性配置 ├──schemas # Schema 配置 ├ ├──${schema_1} # Schema 名称1 ├ ├ ├──datasource # 数据源配置 ├ ├ ├──rule # 规则配置 ├ ├ ├──table # 表结构配置 ├ ├──${schema_2} # Schema 名称2 ├ ├ ├──datasource # 数据源配置 ├ ├ ├──rule # 规则配置 ├ ├ ├──table # 表结构配置 /authentication 权限配置，可配置访问 ShardingSphere-Proxy 的用户名和密码 username: root password: root /props 属性配置，详情请参见配置手册。 executor-size: 20 sql-show: true /schemas/${schemeName}/datasource 多个数据库连接池的集合，不同数据库连接池属性自适配（例如：DBCP，C3P0，Druid, HikariCP）。 dataSources: ds_0: dataSourceClassName: com.zaxxer.hikari.HikariDataSource props: url: jdbc:mysql://127.0.0.1:3306/demo_ds_0?serverTimezone=UTC&useSSL=false password: null maxPoolSize: 50 maintenanceIntervalMilliseconds: 30000 connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 minPoolSize: 1 username: root maxLifetimeMilliseconds: 1800000 ds_1: dataSourceClassName: com.zaxxer.hikari.HikariDataSource props: url: jdbc:mysql://127.0.0.1:3306/demo_ds_1?serverTimezone=UTC&useSSL=false password: null maxPoolSize: 50 maintenanceIntervalMilliseconds: 30000 connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 minPoolSize: 1 username: root maxLifetimeMilliseconds: 1800000 /schemas/${schemeName}/rule 规则配置，可包括数据分片、读写分离、数据加密、影子库压测等配置。 rules: - !SHARDING xxx - !REPLICA_QUERY xxx - !ENCRYPT xxx /schemas/${schemeName}/table 表结构配置，暂不支持动态修改。 tables: # 表 t_order: # 表名 columns: # 列 id: # 列名 caseSensitive: false dataType: 0 generated: false name: id primaryKey: trues order_id: caseSensitive: false dataType: 0 generated: false name: order_id primaryKey: false indexs: # 索引 t_user_order_id_index: # 索引名 name: t_user_order_id_index t_order_item: columns: order_id: caseSensitive: false dataType: 0 generated: false name: order_id primaryKey: false 注册中心 注册中心数据结构 注册中心在定义的命名空间的 states 节点下，创建数据库访问对象运行节点，用于区分不同数据库访问实例。包括 proxynodes 和 datanodes 节点。 namespace ├──states ├ ├──proxynodes ├ ├ ├──${your_instance_ip_a}@${your_instance_pid_x}@${UUID} ├ ├ ├──${your_instance_ip_b}@${your_instance_pid_y}@${UUID} ├ ├ ├──.... ├ ├──datanodes ├ ├ ├──${schema_1} ├ ├ ├ ├──${ds_0} ├ ├ ├ ├──${ds_1} ├ ├ ├──${schema_2} ├ ├ ├ ├──${ds_0} ├ ├ ├ ├──${ds_1} ├ ├ ├──.... /states/proxynodes 数据库访问对象运行实例信息，子节点是当前运行实例的标识。 运行实例标识由运行服务器的 IP 地址和 PID 构成。运行实例标识均为临时节点，当实例上线时注册，下线时自动清理。 注册中心监控这些节点的变化来治理运行中实例对数据库的访问等。 /states/datanodes 可以治理读写分离从库，可动态添加删除以及禁用。 1.1.3. 安装使用 ShardingSphere-JDBC 引入maven依赖 # latest.release.version换成指定版本 org.apache.shardingsphere shardingsphere-jdbc-core ${latest.release.version} 配置说明（yaml） # 数据分片 dataSources: # 省略数据源配置 rules: - !SHARDING tables: # 数据分片规则配置 (+): # 逻辑表名称 actualDataNodes (?): # 由数据源名 + 表名组成（参考Inline语法规则） databaseStrategy (?): # 分库策略，缺省表示使用默认分库策略，以下的分片策略只能选其一 standard: # 用于单分片键的标准分片场景 shardingColumn: # 分片列名称 shardingAlgorithmName: # 分片算法名称 complex: # 用于多分片键的复合分片场景 shardingColumns: #分片列名称，多个列以逗号分隔 shardingAlgorithmName: # 分片算法名称 hint: # Hint 分片策略 shardingAlgorithmName: # 分片算法名称 none: # 不分片 tableStrategy: # 分表策略，同分库策略 keyGenerateStrategy: # 分布式序列策略 column: # 自增列名称，缺省表示不使用自增主键生成器 keyGeneratorName: # 分布式序列算法名称 autoTables: # 自动分片表规则配置 t_order_auto: # 逻辑表名称 actualDataSources (?): # 数据源名称 shardingStrategy: # 切分策略 standard: # 用于单分片键的标准分片场景 shardingColumn: # 分片列名称 shardingAlgorithmName: # 自动分片算法名称 bindingTables (+): # 绑定表规则列表 - - broadcastTables (+): # 广播表规则列表 - - defaultDatabaseStrategy: # 默认数据库分片策略 defaultTableStrategy: # 默认表分片策略 defaultKeyGenerateStrategy: # 默认的分布式序列策略 # 分片算法配置 shardingAlgorithms: (+): # 分片算法名称 type: # 分片算法类型 props: # 分片算法属性配置 # ... # 分布式序列算法配置 keyGenerators: (+): # 分布式序列算法名称 type: # 分布式序列算法类型 props: # 分布式序列算法属性配置 # ... props: # ... # 读写分离 dataSources: # 省略数据源配置 rules: - !REPLICA_QUERY dataSources: (+): # 读写分离逻辑数据源名称 primaryDataSourceName: # 主库数据源名称 replicaDataSourceNames: - (+) # 从库数据源名称 loadBalancerName: # 负载均衡算法名称 # 负载均衡算法配置 loadBalancers: (+): # 负载均衡算法名称 type: # 负载均衡算法类型 props: # 负载均衡算法属性配置 # ... props: # ... # 数据加密 dataSource: # 省略数据源配置 rules: - !ENCRYPT tables: (+): # 加密表名称 columns: (+): # 加密列名称 cipherColumn: # 密文列名称 assistedQueryColumn (?): # 查询辅助列名称 plainColumn (?): # 原文列名称 encryptorName: # 加密算法名称 # 加密算法配置 encryptors: (+): # 加解密算法名称 type: # 加解密算法类型 props: # 加解密算法属性配置 # ... props: # ... # 影子库 dataSources: #省略数据源配置 rules: - !SHADOW column: # 影子字段名 sourceDataSourceNames: # 影子前数据库名 # ... shadowDataSourceNames: # 对应的影子库名 # ... props: # ... # 分布式治理 governance: name: # 治理名称 registryCenter: # 配置中心 type: # 治理持久化类型。如：Zookeeper, etcd serverLists: # 治理服务列表。包括 IP 地址和端口号。多个地址用逗号分隔。如: host1:2181,host2:2181 additionalConfigCenter: type: # 治理持久化类型。如：Zookeeper, etcd, Nacos, Apollo serverLists: # 治理服务列表。包括 IP 地址和端口号。多个地址用逗号分隔。如: host1:2181,host2:2181 overwrite: # 本地配置是否覆盖配置中心配置。如果可覆盖，每次启动都以本地配置为准 规则配置 ShardingSphere-JDBC 的 YAML 配置文件 通过数据源集合、规则集合以及属性配置组成。 以下示例是根据 user_id 取模分库, 且根据 order_id 取模分表的 2 库 2 表的配置。 # 配置真实数据源 dataSources: # 配置第 1 个数据源 ds0: !!org.apache.commons.dbcp2.BasicDataSource driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/ds0 username: root password: # 配置第 2 个数据源 ds1: !!org.apache.commons.dbcp2.BasicDataSource driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/ds1 username: root password: rules: # 配置分片规则 - !SHARDING tables: # 配置 t_order 表规则 t_order: actualDataNodes: ds${0..1}.t_order${0..1} # 配置分库策略 databaseStrategy: standard: shardingColumn: user_id shardingAlgorithmName: database_inline # 配置分表策略 tableStrategy: standard: shardingColumn: order_id shardingAlgorithmName: table_inline t_order_item: # 省略配置 t_order_item 表规则... # ... # 配置分片算法 shardingAlgorithms: database_inline: type: INLINE props: algorithm-expression: ds${user_id % 2} table_inline: type: INLINE props: algorithm-expression: t_order_${order_id % 2} 安装 下载地址：https://archive.apache.org/dist/shardingsphere/ # 下载 wget https://archive.apache.org/dist/shardingsphere/4.1.1/apache-shardingsphere-4.1.1-sharding-jdbc-bin.tar.gz -P /usr/local/src 通过 YamlGovernanceShardingSphereDataSourceFactory 工厂创建的 GovernanceShardingSphereDataSource 实现自 JDBC 的标准接口 DataSource。 // 指定 YAML 文件路径 File yamlFile= // ... // 创建 ShardingSphereDataSource DataSource dataSource=YamlShardingSphereDataSourceFactory.createDataSource(yamlFile); 使用 ShardingSphereDataSource 通过 YamlShardingSphereDataSourceFactory 工厂创建的 ShardingSphereDataSource 实现自 JDBC 的标准接口 DataSource。 可通过 DataSource 选择使用原生 JDBC，或JPA， MyBatis 等 ORM 框架。 DataSource dataSource=YamlShardingSphereDataSourceFactory.createDataSource(yamlFile); String sql=\"SELECT i.* FROM t_order o JOIN t_order_item i ON o.order_id=i.order_id WHERE o.user_id=? AND o.order_id=?\"; try( Connection conn=dataSource.getConnection(); PreparedStatement ps=conn.prepareStatement(sql)){ ps.setInt(1,10); ps.setInt(2,1000); try(ResultSet rs=preparedStatement.executeQuery()){ while(rs.next()){ // ... } } } Shardingsphere-Proxy 配置说明 # 数据源配置 schemaName: # 逻辑数据源名称 dataSources: # 数据源配置，可配置多个 : # 与 ShardingSphere-JDBC 配置不同，无需配置数据库连接池 url: #数据库 URL 连接 username: # 数据库用户名 password: # 数据库密码 connectionTimeoutMilliseconds: # 连接超时毫秒数 idleTimeoutMilliseconds: # 空闲连接回收超时毫秒数 maxLifetimeMilliseconds: # 连接最大存活时间毫秒数 maxPoolSize: 50 # 最大连接数 minPoolSize: 1 # 最小连接数 rules: # 与 ShardingSphere-JDBC 配置一致 # ... # 权限配置 authentication: users: root: # 自定义用户名 password: root # 自定义用户名 sharding: # 自定义用户名 password: sharding # 自定义用户名 authorizedSchemas: sharding_db, replica_query_db # 该用户授权可访问的数据库，多个用逗号分隔。缺省将拥有 root 权限，可访问全部数据库。 # 属性配置 配置规则 schemaName: # 逻辑数据源名称 dataSources: # 数据源配置，可配置多个 : # 与 ShardingSphere-JDBC 配置不同，无需配置数据库连接池 url: #数据库 URL 连接 username: # 数据库用户名 password: # 数据库密码 connectionTimeoutMilliseconds: # 连接超时毫秒数 idleTimeoutMilliseconds: # 空闲连接回收超时毫秒数 maxLifetimeMilliseconds: # 连接最大存活时间毫秒数 maxPoolSize: 50 # 最大连接数 minPoolSize: 1 # 最小连接数 rules: # 与 ShardingSphere-JDBC 配置一致 # ... authentication: users: root: # 自定义用户名 password: root # 自定义用户名 sharding: # 自定义用户名 password: sharding # 自定义用户名 authorizedSchemas: sharding_db, replica_query_db # 该用户授权可访问的数据库，多个用逗号分隔。缺省将拥有 root 权限，可访问全部数据库。 安装 二进制安装 # 下载 wget https://archive.apache.org/dist/shardingsphere/4.1.1/apache-shardingsphere-4.1.1-sharding-proxy-bin.tar.gz -P /usr/local/src # 创建配置 ## 分片 schemaName: sharding_db dataSources: ds_0: url: jdbc:mysql://127.0.0.1:3306/demo_ds_0?serverTimezone=UTC&useSSL=false username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 ds_1: url: jdbc:mysql://127.0.0.1:3306/demo_ds_1?serverTimezone=UTC&useSSL=false username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 shardingRule: tables: t_order: actualDataNodes: ds_${0..1}.t_order_${0..1} tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order_${order_id % 2} keyGenerator: type: SNOWFLAKE column: order_id t_order_item: actualDataNodes: ds_${0..1}.t_order_item_${0..1} tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order_item_${order_id % 2} keyGenerator: type: SNOWFLAKE column: order_item_id bindingTables: - t_order,t_order_item defaultDatabaseStrategy: inline: shardingColumn: user_id algorithmExpression: ds_${user_id % 2} defaultTableStrategy: none: ## 主从 schemaName: master_slave_db dataSources: master_ds: url: jdbc:mysql://127.0.0.1:3306/demo_ds_master?serverTimezone=UTC&useSSL=false username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 slave_ds_0: url: jdbc:mysql://127.0.0.1:3306/demo_ds_slave_0?serverTimezone=UTC&useSSL=false username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 slave_ds_1: url: jdbc:mysql://127.0.0.1:3306/demo_ds_slave_1?serverTimezone=UTC&useSSL=false username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 masterSlaveRule: name: ms_ds masterDataSourceName: master_ds slaveDataSourceNames: - slave_ds_0 - slave_ds_1 ## 加密 schemaName: encrypt_db dataSource: url: jdbc:mysql://127.0.0.1:3306/demo_ds?serverTimezone=UTC&useSSL=false username: root password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 50 encryptRule: encryptors: encryptor_aes: type: aes props: aes.key.value: 123456abc encryptor_md5: type: md5 tables: t_encrypt: columns: user_id: plainColumn: user_plain cipherColumn: user_cipher encryptor: encryptor_aes order_id: cipherColumn: order_cipher encryptor: encryptor_md5 # 拷贝mysql驱动包 wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.23/mysql-connector-java-8.0.23.jar -P /${work_dir}/ext-lib # 配置 cat > /${work_dir}/conf/server.yaml /${work_dir}/conf/config-sharding.yaml /usr/lib/systemd/system/shardingproxy.service docker 安装 # 拉取镜像 docker pull apache/shardingsphere-proxy:4.1.1 # 手动构建 git clone https://github.com/apache/shardingsphere mvn clean install cd shardingsphere-distribution/shardingsphere-proxy-distribution mvn clean package -Prelease,docker # 配置 cat > conf/server.yaml conf/config-sharding.yaml conf/logback.xml [%-5level] %d{HH:mm:ss.SSS} [%thread] %logger{36} - %msg%n EOF # 拷贝mysql 驱动包 cp /usr/local/src/mysql-connector-java-8.0.23.jar ext-lib/ # 创建数据库 create database ds_0 default character set utf8mb4; create database ds_1 default character set utf8mb4; # 创建数据表 CREATE TABLE `tbl_0` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT '', `pad` char(60) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ); CREATE TABLE `tbl_1` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT '', `pad` char(60) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ); CREATE TABLE `t_order_0` ( `order_id` int NOT NULL, `user_id` int NOT NULL, `status` varchar(45) DEFAULT NULL, PRIMARY KEY (`order_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; CREATE TABLE `t_order_1` ( `order_id` int NOT NULL, `user_id` int NOT NULL, `status` varchar(45) DEFAULT NULL, PRIMARY KEY (`order_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; CREATE TABLE `t_item_0` ( `item_id` int NOT NULL, `order_id` int NOT NULL, `user_id` int NOT NULL, `status` varchar(45) DEFAULT NULL, PRIMARY KEY (`item_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; CREATE TABLE `t_item_1` ( `item_id` int NOT NULL, `order_id` int NOT NULL, `user_id` int NOT NULL, `status` varchar(45) DEFAULT NULL, PRIMARY KEY (`item_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; # 运行docker cd ${work_dir}/scripts cat > start.sh kubernetes 安装 # 创建dockerfile # 生成helm chart helm create shardingproxy 连接使用 mysql -h10.100.0.1 -P 13308 -usharding -psharding 1.1.4. 性能测试 测试场景 单路由 在1000数据量的基础上分库分表，根据id分为4个库，部署在同一台机器上，根据k分为1024个表，查询操作路由到单库单表； 作为对比，MySQL运行在1000数据量的基础上，使用INSERT+UPDATE+DELETE和单路由查询语句。 主从 基本主从场景，设置一主库一从库，部署在两台不同的机器上，在10000数据量的基础上，观察读写性能； 作为对比，MySQL运行在10000数据量的基础上，使用INSERT+SELECT+DELETE语句。 主从+加密+分库分表 在1000数据量的基础上，根据id分为4个库，部署在四台不同的机器上，根据k分为1024个表，c使用aes加密，pad使用md5加密，查询操作路由到单库单表； 作为对比，MySQL运行在1000数据量的基础上，使用INSERT+UPDATE+DELETE和单路由查询语句。 全路由 在1000数据量的基础上，分库分表，根据id分为4个库，部署在四台不同的机器上，根据k分为1个表，查询操作使用全路由。 作为对比，MySQL运行在1000数据量的基础上，使用INSERT+UPDATE+DELETE和全路由查询语句。 测试环境搭建 数据库表结构 CREATE TABLE `tbl` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT '', `pad` char(60) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ); 测试场景配置 单路由配置 schemaName: sharding_db dataSources: ds_0: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 ds_1: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 ds_2: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 ds_3: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 rules: - !SHARDING tables: tbl: actualDataNodes: ds_${0..3}.tbl${0..1023} tableStrategy: standard: shardingColumn: k shardingAlgorithmName: tbl_table_inline keyGenerateStrategy: column: id keyGeneratorName: snowflake defaultDatabaseStrategy: standard: shardingColumn: id shardingAlgorithmName: default_db_inline defaultTableStrategy: none: shardingAlgorithms: tbl_table_inline: type: INLINE props: algorithm-expression: tbl${k % 1024} default_db_inline: type: INLINE props: algorithm-expression: ds_${id % 4} keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 123 主从配置 schemaName: sharding_db dataSources: primary_ds: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 replica_ds_0: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 rules: - !REPLICA_QUERY dataSources: pr_ds: name: pr_ds primaryDataSourceName: primary_ds replicaDataSourceNames: - replica_ds_0 主从+加密+分库分表配置 schemaName: sharding_db dataSources: primary_ds_0: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 replica_ds_0: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 primary_ds_1: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 replica_ds_1: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 primary_ds_2: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 replica_ds_2: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 primary_ds_3: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 replica_ds_3: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 rules: - !SHARDING tables: tbl: actualDataNodes: pr_ds_${0..3}.tbl${0..1023} databaseStrategy: standard: shardingColumn: id shardingAlgorithmName: tbl_database_inline tableStrategy: standard: shardingColumn: k shardingAlgorithmName: tbl_table_inline keyGenerateStrategy: column: id keyGeneratorName: snowflake bindingTables: - tbl defaultDataSourceName: primary_ds_1 defaultTableStrategy: none: shardingAlgorithms: tbl_database_inline: type: INLINE props: algorithm-expression: pr_ds_${id % 4} tbl_table_inline: type: INLINE props: algorithm-expression: tbl${k % 1024} keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 123 - !REPLICA_QUERY dataSources: pr_ds_0: primaryDataSourceName: primary_ds_0 replicaDataSourceNames: - replica_ds_0 loadBalancerName: round_robin pr_ds_1: primaryDataSourceName: primary_ds_1 replicaDataSourceNames: - replica_ds_1 loadBalancerName: round_robin pr_ds_2: primaryDataSourceName: primary_ds_2 replicaDataSourceNames: - replica_ds_2 loadBalancerName: round_robin pr_ds_3: primaryDataSourceName: primary_ds_3 replicaDataSourceNames: - replica_ds_3 loadBalancerName: round_robin loadBalancers: round_robin: type: ROUND_ROBIN - !ENCRYPT: encryptors: aes_encryptor: type: AES props: aes-key-value: 123456abc md5_encryptor: type: MD5 tables: sbtest: columns: c: plainColumn: c_plain cipherColumn: c_cipher encryptorName: aes_encryptor pad: cipherColumn: pad_cipher encryptorName: md5_encryptor props: query-with-cipher-column: true 全路由 schemaName: sharding_db dataSources: ds_0: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 ds_1: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 ds_2: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 ds_3: url: jdbc:mysql://***.***.***.***:****/ds?serverTimezone=UTC&useSSL=false username: test password: connectionTimeoutMilliseconds: 30000 idleTimeoutMilliseconds: 60000 maxLifetimeMilliseconds: 1800000 maxPoolSize: 200 rules: - !SHARDING tables: tbl: actualDataNodes: ds_${0..3}.tbl1 tableStrategy: standard: shardingColumn: k shardingAlgorithmName: tbl_table_inline keyGenerateStrategy: column: id keyGeneratorName: snowflake defaultDatabaseStrategy: standard: shardingColumn: id shardingAlgorithmName: default_database_inline defaultTableStrategy: none: shardingAlgorithms: default_database_inline: type: INLINE props: algorithm-expression: ds_${id % 4} tbl_table_inline: type: INLINE props: algorithm-expression: tbl1 keyGenerators: snowflake: type: SNOWFLAKE props: worker-id: 123 测试结果验证 压测语句 INSERT+UPDATE+DELETE语句： INSERT INTO tbl(k, c, pad) VALUES(1, '###-###-###', '###-###'); UPDATE tbl SET c='####-####-####', pad='####-####' WHERE id=?; DELETE FROM tbl WHERE id=? 全路由查询语句： SELECT max(id) FROM tbl WHERE id%4=1 单路由查询语句： SELECT id, k FROM tbl ignore index(`PRIMARY`) WHERE id=1 AND k=1 INSERT+SELECT+DELETE语句： INSERT INTO tbl1(k, c, pad) VALUES(1, '###-###-###', '###-###'); SELECT count(id) FROM tbl1; SELECT max(id) FROM tbl1 ignore index(`PRIMARY`); DELETE FROM tbl1 WHERE id=? 压测执行 # 生成jar包 git clone https://github.com/apache/shardingsphere-benchmark.git cd shardingsphere-benchmark/shardingsphere-benchmark mvn clean install # jmeter 压测 cp target/shardingsphere-benchmark-1.0-SNAPSHOT-jar-with-dependencies.jar apache-jmeter-4.0/lib/ext jmeter –n –t test_plan/test.jmx test.jmx参考https://github.com/apache/shardingsphere-benchmark/tree/master/report/script/test_plan/test.jmx 压测结果处理 sh shardingsphere-benchmark/report/script/gen_report.sh 1.1.5. 问题点 数据库未创建报错 Caused by: java.sql.SQLSyntaxErrorException: Unknown database 'ds_0' # 解决办法，创建对应的数据库 create database ds_0 default character set utf8mb4; create database ds_1 default character set utf8mb4; 需要手动创建库表 # 创建数据库 create database ds_0 default character set utf8mb4; create database ds_1 default character set utf8mb4; # 创建数据表 CREATE TABLE `tbl_0` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT '', `pad` char(60) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ); CREATE TABLE `tbl_1` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `k` int(11) NOT NULL DEFAULT 0, `c` char(120) NOT NULL DEFAULT '', `pad` char(60) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ); Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据结构/":{"url":"数据结构/","title":"数据结构","keywords":"","body":"1. 数据结构1. 数据结构 路漫漫其修远兮 数组 链表 栈 队列 散列表 二叉树 堆 跳表 图 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据结构/算法/":{"url":"数据结构/算法/","title":"算法","keywords":"","body":"1. 算法1.1. 算法简练1. 算法 （路漫漫其修远兮） Algorithm 是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法描述解决问题的策略机制 1.1. 算法简练 递归 排序 二分查找 搜索 哈希算法 贪心算法 分治算法 回溯算法 动态规划 字符串匹配算法 限流算法:计数器、漏桶、令牌桶 LRU缓存淘汰算法 GC垃圾回收分代年龄算法 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据结构/算法/限流算法.html":{"url":"数据结构/算法/限流算法.html","title":"限流算法","keywords":"","body":"1. 限流算法1. 限流算法 通过对并发访问请求进行限速，对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率可以拒绝服务、排队或等待、降级处理 计数器 控制单位时间内的请求数量。 劣势：设每分钟请求数量60个，每秒处理1个请求，用户在 00:59 发送 60 个请求，在 01:00 发送 60 个请求 此时 2 秒钟有 120 个请求( 每秒 60 个请求)，远远大于了每秒钟处理数量的阈值。（突刺现象） import java.util.concurrent.atomic.AtomicInteger; public class Counter { /** * 最大访问数量 */ private final int limit = 10; /** * 访问时间差 */ private final long timeout = 1000; /** * 请求时间 */ private long time; /** * 当前计数器 */ private AtomicInteger reqCount = new AtomicInteger(0); public boolean limit() { long now = System.currentTimeMillis(); if (now 滑动窗口 对计数器的一个改进，增加一个时间粒度的度量单位，一分钟分为若干份（如6份，没10秒一份），在每份上设置独立计数器， leaky bucket漏桶 规定固定容量的桶，进入的水无法管控数量、速度，但是对于流出的水我们可以控制速度 劣势：无法应对短时间突发流量（桶满了就丢弃） public class LeakBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * 水流出速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.max(0, (nowSize - (now - time) * rate)); time = now; if ((nowSize + 1) Token Bucket令牌桶 规定固定容量的桶，token 以固定速度往桶内填充， 当桶满时 token 不会被继续放入， 每过来一个请求把 token 从桶中移除， 如果桶中没有 token 不能请求。 可以准备一个队列，用来保存令牌，另外通过一个线程池定期生成令牌放到队列中，每来一个请求，就从队列中获取一个令牌，并继续执行。 public class TokenBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * 水流出速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.min(total, (nowSize + (now - time) * rate)); time = now; if (nowSize Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"数据结构/线性表.html":{"url":"数据结构/线性表.html","title":"线性表","keywords":"","body":"1.1. 线性存储结构（线性表）1.1.1. 顺序存储结构1.1.2. 链式存储结构1.1. 线性存储结构（线性表） 特性：使用线性表存储的数据，如同向数组中存储数据那样，要求数据类型必须一致，也就是说，线性表存储的数据，要么全部都是整形，要么全部都是字符串。一半是整形，另一半是字符串的一组数据无法使用线性表存储 1.1.1. 顺序存储结构 将数据依次存储在连续的整块物理空间中，这种存储结构称为顺序存储结构（简称顺序表） 数组是一种大小固定的数据结构，对线性表的所有操作都可以通过数组来实现。虽然数组一旦创建之后，它的大小就无法改变了，但是当数组不能再存储线性表中的新元素时，我们可以创建一个新的大的数组来替换当前数组。这样就可以使用数组实现动态的数据结构。 1.1.2. 链式存储结构 数据分散的存储在物理空间中，通过一根线保存着它们之间的逻辑关系，这种存储结构称为链式存储结构（简称链表） Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"编程基础/与、或、异或.html":{"url":"编程基础/与、或、异或.html","title":"与、或、异或","keywords":"","body":"1. 基础运算符1. 基础运算符 与(&)：两位全为1，结果为1 或(|)：两位只要有一位为1，结果为1 异或(^)：两位为异，结果为1，否则为0 取反(~)：将一个数位取反，即~1=0，~0=1 左移( 右移(>>)：将一个数右移若干位，右边舍弃，正数左边补0，负数左边补1。每右移一位，相当于除以一次2 例：4 >> 2 = 1，-14 >> 2 = -4 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/":{"url":"解决方案/","title":"解决方案","keywords":"","body":"1.1. 解决方案1.1. 解决方案 各业务场景的解决方案，如延迟队列设计？mysql分库分表？appleId第三方授权登录 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/异常问题排查/Transaction-rollback-only.html":{"url":"解决方案/异常问题排查/Transaction-rollback-only.html","title":"Transaction Rollback Only","keywords":"","body":"1.1. Transaction rolled back1.2. 原因1.3. 解决方法1.1. Transaction rolled back 异常报错：Transaction rolled back because it has been marked as rollback-only @Transactional public class ServiceA { @Autowired private ServiceB serviceB; public void methodA() { try { serviceB.methodB(); } catch (Exception e) { e.printStackTrace(); } } } @Transactional public class serviceB { public void methodB() { throw new RuntimeException(); } } 1.2. 原因 @Transactional(propagation= Propagation.REQUIRED) ：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是@Transactional的默认方式。裸的@Transactional注解就是这种方式。 外层事务（ServiceA）和内层事务（ServiceB）就是一个事务，任何一个出现异常，都会在methodA执行完毕后回滚。如果内层事务B抛出异常e（没有catch，继续向外层抛出），在内层事务结束时，spring会把事务B标记为“rollback-only”；这时外层事务A发现了异常e，如果外层事务A catch了异常并处理掉，那么外层事务A的方法会继续执行代码，直到外层事务也结束时，这时外层事务A想commit，因为正常结束没有向外抛异常，但是内外层事务AB是同一个事务，事务B（同时也是事务A）已经被内层方法标记为“rollback-only”，需要回滚，无法commit，这时spring就会抛出org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only，意思是“事务已经被标记为回滚，无法提交” 1.3. 解决方法 如果希望内层事务回滚，但不影响外层事务提交，需要将内层事务的传播方式指定为@Transactional(propagation= Propagation.NESTED) ，外层事务的提交和回滚能够控制嵌套的内层事务回滚；而内层事务报错时，只回滚内层事务，外层事务可以继续提交。（JPA不支持NESTED，有时可以用REQUIRES_NEW替代一下） 如果这个异常发生时，内层需要事务回滚的代码还没有执行，则可以@Transactional(noRollbackFor = {内层抛出的异常}.class)，指定内层也不为这个异常回滚 回滚整个方法 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); 如果遇此问题，说明是spring事务原理不清晰，建议移步至 Spring事务 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/7千万+用户数据分库分表.html":{"url":"解决方案/7千万+用户数据分库分表.html","title":"7千万+用户数据分库分表","keywords":"","body":"1. 公司需要存储7000万+会员数据1.1. 方案设计阶段1.2. 读写分离造成的性能瓶颈1.2.1. 单表太大1.2.2. 分库分表方式1.2.3. 分库分表产生的问题1.2.4. 分库分表多少合适1. 公司需要存储7000万+会员数据 背景 起初会员数据由数据侧存储，因为业务的发展，需要将会员数据存放至业务侧，所以需要应用技术方案来实现会员数据的增删改查 1.1. 方案设计阶段 采用数据库读写分离，写库操作在主库，读库操作在从库（当有大量的写操作），加一个Master不能解决问题，因为数据要一致性，写操作需要2个甚至多个Master之间同步，相当于重复 考量数据库分库分表方案（sharding），对写操作进行切分 简单的读写分离 mysql的主从复制，结局了数据库的读写分离，提升了读的性能 1.2. 读写分离造成的性能瓶颈 写入无法扩展，需要扩展master 写入无法缓存 锁表率上升 表变大，缓存率下降 分库分表 单库太大 单个数据库处理能力有限；单库所在服务器上磁盘空间不足；单库上操作的IO瓶颈 解决方法：切分成更多更小的库 1.2.1. 单表太大 CRUD都成问题；索引膨胀，查询超时 解决方法：切分成多个数据集更小的表。 1.2.2. 分库分表方式 分库分表的顺序应该是先垂直分，后水平分 垂直拆分 垂直分表 大表拆小表，基于列字段，将不常用的，数据较大，长度较长的字段拆分到扩展表，一般针对几百列的大表。 垂直分库 一个系统中不同业务进行拆分，比如user库，product库，order库，切分放在多个服务器上，而不是同一个服务器。单数据服务器上的性能瓶颈会将用户订单的crud会让当个服务器的磁盘空间，内存，tps等资源不足。高并发场景下，垂直分库一定程度上能够突破IO,连接数及单机硬件资源的瓶颈 水平拆分 水平分表 针对数据量巨大的单张表，按照某种规则（hash，range等），切分到多张表，但是表仍然在同一个库，所以库级别的操作还是IO瓶颈 水平分库分表 将单张表数据切分到不同的服务器上，每个服务器有相应的库表，能够有效缓解单机和单库的性能瓶颈和压力 水平分库分表切分规则 range:从0-10000一个表，10001-20000一个表 Hash取模：将用户，订单作为主表，然后将和它们相关的作为附表，这样不会造成跨库事务之类的问题。取用户id，然后hash取模，分配到不同的数据库上 地理区域：按照华东、华南、华北这样来区分 时间：按照时间切分，一年前数据切分放到另外的表，冷热数据分离 1.2.3. 分库分表产生的问题 维度问题 假如用户购买商品A,需要保存交易记录，如果按照用户维度拆分，则每个用户交易记录都在同一个表，所以可以很快查询某个用户维度的交易记录；但是要查某件商品的被购买情况，则比较麻烦。常见解决方案：1.记录两份数据，一份按用户维度，一份按商品维度。 搜索引擎 联合查询问题 基本不可能，因为关联表可能不在同一个库 避免跨库事务，避免在修改db1的时候同步修改db2的操作，效率会有影响 1.2.4. 分库分表多少合适 单表1000万条记录一下,写入读取性能是比较好的.这样在留点buffer,那么单表全是数据字型的保持在800万条记录以下, 有字符型的单表保持在500万以下。 如果按 100库100表来规划,如用户业务:500万100100 = 50000000万 = 5000亿记录 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/AppleId第三方授权登录流程调研.html":{"url":"解决方案/AppleId第三方授权登录流程调研.html","title":"AppleId第三方授权登录流程调研","keywords":"","body":"1.1. apple id登录主要流程如图(双重认证)附件图1.2. 开发前准备以及注意点1.3. 通信流程1.4. 获取/刷新token1.5. 对id_token解密1.1. apple id登录主要流程如图(双重认证)附件图 1.2. 开发前准备以及注意点 需要苹果开发者账户(https://developer.apple.com/account/#/welcome) 加入开发者计划付费 1.3. 通信流程 客户端与苹果服务器通信获得identitytoken，然后把identitytoken交给业务后台验证，验证通过就可以了。其中appServer涉及到的验证，就是identitytoken，其实identitytoken就是一个jws。（jws是json web signature）,验证jws的签名，保证数据没有被篡改之后，还要校验从identitytokendecode出来的nonce,iss,aud,exp，主要是iss和exp这两个 1.4. 获取/刷新token 首先获取code:GET https://appleid.apple.com/auth/authorize?response_type=code&client_id=&redirect_uri=&state=1234 根据code获取token:POST https://appleid.apple.com/auth/token?grant_type=authorization_code&code=code&redirect_uri=url&client_id=id&client_secret=secret 获取令牌所需参数： grant_type:'authorization_code'为获取令牌 client_id：client_id redirect_uri：redirect_uri code：上一步获取到的授权码 codeclient_secret：secret（一个自己生成的jwt https://developer.apple.com/documentation/signinwithapplerestapi/generate_and_validate_tokens） 返回值示例{ \"access_token\":\"a0996b16cfb674c0eb0d29194c880455b.0.nsww.5fi5MVC-i3AVNhddrNg7Qw\", \"token_type\":\"Bearer\", \"expires_in\":3600, \"refresh_token\":\"r9ee922f1c8b048208037f78cd7dfc91a.0.nsww.KlV2TeFlTr7YDdZ0KtvEQQ\", \"id_token\":\"eyJraWQiOiJBSURPUEsxIiwiYWxnIjoiUlMyNTYifQ.eyJpc3MiOiJodHRwczovL2FwcGxlaWQuYXBwbGUuY29tIiwiYXVkIjoiY29tLnNreW1pbmcuYXBwbGVsb2dpbmRlbW8iLCJleHAiOjE1NjU2NjU1OTQsImlhdCI6MTU2NTY2NDk5NCwic3ViIjoiMDAwMjY2LmRiZTg2NWIwYWE3MjRlMWM4ODM5MDIwOWI5YzdkNjk1LjAyNTYiLCJhdF9oYXNoIjoiR0ZmODhlX1ptc0pqQ2VkZzJXem85ZyIsImF1dGhfdGltZSI6MTU2NTY2NDk2M30.J6XFWmbr0a1hkJszAKM2wevJF57yZt-MoyZNI9QF76dHfJvAmFO9_RP9-tz4pN4ua3BuSJpUbwzT2xFD_rBjsNWkU-ZhuSAONdAnCtK2Vbc2AYEH9n7lB2PnOE1mX5HwY-dI9dqS9AdU4S_CjzTGnvFqC9H5pt6LVoCF4N9dFfQnh2w7jQrjTic_JvbgJT5m7vLzRx-eRnlxQIifEsHDbudzi3yg7XC9OL9QBiTyHdCQvRdsyRLrewJT6QZmi6kEWrV9E21WPC6qJMsaIfGik44UgPOnNnjdxKPzxUAa-Lo1HAzvHcAX5i047T01ltqvHbtsJEZxAB6okmwco78JQA\" } 刷新令牌所需参数： grant_type:'refresh_token'为刷新令牌 client_id：client_id client_secret：client_secret refresh_token：上一步获取到的id_token 1.5. 对id_token解密 通过 GET:https://appleid.apple.com/auth/keys 接口获取公钥{ \"keys\": [ { \"kty\": \"RSA\", \"kid\": \"86D88Kf\", \"use\": \"sig\", \"alg\": \"RS256\", \"n\": \"iGaLqP6y-SJCCBq5Hv6pGDbG_SQ11MNjH7rWHcCFYz4hGwHC4lcSurTlV8u3avoVNM8jXevG1Iu1SY11qInqUvjJur--hghr1b56OPJu6H1iKulSxGjEIyDP6c5BdE1uwprYyr4IO9th8fOwCPygjLFrh44XEGbDIFeImwvBAGOhmMB2AD1n1KviyNsH0bEB7phQtiLk-ILjv1bORSRl8AK677-1T8isGfHKXGZ_ZGtStDe7Lu0Ihp8zoUt59kx2o9uWpROkzF56ypresiIl4WprClRCjz8x6cPZXU2qNWhu71TQvUFwvIvbkE1oYaJMb0jcOTmBRZA2QuYw-zHLwQ\", \"e\": \"AQAB\" }, { \"kty\": \"RSA\", \"kid\": \"eXaunmL\", \"use\": \"sig\", \"alg\": \"RS256\", \"n\": \"4dGQ7bQK8LgILOdLsYzfZjkEAoQeVC_aqyc8GC6RX7dq_KvRAQAWPvkam8VQv4GK5T4ogklEKEvj5ISBamdDNq1n52TpxQwI2EqxSk7I9fKPKhRt4F8-2yETlYvye-2s6NeWJim0KBtOVrk0gWvEDgd6WOqJl_yt5WBISvILNyVg1qAAM8JeX6dRPosahRVDjA52G2X-Tip84wqwyRpUlq2ybzcLh3zyhCitBOebiRWDQfG26EH9lTlJhll-p_Dg8vAXxJLIJ4SNLcqgFeZe4OfHLgdzMvxXZJnPp_VgmkcpUdRotazKZumj6dBPcXI_XID4Z4Z3OM1KrZPJNdUhxw\", \"e\": \"AQAB\" } ] } 然后我们用jwt.verify通过公钥解密id_token 解密后得到的verify.sub就是用户apple账号登录在该程序中的唯一标识，我们可以把它存到程序的数据库中与用户信息做映射，用于标识用户身份 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/Idea远程调试监控端口.html":{"url":"解决方案/Idea远程调试监控端口.html","title":"Idea远程调试监控端口","keywords":"","body":"1. IDEA远程调试1. IDEA远程调试 大家知道，线上环境定位问题不是那么简单的，如果有非常完善的日志以及监控系统是不必担心的，但是应对这些并不完善的场景下，IDEA提供了一种远程调试的功能，remote集成了可以远程调试的功能，只需要在你的生产环境开放某个端口供外部远程访问即可，下面讲解一下教程： 首先是IDEA配置Remote, 如上图，只需要添加要监控的服务器，以及开启监控的端口即可。 IDEA配置好监控之后，需要在服务器上开放对应的端口供外部监控，我的启用方式是在启动jar包的时候指定，如下命令： nohup java -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,address=5005,server=y,suspend=n -jar jardemo.jar --spring.profiles.active=prod & 启动之后启动IDEA配置的remote，看到如下图，即已经开启监控，这时候打断点debug就可以远程调试了 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/Java获取真实ip地址.html":{"url":"解决方案/Java获取真实ip地址.html","title":"Java获取真实ip地址","keywords":"","body":"1. 获取本地ip1. 获取本地ip 在实际项目中，有调用微信支付完成支付功能，在微信支付的请求参数中需要传递一个本机的ip地址，java代码运行环境目前为windows10以及centos7 package com.dq.schooldomain.utils; import java.net.InetAddress; import java.net.NetworkInterface; import java.net.UnknownHostException; import java.util.Enumeration; /** * @Author Allen.Lv * @Description //TODO * @Date 9:50 2019/4/11 * @Desc: Coding Happy! **/ public class IpAddress { public static InetAddress getLocalHostLANAddress() throws UnknownHostException { try { InetAddress candidateAddress = null; // 遍历所有的网络接口 for (Enumeration ifaces = NetworkInterface.getNetworkInterfaces(); ifaces.hasMoreElements(); ) { NetworkInterface iface = (NetworkInterface) ifaces.nextElement(); // 在所有的接口下再遍历IP for (Enumeration inetAddrs = iface.getInetAddresses(); inetAddrs.hasMoreElements(); ) { InetAddress inetAddr = (InetAddress) inetAddrs.nextElement(); if (!inetAddr.isLoopbackAddress()) {// 排除loopback类型地址 if (inetAddr.isSiteLocalAddress()) { // 如果是site-local地址，就是它了 return inetAddr; } else if (candidateAddress == null) { // site-local类型的地址未被发现，先记录候选地址 candidateAddress = inetAddr; } } } } if (candidateAddress != null) { return candidateAddress; } // 如果没有发现 non-loopback地址.只能用最次选的方案 InetAddress jdkSuppliedAddress = InetAddress.getLocalHost(); if (jdkSuppliedAddress == null) { throw new UnknownHostException(\"The JDK InetAddress.getLocalHost() method unexpectedly returned null.\"); } return jdkSuppliedAddress; } catch (Exception e) { UnknownHostException unknownHostException = new UnknownHostException( \"Failed to determine LAN address: \" + e); unknownHostException.initCause(e); throw unknownHostException; } } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/Jenkins脚本实例.html":{"url":"解决方案/Jenkins脚本实例.html","title":"Jenkins脚本实例","keywords":"","body":"1. Jenkins脚本示例1. Jenkins脚本示例 sehll脚本 #!/bin/bash #这里可替换为你自己的执行程序，其他代码无需更改 export JAVA_HOME=/usr/src/java/jdk1.8.0_201 APP_NAME=school-rest-1.0.0-SNAPSHOT.jar FULL_PATH=/usr/data/school-rest/school-rest-1.0.0-SNAPSHOT.jar #启动方法 start(){ pid=`ps -ef|grep $APP_NAME|grep -v grep|awk '{print $2}'` if [ \"$pid\" ]; then echo \"$APP_NAME is already running. pid=$pid .\" else nohup $JAVA_HOME/bin/java -jar $FULL_PATH --spring.profiles.active=dev >> catalina.out 2>&1 & echo $! echo \"$APP_NAME now is running\" fi } #停止方法 stop(){ pid=`ps -ef|grep $APP_NAME|grep -v grep|awk '{print $2}'` if [ \"$pid\" ]; then kill -9 $pid echo \"Pid:$pid stopped\" else echo \"$APP_NAME is not running\" fi } #输出运行状态 status(){ pid=`ps -ef|grep $APP_NAME|grep -v grep|awk '{print $2}'` if [ \"$pid\" ]; then echo \"$APP_NAME is running. Pid is ${pid}\" else echo \"$APP_NAME is NOT running.\" fi } #根据输入参数，选择执行对应方法，不输入则执行使用说明 case \"$1\" in start) start ;; stop) stop ;; status) status ;; restart) stop sleep 5 start ;; *) echo \"Usage:{start|stop|status|restart}\" ;; esac exit 0 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/MybatisPlus多数据源.html":{"url":"解决方案/MybatisPlus多数据源.html","title":"MybatisPlus多数据源","keywords":"","body":"1.1. mybatis-plus配置多数据1.1.1. mybatis-plus官方提供的多数据源配置方式1.1.2. mybatis-plus+druid配置1.1. mybatis-plus配置多数据 mybatis-plus+druid为一种配置方式 mybatis-plus官方提供一种配置方式1.1.1. mybatis-plus官方提供的多数据源配置方式 dynamic-datasource-spring-boot-starter 是一个基于springboot的快速集成多数据源的启动器。其支持 Jdk 1.7+, SpringBoot 1.4.x 1.5.x 2.x.x。使用方法 引入dynamic-datasource-spring-boot-starter com.baomidou dynamic-datasource-spring-boot-starter ${version} 配置数据源 spring: datasource: dynamic: primary: master #设置默认的数据源或者数据源组,默认值即为master strict: false #设置严格模式,默认false不启动. 启动后在未匹配到指定数据源时候会抛出异常,不启动则使用默认数据源. datasource: master: url: jdbc:mysql://xx.xx.xx.xx:3306/dynamic username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver # 3.2.0开始支持SPI可省略此配置 slave_1: url: jdbc:mysql://xx.xx.xx.xx:3307/dynamic username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver slave_2: url: ENC(xxxxx) # 内置加密,使用请查看详细文档 username: ENC(xxxxx) password: ENC(xxxxx) driver-class-name: com.mysql.jdbc.Driver schema: db/schema.sql # 配置则生效,自动初始化表结构 data: db/data.sql # 配置则生效,自动初始化数据 continue-on-error: true # 默认true,初始化失败是否继续 separator: \";\" # sql默认分号分隔符 #......省略 #以上会配置一个默认库master，一个组slave下有两个子库slave_1,slave_2 # 多主多从 纯粹多库（记得设置primary） 混合配置 spring: spring: spring: datasource: datasource: datasource: dynamic: dynamic: dynamic: datasource: datasource: datasource: master_1: mysql: master: master_2: oracle: slave_1: slave_1: sqlserver: slave_2: slave_2: postgresql: oracle_1: slave_3: h2: oracle_2: 使用 @DS 切换数据源 @DS 可以注解在方法上或类上，同时存在就近原则 方法上注解 优先于 类上注解。 没有使用@DS，使用默认数据源；@DS可以直接指定库名。 @Service @DS(\"slave\") public class UserServiceImpl implements UserService { @Autowired private JdbcTemplate jdbcTemplate; public List selectAll() { return jdbcTemplate.queryForList(\"select * from user\"); } @Override @DS(\"slave_1\") public List selectByCondition() { return jdbcTemplate.queryForList(\"select * from user where age >10\"); } } 1.1.2. mybatis-plus+druid配置 使用druid注入多数据源 yml中配置多数据源信息 spring: datasource: druid: datasource1: url: jdbc:mysql://127.0.0.1:3306/db1?serverTimezone=CTT&useUnicode=true&characterEncoding=utf-8&useSSL=true username: root password: 123456 driver-class-name: com.mysql.cj.jdbc.Driver datasource2: url: jdbc:mysql://127.0.0.1:3306/db2?serverTimezone=CTT&useUnicode=true&characterEncoding=utf-8&useSSL=true username: root password: 123456 driver-class-name: com.mysql.cj.jdbc.Driver datasource3: url: jdbc:mysql://127.0.0.1:3306/db3?serverTimezone=CTT&useUnicode=true&characterEncoding=utf-8&useSSL=true username: root password: 123456 driver-class-name: com.mysql.cj.jdbc.Driver 新增MybatisPlusConfig.java文件 @EnableTransactionManagement @Configuration @MapperScan(\"com.dadi01.scrm.service.member.provider.mapper.db*.*\") public class MybatisPlusConfig { /** * 分页 */ @Bean public PaginationInterceptor paginationInterceptor() { return new PaginationInterceptor(); } @Bean(name = \"db1\") @ConfigurationProperties(prefix = \"spring.datasource.druid.datasource1\") public DataSource db1() { return DruidDataSourceBuilder.create().build(); } @Bean(name = \"db2\") @ConfigurationProperties(prefix = \"spring.datasource.druid.datasource2\") public DataSource db2() { return DruidDataSourceBuilder.create().build(); } @Bean(name = \"db3\") @ConfigurationProperties(prefix = \"spring.datasource.druid.datasource3\") public DataSource db3() { return DruidDataSourceBuilder.create().build(); } /** * 动态数据源配置 * * @return */ @Bean @Primary public DataSource multipleDataSource(@Qualifier(\"db1\") DataSource db1, @Qualifier(\"db2\") DataSource db2, @Qualifier(\"db3\") DataSource db3) { DynamicDataSource dynamicDataSource = new DynamicDataSource(); Map targetDataSources = new HashMap<>(); targetDataSources.put(DBTypeEnum.db1.getValue(), db1); targetDataSources.put(DBTypeEnum.db2.getValue(), db2); targetDataSources.put(DBTypeEnum.db3.getValue(), db3); dynamicDataSource.setTargetDataSources(targetDataSources); dynamicDataSource.setDefaultTargetDataSource(db2); return dynamicDataSource; } @Bean(\"sqlSessionFactory\") public SqlSessionFactory sqlSessionFactory() throws Exception { MybatisSqlSessionFactoryBean sqlSessionFactory = new MybatisSqlSessionFactoryBean(); sqlSessionFactory.setDataSource(multipleDataSource(db1(), db2(), db3())); MybatisConfiguration configuration = new MybatisConfiguration(); configuration.setJdbcTypeForNull(JdbcType.NULL); configuration.setMapUnderscoreToCamelCase(true); configuration.setCacheEnabled(false); sqlSessionFactory.setConfiguration(configuration); //PerformanceInterceptor(),OptimisticLockerInterceptor() //添加分页功能 sqlSessionFactory.setPlugins(new Interceptor[]{ paginationInterceptor() }); return sqlSessionFactory.getObject(); } } 新增数据源切换拦截器DataSourceSwitchAspect.java import com.dadi01.scrm.service.member.provider.util.DBTypeEnum; import lombok.extern.slf4j.Slf4j; import org.aspectj.lang.annotation.Aspect; import org.aspectj.lang.annotation.Before; import org.aspectj.lang.annotation.Pointcut; import org.springframework.core.annotation.Order; import org.springframework.stereotype.Component; @Component @Order(value = -100) @Slf4j @Aspect public class DataSourceSwitchAspect { @Pointcut(\"execution(* com.dadi01.scrm.service.member.provider.mapper.db1..*.*(..))\") private void db1Aspect() { } @Pointcut(\"execution(* com.dadi01.scrm.service.member.provider.mapper.db2..*.*(..))\") private void db2Aspect() { } @Pointcut(\"execution(* com.dadi01.scrm.service.member.provider.mapper.db3..*.*(..))\") private void db3Aspect() { } @Before(\"db1Aspect()\") public void db1() { log.info(\"切换到db1 数据源...\"); DbContextHolder.setDbType(DBTypeEnum.db1); } @Before(\"db2Aspect()\") public void db2() { log.info(\"切换到db2 数据源...\"); DbContextHolder.setDbType(DBTypeEnum.db2); } @Before(\"db3Aspect()\") public void db3() { log.info(\"切换到db3 数据源...\"); DbContextHolder.setDbType(DBTypeEnum.db3); } } 设置上下文数据源DbContextHolder.java import com.dadi01.scrm.service.member.provider.util.DBTypeEnum; public class DbContextHolder { private static final ThreadLocal contextHolder = new ThreadLocal<>(); /** * 设置数据源 * @param dbTypeEnum */ public static void setDbType(DBTypeEnum dbTypeEnum) { contextHolder.set(dbTypeEnum.getValue()); } /** * 取得当前数据源 * @return */ public static String getDbType() { return (String) contextHolder.get(); } /** * 清除上下文数据 */ public static void clearDbType() { contextHolder.remove(); } } 实现数据源切换DynamicDataSource.java import org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource; public class DynamicDataSource extends AbstractRoutingDataSource { @Override protected Object determineCurrentLookupKey() { return DbContextHolder.getDbType(); } } 多数据源枚举类DBTypeEnum.java public enum DBTypeEnum { db1(\"db1\"), db2(\"db2\"), db3(\"db3\"); private String value; DBTypeEnum(String value) { this.value = value; } public String getValue() { return value; } } mapper内文件结构为 ``` -- mapper -- db1 -- MemberRepository.java -- db2 -- OrderRepository.java -- db3 -- ShoppingRepository.java ``` 参考：https://baomidou.com/guide/dynamic-datasource.html#%E6%96%87%E6%A1%A3-documentation 参考：https://cloud.tencent.com/developer/article/1181415 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/Mysql分库分表.html":{"url":"解决方案/Mysql分库分表.html","title":"Mysql分库分表","keywords":"","body":"1.1. DDYW需要存储7000万+会员数据1.1.1. 背景1.1.2. 方案设计阶段1.1.3. 简单的读写分离1.1.4. 分库分表1.1. DDYW需要存储7000万+会员数据 1.1.1. 背景 起初会员数据由数据侧存储，因为业务的发展，需要将会员数据存放至业务侧，所以需要应用技术方案来实现会员数据的增删改查 1.1.2. 方案设计阶段 采用数据库读写分离，写库操作在主库，读库操作在从库（当有大量的写操作），加一个Master不能解决问题，因为数据要一致性，写操作需要2个甚至多个Master之间同步，相当于重复 考量数据库分库分表方案（sharding），对写操作进行切分 1.1.3. 简单的读写分离 mysql的主从复制，结局了数据库的读写分离，提升了读的性能 读写分离造成的性能瓶颈 写入无法扩展，需要扩展master 写入无法缓存 锁表率上升 表变大，缓存率下降 1.1.4. 分库分表 单库太大 单个数据库处理能力有限；单库所在服务器上磁盘空间不足；单库上操作的IO瓶颈 解决方法：切分成更多更小的库 单表太大 CRUD都成问题；索引膨胀，查询超时 解决方法：切分成多个数据集更小的表。 分库分表方式 分库分表的顺序应该是先垂直分，后水平分 垂直拆分 垂直分表 大表拆小表，基于列字段，将不常用的，数据较大，长度较长的字段拆分到扩展表，一般针对几百列的大表。 垂直分库 一个系统中不同业务进行拆分，比如user库，product库，order库，切分放在多个服务器上，而不是同一个服务器。单数据服务器上的性能瓶颈会将用户订单的crud会让当个服务器的磁盘空间，内存，tps等资源不足。高并发场景下，垂直分库一定程度上能够突破IO,连接数及单机硬件资源的瓶颈 水平拆分 水平分表 针对数据量巨大的单张表，按照某种规则（hash，range等），切分到多张表，但是表仍然在同一个库，所以库级别的操作还是IO瓶颈 水平分库分表 将单张表数据切分到不同的服务器上，每个服务器有相应的库表，能够有效缓解单机和单库的性能瓶颈和压力 水平分库分表切分规则 range:从0-10000一个表，10001-20000一个表 Hash取模：将用户，订单作为主表，然后将和它们相关的作为附表，这样不会造成跨库事务之类的问题。取用户id，然后hash取模，分配到不同的数据库上 地理区域：按照华东、华南、华北这样来区分 时间：按照时间切分，一年前数据切分放到另外的表，冷热数据分离 分库分表产生的问题 维度问题 假如用户购买商品A,需要保存交易记录，如果按照用户维度拆分，则每个用户交易记录都在同一个表，所以可以很快查询某个用户维度的交易记录；但是要查某件商品的被购买情况，则比较麻烦。常见解决方案：1.记录两份数据，一份按用户维度，一份按商品维度。2.搜索引擎 联合查询问题 基本不可能，因为关联表可能不在同一个库 避免跨库事务，避免在修改db1的时候同步修改db2的操作，效率会有影响 分库分表多少合适 单表1000万条记录一下,写入读取性能是比较好的.这样在留点buffer,那么单表全是数据字型的保持在800万条记录以下, 有字符型的单表保持在500万以下。 如果按 100库100表来规划,如用户业务:500万100100 = 50000000万 = 5000亿记录 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/nacos使用文档说明简介.html":{"url":"解决方案/nacos使用文档说明简介.html","title":"nacos使用文档说明简介","keywords":"","body":"1.1. 什么是Nacos1.1.1. Nacos 概览1.1.2. Nacos 功能1.1.3. Nacos 架构1.1.4. Nacos 部署1.1.5. Nacos 应用1.1. 什么是Nacos 1.1.1. Nacos 概览 Nacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。 Nacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。 Nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。 1.1.2. Nacos 功能 服务（Service）是 Nacos 世界的一等公民。Nacos 支持几乎所有主流类型的“服务”的发现、配置和管理： Kubernetes Service gRPC & Dubbo RPC Service Spring Cloud RESTful Service Nacos 的关键特性包括: 服务发现和服务健康监测 Nacos 支持基于 DNS 和基于 RPC 的服务发现。服务提供者使用 原生SDK、OpenAPI、或一个独立的Agent TODO注册 Service 后，服务消费者可以使用DNS TODO 或HTTP&API查找和发现服务。 Nacos 提供对服务的实时的健康检查，阻止向不健康的主机或服务实例发送请求。Nacos 支持传输层 (PING 或 TCP)和应用层 (如 HTTP、MySQL、用户自定义）的健康检查。 对于复杂的云环境和网络拓扑环境中（如 VPC、边缘网络等）服务的健康检查，Nacos 提供了 agent 上报模式和服务端主动检测2种健康检查模式。Nacos 还提供了统一的健康检查仪表盘，帮助您根据健康状态管理服务的可用性及流量。 动态配置服务 动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。 动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。 配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。 Nacos 提供了一个简洁易用的UI (控制台样例 Demo) 帮助您管理所有的服务和应用的配置。Nacos 还提供包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。 动态DNS服务 动态 DNS 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单DNS解析服务。动态DNS服务还能让您更容易地实现以 DNS 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 API 上的风险。 Nacos 提供了一些简单的 DNS APIs TODO 帮助您管理服务的关联域名和可用的 IP:PORT 列表. 服务及其元数据管理 Nacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 SLA 以及最首要的 metrics 统计数据。 1.1.3. Nacos 架构 服务 (Service) 服务是指一个或一组软件功能（例如特定信息的检索或一组操作的执行），其目的是不同的客户端可以为不同的目的重用（例如通过跨进程的网络调用）。Nacos 支持主流的服务生态，如 Kubernetes Service、gRPC|Dubbo RPC Service 或者 Spring Cloud RESTful Service. 服务注册中心 (Service Registry) 服务注册中心，它是服务，其实例及元数据的数据库。服务实例在启动时注册到服务注册表，并在关闭时注销。服务和路由器的客户端查询服务注册表以查找服务的可用实例。服务注册中心可能会调用服务实例的健康检查 API 来验证它是否能够处理请求。 服务元数据 (Service Metadata) 服务元数据是指包括服务端点(endpoints)、服务标签、服务版本号、服务实例权重、路由规则、安全策略等描述服务的数据 服务提供方 (Service Provider) 是指提供可复用和可调用服务的应用方 服务消费方 (Service Consumer) 是指会发起对某个服务调用的应用方 配置 (Configuration) 在系统开发过程中通常会将一些需要变更的参数、变量等从代码中分离出来独立管理，以独立的配置文件的形式存在。目的是让静态的系统工件或者交付物（如 WAR，JAR 包等）更好地和实际的物理运行环境进行适配。配置管理一般包含在系统部署的过程中，由系统管理员或者运维人员完成这个步骤。配置变更是调整系统运行时的行为的有效手段之一。 配置管理 (Configuration Management) 在数据中心中，系统中所有配置的编辑、存储、分发、变更管理、历史版本管理、变更审计等所有与配置相关的活动统称为配置管理。 名字服务 (Naming Service) 提供分布式系统中所有对象(Object)、实体(Entity)的“名字”到关联的元数据之间的映射管理服务，例如 ServiceName -> Endpoints Info, Distributed Lock Name -> Lock Owner/Status Info, DNS Domain Name -> IP List, 服务发现和 DNS 就是名字服务的2大场景。 配置服务 (Configuration Service) 在服务或者应用运行过程中，提供动态配置或者元数据以及配置管理的服务提供者。 逻辑架构及其组件 服务管理：实现服务CRUD，域名CRUD，服务健康状态检查，服务权重管理等功能 配置管理：实现配置管CRUD，版本管理，灰度管理，监听管理，推送轨迹，聚合数据等功能 元数据管理：提供元数据CURD 和打标能力 插件机制：实现三个模块可分可合能力，实现扩展点SPI机制 事件机制：实现异步化事件通知，sdk数据变化异步通知等逻辑 日志模块：管理日志分类，日志级别，日志可移植性（尤其避免冲突），日志格式，异常码+帮助文档 回调机制：sdk通知数据，通过统一的模式回调用户处理。接口和数据结构需要具备可扩展性 寻址模式：解决ip，域名，nameserver、广播等多种寻址模式，需要可扩展 推送通道：解决server与存储、server间、server与sdk间推送性能问题 容量管理：管理每个租户，分组下的容量，防止存储被写爆，影响服务可用性 流量管理：按照租户，分组等多个维度对请求频率，长链接个数，报文大小，请求流控进行控制 缓存机制：容灾目录，本地缓存，server缓存机制。容灾目录使用需要工具 启动模式：按照单机模式，配置模式，服务模式，dns模式，或者all模式，启动不同的程序+UI 一致性协议：解决不同数据，不同一致性要求情况下，不同一致性机制 存储模块：解决数据持久化、非持久化存储，解决数据分片问题 Nameserver：解决namespace到clusterid的路由问题，解决用户环境与nacos物理环境映射问题 CMDB：解决元数据存储，与三方cmdb系统对接问题，解决应用，人，资源关系 Metrics：暴露标准metrics数据，方便与三方监控系统打通 Trace：暴露标准trace，方便与SLA系统打通，日志白平化，推送轨迹等能力，并且可以和计量计费系统打通 接入管理：相当于阿里云开通服务，分配身份、容量、权限过程 用户管理：解决用户管理，登录，sso等问题 权限管理：解决身份识别，访问控制，角色管理等问题 审计系统：扩展接口方便与不同公司审计系统打通 通知系统：核心数据变更，或者操作，方便通过SMS系统打通，通知到对应人数据变更 OpenAPI：暴露标准Rest风格HTTP接口，简单易用，方便多语言集成 Console：易用控制台，做服务管理、配置管理等操作 SDK：多语言sdk Agent：dns-f类似模式，或者与mesh等方案集成 CLI：命令行对产品进行轻量化管理，像git一样好用 1.1.4. Nacos 部署 集群部署架构图 http://ip1:port/openAPI 直连ip模式，机器挂则需要修改ip才可以使用。 http://VIP:port/openAPI 挂载VIP模式，直连vip即可，下面挂server真实ip，可读性不好。 http://nacos.com:port/openAPI 域名 + VIP模式，可读性好，而且换ip方便，推荐模式 安装 # helm repo 下载 helm pull aliyuncs/nacos # 配置values ## 修改模式为集群模式 global: #mode: quickstart #mode: standalone mode: cluster ## 添加sc提供数据存储 persistence: enabled: false existingClaim: mysql-slave-data #existingClaim: claim: name: mysql-slave-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 5G storageClassName: sc-mysql-slave ## 修改副本数 replicaCount: 3 ## 利用阿里云nas动态卷创建sc apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: nacos meta.helm.sh/release-namespace: middleware labels: app.kubernetes.io/managed-by: Helm name: alibabacloud-nas-nacos mountOptions: - nolock,tcp,noresvport - vers=3 parameters: server: nas_server:/nacos/ volumeAs: subpath provisioner: nasplugin.csi.alibabacloud.com reclaimPolicy: Delete volumeBindingMode: Immediate # 配置mysql数据源 ## mysql 高可用集群，db.num配置为1 ng server.servlet.contextPath=${SERVER_SERVLET_CONTEXTPATH:/nacos} server.contextPath=/nacos server.port=${NACOS_APPLICATION_PORT:8848} spring.datasource.platform=${SPRING_DATASOURCE_PLATFORM:\"\"} nacos.cmdb.dumpTaskInterval=3600 nacos.cmdb.eventTaskInterval=10 nacos.cmdb.labelTaskInterval=300 nacos.cmdb.loadDataAtStart=false db.num=${MYSQL_DATABASE_NUM:1} db.url.0=jdbc:mysql://${MYSQL_SERVICE_HOST}:${MYSQL_SERVICE_PORT:3306}/${MYSQL_SERVICE_DB_NAME}?${MYSQL_SERVICE_DB_PARAM:characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true} db.url.1=jdbc:mysql://${MYSQL_SERVICE_HOST}:${MYSQL_SERVICE_PORT:3306}/${MYSQL_SERVICE_DB_NAME}?${MYSQL_SERVICE_DB_PARAM:characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true} db.user=${MYSQL_SERVICE_USER} db.password=${MYSQL_SERVICE_PASSWORD} # 安装 helm upgrade --install nacos aliyun/nacos -n ddyw-mmeber-test --debug 初始化数据库 /* * Copyright 1999-2018 Alibaba Group Holding Ltd. * * Licensed under the Apache License, Version 2.0 (the \"License\"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = config_info */ /******************************************/ CREATE TABLE `config_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(255) DEFAULT NULL, `content` longtext NOT NULL COMMENT 'content', `md5` varchar(32) DEFAULT NULL COMMENT 'md5', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `src_user` text COMMENT 'source user', `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip', `app_name` varchar(128) DEFAULT NULL, `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', `c_desc` varchar(256) DEFAULT NULL, `c_use` varchar(64) DEFAULT NULL, `effect` varchar(64) DEFAULT NULL, `type` varchar(64) DEFAULT NULL, `c_schema` text, PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfo_datagrouptenant` (`data_id`,`group_id`,`tenant_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = config_info_aggr */ /******************************************/ CREATE TABLE `config_info_aggr` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(255) NOT NULL COMMENT 'group_id', `datum_id` varchar(255) NOT NULL COMMENT 'datum_id', `content` longtext NOT NULL COMMENT '内容', `gmt_modified` datetime NOT NULL COMMENT '修改时间', `app_name` varchar(128) DEFAULT NULL, `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfoaggr_datagrouptenantdatum` (`data_id`,`group_id`,`tenant_id`,`datum_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='增加租户字段'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = config_info_beta */ /******************************************/ CREATE TABLE `config_info_beta` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(128) NOT NULL COMMENT 'group_id', `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name', `content` longtext NOT NULL COMMENT 'content', `beta_ips` varchar(1024) DEFAULT NULL COMMENT 'betaIps', `md5` varchar(32) DEFAULT NULL COMMENT 'md5', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `src_user` text COMMENT 'source user', `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip', `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfobeta_datagrouptenant` (`data_id`,`group_id`,`tenant_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info_beta'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = config_info_tag */ /******************************************/ CREATE TABLE `config_info_tag` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(128) NOT NULL COMMENT 'group_id', `tenant_id` varchar(128) DEFAULT '' COMMENT 'tenant_id', `tag_id` varchar(128) NOT NULL COMMENT 'tag_id', `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name', `content` longtext NOT NULL COMMENT 'content', `md5` varchar(32) DEFAULT NULL COMMENT 'md5', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', `src_user` text COMMENT 'source user', `src_ip` varchar(50) DEFAULT NULL COMMENT 'source ip', PRIMARY KEY (`id`), UNIQUE KEY `uk_configinfotag_datagrouptenanttag` (`data_id`,`group_id`,`tenant_id`,`tag_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_info_tag'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = config_tags_relation */ /******************************************/ CREATE TABLE `config_tags_relation` ( `id` bigint(20) NOT NULL COMMENT 'id', `tag_name` varchar(128) NOT NULL COMMENT 'tag_name', `tag_type` varchar(64) DEFAULT NULL COMMENT 'tag_type', `data_id` varchar(255) NOT NULL COMMENT 'data_id', `group_id` varchar(128) NOT NULL COMMENT 'group_id', `tenant_id` varchar(128) DEFAULT '' COMMENT 'tenant_id', `nid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`nid`), UNIQUE KEY `uk_configtagrelation_configidtag` (`id`,`tag_name`,`tag_type`), KEY `idx_tenant_id` (`tenant_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='config_tag_relation'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = group_capacity */ /******************************************/ CREATE TABLE `group_capacity` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `group_id` varchar(128) NOT NULL DEFAULT '' COMMENT 'Group ID，空字符表示整个集群', `quota` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '配额，0表示使用默认值', `usage` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '使用量', `max_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个配置大小上限，单位为字节，0表示使用默认值', `max_aggr_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '聚合子配置最大个数，，0表示使用默认值', `max_aggr_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值', `max_history_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '最大变更历史数量', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_group_id` (`group_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='集群、各Group容量信息表'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = his_config_info */ /******************************************/ CREATE TABLE `his_config_info` ( `id` bigint(64) unsigned NOT NULL, `nid` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `data_id` varchar(255) NOT NULL, `group_id` varchar(128) NOT NULL, `app_name` varchar(128) DEFAULT NULL COMMENT 'app_name', `content` longtext NOT NULL, `md5` varchar(32) DEFAULT NULL, `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, `src_user` text, `src_ip` varchar(50) DEFAULT NULL, `op_type` char(10) DEFAULT NULL, `tenant_id` varchar(128) DEFAULT '' COMMENT '租户字段', PRIMARY KEY (`nid`), KEY `idx_gmt_create` (`gmt_create`), KEY `idx_gmt_modified` (`gmt_modified`), KEY `idx_did` (`data_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='多租户改造'; /******************************************/ /* 数据库全名 = nacos_config */ /* 表名称 = tenant_capacity */ /******************************************/ CREATE TABLE `tenant_capacity` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `tenant_id` varchar(128) NOT NULL DEFAULT '' COMMENT 'Tenant ID', `quota` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '配额，0表示使用默认值', `usage` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '使用量', `max_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个配置大小上限，单位为字节，0表示使用默认值', `max_aggr_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '聚合子配置最大个数', `max_aggr_size` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值', `max_history_count` int(10) unsigned NOT NULL DEFAULT '0' COMMENT '最大变更历史数量', `gmt_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_tenant_id` (`tenant_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='租户容量信息表'; CREATE TABLE `tenant_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'id', `kp` varchar(128) NOT NULL COMMENT 'kp', `tenant_id` varchar(128) default '' COMMENT 'tenant_id', `tenant_name` varchar(128) default '' COMMENT 'tenant_name', `tenant_desc` varchar(256) DEFAULT NULL COMMENT 'tenant_desc', `create_source` varchar(32) DEFAULT NULL COMMENT 'create_source', `gmt_create` bigint(20) NOT NULL COMMENT '创建时间', `gmt_modified` bigint(20) NOT NULL COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_tenant_info_kptenantid` (`kp`,`tenant_id`), KEY `idx_tenant_id` (`tenant_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin COMMENT='tenant_info'; CREATE TABLE `users` ( `username` varchar(50) NOT NULL PRIMARY KEY, `password` varchar(500) NOT NULL, `enabled` boolean NOT NULL ); CREATE TABLE `roles` ( `username` varchar(50) NOT NULL, `role` varchar(50) NOT NULL, UNIQUE INDEX `idx_user_role` (`username` ASC, `role` ASC) USING BTREE ); CREATE TABLE `permissions` ( `role` varchar(50) NOT NULL, `resource` varchar(255) NOT NULL, `action` varchar(8) NOT NULL, UNIQUE INDEX `uk_role_permission` (`role`,`resource`,`action`) USING BTREE ); INSERT INTO users (username, password, enabled) VALUES ('nacos', '123', TRUE); INSERT INTO roles (username, role) VALUES ('nacos', 'ROLE_ADMIN'); 服务注册&发现配置管理 服务注册 # 注册服务 curl -X PUT 'http://127.0.0.1:8848/nacos/v1/ns/instance？serviceName=nacos.naming.serviceName&ip=20.18.7.10&port=8080' 返回：ok 服务发现 # 查询服务 curl -X GET 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=nacos.naming.serviceName' {\"dom\":\"nacos.naming.serviceName\",\"name\":\"DEFAULT_GROUP@@nacos.naming.serviceName\",\"cacheMillis\":3000,\"lastRefTime\":1611892884808,\"checksum\":\"7f882d81002bc22181203d423a20a16d\",\"useSpecifiedURL\":false,\"clusters\":\"\",\"env\":\"\",\"hosts\":[],\"metadata\":{}} 发布配置 # 发布配置 curl -X POST \"http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=helloWorld\" 返回：true 获取配置 # 获取配置 curl -X GET \"http://127.0.0.1:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test\" 返回：helloWorld 1.1.5. Nacos 应用 接入dubbo 项目结构如下： provider 服务提供者 引入依赖 com.alibaba.nacos nacos-client 1.4.0 org.apache.dubbo dubbo-registry-nacos 2.7.5 # 完整pom文件 org.springframework.boot spring-boot-starter-parent 2.4.2 4.0.0 user-service-provider top.mikel.springboot.users user-service-api 1.0-SNAPSHOT org.springframework.boot spring-boot-starter org.apache.dubbo dubbo 2.7.5 org.apache.dubbo dubbo-spring-boot-starter 2.7.5 --> org.apache.curator--> curator-framework--> 2.13.0--> --> --> org.apache.curator--> curator-recipes--> 2.13.0--> --> com.alibaba.nacos nacos-client 1.4.0 org.apache.dubbo dubbo-registry-nacos 2.7.5 修改配置文件 # dubbo 配置项，对应 DubboConfigurationProperties 配置类 dubbo: # Dubbo 应用配置 application: name: user-service-provider # 应用名 # Dubbo 注册中心配 registry: address: nacos://127.0.0.1:8848 # 注册中心地址。个鞥多注册中心，可见 http://dubbo.apache.org/zh-cn/docs/user/references/registry/introduction.html 文档。 # Dubbo 服务提供者协议配置 protocol: port: -1 # 协议端口。使用 -1 表示随机端口。 name: dubbo # 使用 `dubbo://` 协议。更多协议，可见 http://dubbo.apache.org/zh-cn/docs/user/references/protocol/introduction.html 文档 # Dubbo 服务提供者配置 provider: timeout: 1000 # 【重要】远程服务调用超时时间，单位：毫秒。默认为 1000 毫秒，胖友可以根据自己业务修改 UserRpcService: version: 1.0.0 # 配置扫描 Dubbo 自定义的 @Service 注解，暴露成 Dubbo 服务提供者 scan: base-packages: top.mikel.springboot.users.service.service consumer 消费者 引入依赖 com.alibaba.nacos nacos-client 1.4.0 org.apache.dubbo dubbo-registry-nacos 2.7.5 # 完整pom文件 org.springframework.boot spring-boot-starter-parent 2.4.2 4.0.0 user-service-consumer top.mikel.springboot.users user-service-api 1.0-SNAPSHOT org.springframework.boot spring-boot-starter org.apache.dubbo dubbo 2.7.5 org.apache.dubbo dubbo-spring-boot-starter 2.7.5 --> org.apache.curator--> curator-framework--> 2.13.0--> --> --> org.apache.curator--> curator-recipes--> 2.13.0--> --> com.alibaba.nacos nacos-client 1.4.0 org.apache.dubbo dubbo-registry-nacos 2.7.5 修改配置文件 # dubbo 配置项，对应 DubboConfigurationProperties 配置类 dubbo: # Dubbo 应用配置 application: name: user-service-consumer # 应用名 # Dubbo 注册中心配置 registry: address: nacos://127.0.0.1:8848 # 注册中心地址。个鞥多注册中心，可见 http://dubbo.apache.org/zh-cn/docs/user/references/registry/introduction.html 文档。 # Dubbo 消费者配置 consumer: timeout: 50000 # 【重要】远程服务调用超时时间，单位：毫秒。默认为 1000 毫秒，胖友可以根据自己业务修改 UserService: version: 1.0.0 测试 使用 ProviderApplication 启动服务提供者。在 Nacos 控台中，我们可以看到以 provider 为开头的服务消费者，如下图所示： 服务提供者：provider 服务名称：top.mikel.springboot.users.service.api.UserService (包名) 版本号：1.0.0 # 查询注册的服务 curl -sX GET 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=providers:top.mikel.springboot.users.service.api.UserService:1.0.0:' | jq { \"hosts\": [ { \"ip\": \"10.11.1.70\", \"port\": 20880, \"valid\": true, \"healthy\": true, \"marked\": false, \"instanceId\": \"10.11.1.70#20880#DEFAULT#DEFAULT_GROUP@@providers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"metadata\": { \"side\": \"provider\", \"methods\": \"get\", \"release\": \"2.7.5\", \"deprecated\": \"false\", \"dubbo\": \"2.0.2\", \"pid\": \"22139\", \"interface\": \"top.mikel.springboot.users.service.api.UserService\", \"version\": \"1.0.0\", \"generic\": \"false\", \"timeout\": \"50000\", \"revision\": \"1.0.0\", \"path\": \"top.mikel.springboot.users.service.api.UserService\", \"protocol\": \"dubbo\", \"application\": \"user-service-provider\", \"dynamic\": \"true\", \"category\": \"providers\", \"anyhost\": \"true\", \"timestamp\": \"1611904293326\" }, \"enabled\": true, \"weight\": 1, \"clusterName\": \"DEFAULT\", \"serviceName\": \"providers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"ephemeral\": true } ], \"dom\": \"providers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"name\": \"DEFAULT_GROUP@@providers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"cacheMillis\": 3000, \"lastRefTime\": 1611904380175, \"checksum\": \"f1cecd4e4e713b97a0d5079a036971d3\", \"useSpecifiedURL\": false, \"clusters\": \"\", \"env\": \"\", \"metadata\": {} } 可查看服务提供者详情信息，可编辑和下线操作 使用 ConsumerApplication 启动服务消费者。在 Nacos 控台中，我们可以看到以 consumers 为开头的服务消费者，如下图所示： 服务消费者：consumer 服务名称：top.mikel.springboot.users.service.api.UserService 版本号：1.0.0 curl -sX GET 'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=consumers:top.mikel.springboot.users.service.api.UserService:1.0.0:' | { \"hosts\": [ { \"ip\": \"10.11.1.70\", \"port\": 0, \"valid\": true, \"healthy\": true, \"marked\": false, \"instanceId\": \"10.11.1.70#0#DEFAULT#DEFAULT_GROUP@@consumers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"metadata\": { \"init\": \"false\", \"side\": \"consumer\", \"methods\": \"get\", \"release\": \"2.7.5\", \"logger\": \"slf4j\", \"dubbo\": \"2.0.2\", \"pid\": \"24431\", \"check\": \"false\", \"interface\": \"top.mikel.springboot.users.service.api.UserService\", \"version\": \"1.0.0\", \"qos.enable\": \"false\", \"timeout\": \"50000\", \"revision\": \"1.0.0\", \"path\": \"top.mikel.springboot.users.service.api.UserService\", \"protocol\": \"consumer\", \"application\": \"user-service-consumer\", \"sticky\": \"false\", \"category\": \"consumers\", \"timestamp\": \"1611909492701\" }, \"enabled\": true, \"weight\": 1, \"clusterName\": \"DEFAULT\", \"serviceName\": \"consumers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"ephemeral\": true } ], \"dom\": \"consumers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"name\": \"DEFAULT_GROUP@@consumers:top.mikel.springboot.users.service.api.UserService:1.0.0:\", \"cacheMillis\": 3000, \"lastRefTime\": 1611909945384, \"checksum\": \"014c1cf9537cf92ce5ca74881024c6a4\", \"useSpecifiedURL\": false, \"clusters\": \"\", \"env\": \"\", \"metadata\": {} } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/OAuth2.0授权登录调研.html":{"url":"解决方案/OAuth2.0授权登录调研.html","title":"OAuth2.0授权登录调研","keywords":"","body":"1.1. oauth2授权登录1.1.1. Oauth是什么1.1.2. Oauth定义了四种角色1.1.3. Oauth提供四种获得令牌的流程，向第三方应用颁布1.1.4. 四种模式不同的应用场景1.1.5. 授权码模式1.1. oauth2授权登录 1.1.1. Oauth是什么 OAuth 引入了一个授权层，用来分离两种不同的角色：客户端和资源所有者。......资源所有者同意以后，资源服务器可以向客户端颁发令牌。客户端通过令牌，去请求数据。 1.1.2. Oauth定义了四种角色 client,这里的“客户端\"不包含任何特定的实现特性,比如辰星就是一个客户端. resource owner 资源拥有者，知蓝认证. 此时你就是资源拥有者. authorization server 授权服务器，我们提供的登录授权服务器 resource server 资源服务器, 当你点击同意授权之后，资源服务器会将你的信息发往client.1.1.3. Oauth提供四种获得令牌的流程，向第三方应用颁布 授权码（authorization-code） 隐藏式（implicit） 密码式（resource owner password credentials） 客户端凭证（client credentials）1.1.4. 四种模式不同的应用场景 授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌，最常用的流程，安全性也最高，它适用于那些有后端的Web应用，授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏 隐藏式，有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。RFC 6749 就规定了第二种方式，允许直接向前端颁发令牌。这种方式没有授权码这个中间步骤，所以称为（授权码）\"隐藏式\"（implicit） 密码式，如果你高度信任某个应用，RFC6749也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为\"密码式\"（password） 凭证式，适用于没有前端的命令行应用，即在命令行下请求令牌1.1.5. 授权码模式 请求参数 response_type(必须携带)，在授权码模式中response_type的参数值必须为\"code\".(为了区分和其他三种请求模式的区别, 而且从字面上也方便理解, \"code\" 意思是先拿到授权码在拿token) client_id(必须携带)，客户端标识符 redirect_uri(可选参数)，表示重定向的uri(可以理解为当授权服务器返回code时的返回地址) scope(可选参数)，表示申请权限的范围 state(推荐参数)，state参数通常是一个客户端随机值，发送给授权服务器,授权服务器在原封不动的返回给客户端。这样做是为了预防CSRF攻击。 接口设计 第一步：获取code： eg：oauthServer+\"/oauth/authorize?client_id=\"+clientId+\"&response_type=code&redirect_uri=\"+redirectUrl+\"&scope=all\" 如果没有登录，则会跳转到统一身份认证登录页面。如果用户登录了，调用接口后，会重定向到redirect_uri，授权码会作为它的参数（返回给当前请求授权的客户端的授权码，通常设置为10分钟,并且只能使用一次） 第二步：获取access_token eg：oauthServer+\"/oauth/token?code=\"+code+\"&grant_type=authorization_code&client_secret=\"+clientSecret+\"&redirect_uri=\"+redirectUri+\"&client_id=\"+clientId{ \"access_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1ODk1MzQ5NzMsInVzZXJfbmFtZSI6Im5pY2t5IiwiYXV0aG9yaXRpZXMiOlsiUk9MRV9hZG1pbiJdLCJqdGkiOiJmMjM0M2Q0NC1hODViLTQyOGYtOWE1ZS1iNTE4NTAwNTM5ODgiLCJjbGllbnRfaWQiOiJvYSIsInNjb3BlIjpbImFsbCJdfQ.LWkN2gC2dBrGTn5uSPzfdW6yRj7jhlX87EE8scY02hI\", \"token_type\": \"bearer\", \"expires_in\": 59, \"scope\": \"all\", \"user_name\": \"nicky\", \"jti\": \"f2343d44-a85b-428f-9a5e-b51850053988\" } token_type的值大小写不敏感，通常是bearer类型或mac类型 refresh_token(可选)，用来获取新的授权令牌 第三步：访问系统资源，此时统一认证服务会根据该认证客户端权限信息判断，决定是否返回信息。 eg:http://localhost:8084/api/userinfo?access_token=${accept_token} Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/TiDB性能测试.html":{"url":"解决方案/TiDB性能测试.html","title":"TiDB性能测试","keywords":"","body":"1.1. TIDB 性能测试1.1.1. TIDB整体架构1.1.2. 使用TiUP集群在一台机器上设置测试环境1.1.3. 分布式部署1.1.4. TIDB集群管理1.1.5. 数据迁移1.1.6. Sysbench测试TIDB1.1. TIDB 性能测试 感谢大佬 1.1.1. TIDB整体架构 与传统的单机数据库相比，TiDB 具有以下优势： 纯分布式架构，拥有良好的扩展性，支持弹性的扩缩容 支持 SQL，对外暴露 MySQL 的网络协议，并兼容大多数 MySQL 的语法，在大多数场景下可以直接替换 MySQL 默认支持高可用，在少数副本失效的情况下，数据库本身能够自动进行数据修复和故障转移，对业务透明 支持 ACID 事务，对于一些有强一致需求的场景友好，例如：银行转账 具有丰富的工具链生态，覆盖数据迁移、同步、备份等多种场景 在内核设计上，TiDB 分布式数据库将整体架构拆分成了多个模块，各模块之间互相通信，组成完整的 TiDB 系统。对应的架构图如下： TiDB Server：SQL 层，对外暴露 MySQL 协议的连接 endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 LVS、HAProxy 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。 PD Server：整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。 存储节点 TiKV Server：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。 TiFlash：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。 1.1.2. 使用TiUP集群在一台机器上设置测试环境 场景：体验具有完整拓扑的最小TiDB集群，并在单个Linux服务器上模拟生产部署步骤 准备 官方推荐最小TIDB集群拓扑 实例 副本数 IP地址 端口 TIKV 3 10.101.16.245 20160/20180,20161/20181,20162/20182 TIDB 1 10.101.16.245 4000/10080 PD 1 10.101.16.245 2379/2380 TiFlash 1 10.101.16.245 9000/8123/3930/20170/20292/8234 Monitor prometheusgrafana 10.101.16.245 9090,3000 部署 下载并安装TiUP curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh 安装TiUP集群组件 tiup cluster # 更新 tiup update --self && tiup update cluster 创建并启动集群 创建配置文件 cat > topo.yaml 执行集群部署命令 tiup cluster deploy tidb-ddyw v4.0.8 ./topo.yaml --user root Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster deploy tidb-ddyw v4.0.8 ./topo.yaml --user root Please confirm your topology: Cluster type: tidb Cluster name: tidb-ddyw Cluster version: v4.0.8 Type Host Ports OS/Arch Directories ---- ---- ----- ------- ----------- pd 10.101.16.245 2379/2380 linux/x86_64 /tidb-deploy/pd-2379,/tidb-data/pd-2379 tikv 10.101.16.245 20160/20180 linux/x86_64 /tidb-deploy/tikv-20160,/tidb-data/tikv-20160 tikv 10.101.16.245 20161/20181 linux/x86_64 /tidb-deploy/tikv-20161,/tidb-data/tikv-20161 tikv 10.101.16.245 20162/20182 linux/x86_64 /tidb-deploy/tikv-20162,/tidb-data/tikv-20162 tidb 10.101.16.245 4000/10080 linux/x86_64 /tidb-deploy/tidb-4000 tiflash 10.101.16.245 9000/8123/3930/20170/20292/8234 linux/x86_64 /tidb-deploy/tiflash-9000,/tidb-data/tiflash-9000 prometheus 10.101.16.245 9090 linux/x86_64 /tidb-deploy/prometheus-9090,/tidb-data/prometheus-9090 grafana 10.101.16.245 3000 linux/x86_64 /tidb-deploy/grafana-3000 Attention: 1. If the topology is not what you expected, check your yaml file. 2. Please confirm there is no port/directory conflicts in same host. Do you want to continue? [y/N]: y + Generate SSH keys ... Done + Download TiDB components - Download pd:v4.0.8 (linux/amd64) ... Done - Download tikv:v4.0.8 (linux/amd64) ... Done - Download tidb:v4.0.8 (linux/amd64) ... Done - Download tiflash:v4.0.8 (linux/amd64) ... Done - Download prometheus:v4.0.8 (linux/amd64) ... Done - Download grafana:v4.0.8 (linux/amd64) ... Done - Download node_exporter:v0.17.0 (linux/amd64) ... Done - Download blackbox_exporter:v0.12.0 (linux/amd64) ... Done + Initialize target host environments - Prepare 10.101.16.245:22 ... Done + Copy files - Copy pd -> 10.101.16.245 ... Done - Copy tikv -> 10.101.16.245 ... Done - Copy tikv -> 10.101.16.245 ... Done - Copy tikv -> 10.101.16.245 ... Done - Copy tidb -> 10.101.16.245 ... Done - Copy tiflash -> 10.101.16.245 ... Done - Copy prometheus -> 10.101.16.245 ... Done - Copy grafana -> 10.101.16.245 ... Done - Copy node_exporter -> 10.101.16.245 ... Done - Copy blackbox_exporter -> 10.101.16.245 ... Done + Check status Enabling component pd + Enable cluster Enable pd 10.101.16.245:2379 success + Enable cluster + Enable cluster Enabling component tikv Enabling instance tikv 10.101.16.245:20162 Enabling instance tikv 10.101.16.245:20160 + Enable cluster + Enable cluster + Enable cluster Enable tikv 10.101.16.245:20162 success Enabling component tidb + Enable cluster Enable tidb 10.101.16.245:4000 success Enabling component tiflash + Enable cluster Enable tiflash 10.101.16.245:9000 success Enabling component prometheus + Enable cluster Enable prometheus 10.101.16.245:9090 success Enabling component grafana + Enable cluster + Enable cluster Deployed cluster `tidb-ddyw` successfully, you can start the cluster via `tiup cluster start tidb-ddyw` 启动集群 tiup cluster start tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster start tidb-ddyw Starting cluster tidb-ddyw... + [ Serial ] - SSHKeySet: privateKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa, publicKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa.pub + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [ Serial ] - StartCluster Starting component pd Starting instance pd 10.101.16.245:2379 Start pd 10.101.16.245:2379 success Starting component node_exporter Starting instance 10.101.16.245 Start 10.101.16.245 success Starting component blackbox_exporter Starting instance 10.101.16.245 Start 10.101.16.245 success Starting component tikv Starting instance tikv 10.101.16.245:20162 Starting instance tikv 10.101.16.245:20160 Starting instance tikv 10.101.16.245:20161 Start tikv 10.101.16.245:20160 success Start tikv 10.101.16.245:20161 success Start tikv 10.101.16.245:20162 success Starting component tidb Starting instance tidb 10.101.16.245:4000 Start tidb 10.101.16.245:4000 success Starting component tiflash Starting instance tiflash 10.101.16.245:9000 Start tiflash 10.101.16.245:9000 success Starting component prometheus Starting instance prometheus 10.101.16.245:9090 Start prometheus 10.101.16.245:9090 success Starting component grafana Starting instance grafana 10.101.16.245:3000 Start grafana 10.101.16.245:3000 success + [ Serial ] - UpdateTopology: cluster=tidb-ddyw Started cluster `tidb-ddyw` successfully 访问集群 # 安装mysql client客户端 yum install -y mysql # 访问TIDB,默认为空密码 mysql -h 10.101.16.245 -P 4000 -u root MySQL [(none)]> show databases; +--------------------+ | Database | +--------------------+ | INFORMATION_SCHEMA | | METRICS_SCHEMA | | PERFORMANCE_SCHEMA | | mysql | | test | +--------------------+ 5 rows in set (0.00 sec) # 查看集群列表 tiup cluster list Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster list Name User Version Path PrivateKey ---- ---- ------- ---- ---------- tidb-ddyw tidb v4.0.8 /root/.tiup/storage/cluster/clusters/tidb-ddyw /root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa # 查看集群拓扑 tiup cluster display tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster display tidb-ddyw Cluster type: tidb Cluster name: tidb-ddyw Cluster version: v4.0.8 SSH type: builtin ID Role Host Ports OS/Arch Status Data Dir Deploy Dir -- ---- ---- ----- ------- ------ -------- ---------- 10.101.16.245:3000 grafana 10.101.16.245 3000 linux/x86_64 Up - /tidb-deploy/grafana-3000 10.101.16.245:2379 pd 10.101.16.245 2379/2380 linux/x86_64 Up|L|UI /tidb-data/pd-2379 /tidb-deploy/pd-2379 10.101.16.245:9090 prometheus 10.101.16.245 9090 linux/x86_64 Up /tidb-data/prometheus-9090 /tidb-deploy/prometheus-9090 10.101.16.245:4000 tidb 10.101.16.245 4000/10080 linux/x86_64 Up - /tidb-deploy/tidb-4000 10.101.16.245:9000 tiflash 10.101.16.245 9000/8123/3930/20170/20292/8234 linux/x86_64 Up /tidb-data/tiflash-9000 /tidb-deploy/tiflash-9000 10.101.16.245:20160 tikv 10.101.16.245 20160/20180 linux/x86_64 Up /tidb-data/tikv-20160 /tidb-deploy/tikv-20160 10.101.16.245:20161 tikv 10.101.16.245 20161/20181 linux/x86_64 Up /tidb-data/tikv-20161 /tidb-deploy/tikv-20161 10.101.16.245:20162 tikv 10.101.16.245 20162/20182 linux/x86_64 Up /tidb-data/tikv-20162 /tidb-deploy/tikv-20162 Total nodes: 8 # TIDB访问 To connect TiDB: mysql --host 10.101.16.245 --port 4000 -u root To view the dashboard: http://10.101.16.245:2379/dashboard To view the monitor: http://10.101.16.245:9090 1.1.3. 分布式部署 准备 本次测试环境TIDB集群拓扑 实例 配置 副本数 IP地址 端口 TiDB 8核32G500G 1 10.101.16.246 4000/10080 PD 8核32G500G 1 10.101.16.247 2379/2380 TIKV 8核32G500G 3 10.101.16.24610.101.16.24710.101.16.248 20160/20180,20161/20181,20162/20182 TiFlash 8核32G500G 1 10.101.16.248 9000/8123/3930/20170/20292/8234 Monitor 8核32G500G prometheusgrafana 10.101.16.246 9090,3000 部署 磁盘格式化(三台机器执行) # 数据盘分区 fdisk /dev/vdb 欢迎使用 fdisk (util-linux 2.23.2)。 更改将停留在内存中，直到您决定将更改写入磁盘。 使用写入命令前请三思。 Device does not contain a recognized partition table 使用磁盘标识符 0xa1b68466 创建新的 DOS 磁盘标签。 命令(输入 m 获取帮助)：n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p 分区号 (1-4，默认 1)：1 起始 扇区 (2048-943718399，默认为 2048)： 将使用默认值 2048 Last 扇区, +扇区 or +size{K,M,G} (2048-943718399，默认为 943718399)： 将使用默认值 943718399 分区 1 已设置为 Linux 类型，大小设为 450 GiB 命令(输入 m 获取帮助)：p 磁盘 /dev/vdb：483.2 GB, 483183820800 字节，943718400 个扇区 Units = 扇区 of 1 * 512 = 512 bytes 扇区大小(逻辑/物理)：512 字节 / 512 字节 I/O 大小(最小/最佳)：512 字节 / 512 字节 磁盘标签类型：dos 磁盘标识符：0xa1b68466 设备 Boot Start End Blocks Id System /dev/vdb1 2048 943718399 471858176 83 Linux 命令(输入 m 获取帮助)：w The partition table has been altered! Calling ioctl() to re-read partition table. 正在同步磁盘。 # 创建文件系统 mkfs -t xfs /dev/vdb1 # 挂载使用 mkdir /tidbdata mount /dev/vdb1 /tidbdata # 挂载开机启动 blkid /dev/vda1: UUID=\"9f2d3e15-a78a-4f3d-8385-0165b4b67864\" TYPE=\"ext4\" /dev/vdb1: UUID=\"b2c45077-0f11-42ff-90d1-864a6d9c7490\" TYPE=\"xfs\" cat >> /etc/fstab UUID=b2c45077-0f11-42ff-90d1-864a6d9c7490 /tidbdata xfs default 0 0 > EOF 10.101.16.246 上安装tiup并初始化tidb集群 下载并安装TiUP curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh 安装TiUP集群组件 # source bash_profile source ~/.bash_profile # 安装tiup tiup cluster # 更新 tiup update --self && tiup update cluster 创建并启动集群 创建配置文件 cat > topo.yaml 执行集群部署命令 tiup cluster deploy tidb-ddyw v4.0.8 ./topo.yaml --user root Deployed cluster `tidb-ddyw` successfully, you can start the cluster via `tiup cluster start tidb-ddyw` 启动集群 tiup cluster start tidb-ddyw Started cluster `tidb-ddyw` successfully 访问集群 # 安装mysql client客户端 yum install -y mysql # 访问TIDB,默认为空密码 mysql -h 10.101.16.246 -P 4000 -u root MySQL [(none)]> show databases; +--------------------+ | Database | +--------------------+ | INFORMATION_SCHEMA | | METRICS_SCHEMA | | PERFORMANCE_SCHEMA | | mysql | | test | +--------------------+ 5 rows in set (0.00 sec) # 查看集群列表 tiup cluster display tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster display tidb-ddyw Cluster type: tidb Cluster name: tidb-ddyw Cluster version: v4.0.8 SSH type: builtin ID Role Host Ports OS/Arch Status Data Dir Deploy Dir -- ---- ---- ----- ------- ------ -------- ---------- 10.101.16.246:3000 grafana 10.101.16.246 3000 linux/x86_64 Up - /tidbdata/tidb-deploy/grafana-3000 10.101.16.247:2379 pd 10.101.16.247 2379/2380 linux/x86_64 Up|L|UI /tidbdata/tidb-data/pd-2379 /tidbdata/tidb-deploy/pd-2379 10.101.16.246:9090 prometheus 10.101.16.246 9090 linux/x86_64 Up /tidbdata/tidb-data/prometheus-9090 /tidbdata/tidb-deploy/prometheus-9090 10.101.16.246:4000 tidb 10.101.16.246 4000/10080 linux/x86_64 Up - /tidbdata/tidb-deploy/tidb-4000 10.101.16.248:9000 tiflash 10.101.16.248 9000/8123/3930/20170/20292/8234 linux/x86_64 Up /tidbdata/tidb-data/tiflash-9000 /tidbdata/tidb-deploy/tiflash-9000 10.101.16.246:20160 tikv 10.101.16.246 20160/20180 linux/x86_64 Up /tidbdata/tidb-data/tikv-20160 /tidbdata/tidb-deploy/tikv-20160 10.101.16.247:20161 tikv 10.101.16.247 20161/20181 linux/x86_64 Up /tidbdata/tidb-data/tikv-20161 /tidbdata/tidb-deploy/tikv-20161 10.101.16.248:20162 tikv 10.101.16.248 20162/20182 linux/x86_64 Up /tidbdata/tidb-data/tikv-20162 /tidbdata/tidb-deploy/tikv-20162 Total nodes: 8 # TIDB访问 ## 内网访问 To connect TiDB: mysql --host 10.101.16.246 --port 4000 -u root To view the dashboard: http://10.101.16.246:2379/dashboard To view the monitor: http://10.101.16.246:9090 ## 外网访问 To connect TiDB: mysql --host 47.93.55.89 --port 32002 -u root To view the dashboard: http://47.93.55.89:32003/dashboard To view the monitor prometheus: http://47.93.55.89:32004 To view the monitor grafana: http://47.93.55.89:32001 admin admin 1.1.4. TIDB集群管理 集群重启 在停止node_exporter时,出现超时现象报错,导致集群直接故障,需要手动启动集群,才可回复正常,查看官方提供的tiup部署工具为二进制包,所以需要修改源码才可以解决此问题. # 集群重启 ## 出现超时报错 tiup cluster restart tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster restart tidb-ddyw + [ Serial ] - SSHKeySet: privateKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa, publicKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa.pub + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [ Serial ] - RestartCluster Stopping component grafana Stopping instance 10.101.16.245 Stop grafana 10.101.16.245:3000 success Stopping component prometheus Stopping instance 10.101.16.245 Stop prometheus 10.101.16.245:9090 success Stopping component tiflash Stopping instance 10.101.16.245 Stop tiflash 10.101.16.245:9000 success Stopping component tidb Stopping instance 10.101.16.245 Stop tidb 10.101.16.245:4000 success Stopping component tikv Stopping instance 10.101.16.245 Stopping instance 10.101.16.245 Stopping instance 10.101.16.245 Stop tikv 10.101.16.245:20162 success Stop tikv 10.101.16.245:20160 success Stop tikv 10.101.16.245:20161 success Stopping component pd Stopping instance 10.101.16.245 Stop pd 10.101.16.245:2379 success Stopping component node_exporter retry error: operation timed out after 2m0s pd 10.101.16.245:2379 failed to stop: timed out waiting for port 9100 to be stopped after 2m0s Error: failed to stop: pd 10.101.16.245:2379 failed to stop: timed out waiting for port 9100 to be stopped after 2m0s: timed out waiting for port 9100 to be stopped after 2m0s Verbose debug logs has been written to /tidb-data/tikv-20160/logs/tiup-cluster-debug-2020-12-02-10-50-06.log. Error: run `/root/.tiup/components/cluster/v1.2.5/tiup-cluster` (wd:/root/.tiup/data/SHyfGAX) failed: exit status 1 ## 再次查看集群拓扑,出现异常 Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster display tidb-ddyw Cluster type: tidb Cluster name: tidb-ddyw Cluster version: v4.0.8 SSH type: builtin ID Role Host Ports OS/Arch Status Data Dir Deploy Dir -- ---- ---- ----- ------- ------ -------- ---------- 10.101.16.245:3000 grafana 10.101.16.245 3000 linux/x86_64 inactive - /tidb-deploy/grafana-3000 10.101.16.245:2379 pd 10.101.16.245 2379/2380 linux/x86_64 Down /tidb-data/pd-2379 /tidb-deploy/pd-2379 10.101.16.245:9090 prometheus 10.101.16.245 9090 linux/x86_64 inactive /tidb-data/prometheus-9090 /tidb-deploy/prometheus-9090 10.101.16.245:4000 tidb 10.101.16.245 4000/10080 linux/x86_64 Down - /tidb-deploy/tidb-4000 10.101.16.245:9000 tiflash 10.101.16.245 9000/8123/3930/20170/20292/8234 linux/x86_64 Down /tidb-data/tiflash-9000 /tidb-deploy/tiflash-9000 10.101.16.245:20160 tikv 10.101.16.245 20160/20180 linux/x86_64 Down /tidb-data/tikv-20160 /tidb-deploy/tikv-20160 10.101.16.245:20161 tikv 10.101.16.245 20161/20181 linux/x86_64 Down /tidb-data/tikv-20161 /tidb-deploy/tikv-20161 10.101.16.245:20162 tikv 10.101.16.245 20162/20182 linux/x86_64 Down /tidb-data/tikv-20162 /tidb-deploy/tikv-20162 Total nodes: 8 WARN: get location labels from pd failed: Get http://10.101.16.245:2379/pd/api/v1/config/replicate: dial tcp 10.101.16.245:2379: connect: connection refused ## 再启动TIDB,恢复正常 tiup cluster start tidb-ddyw tiup cluster display tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster display tidb-ddyw Cluster type: tidb Cluster name: tidb-ddyw Cluster version: v4.0.8 SSH type: builtin ID Role Host Ports OS/Arch Status Data Dir Deploy Dir -- ---- ---- ----- ------- ------ -------- ---------- 10.101.16.245:3000 grafana 10.101.16.245 3000 linux/x86_64 Up - /tidb-deploy/grafana-3000 10.101.16.245:2379 pd 10.101.16.245 2379/2380 linux/x86_64 Up|L|UI /tidb-data/pd-2379 /tidb-deploy/pd-2379 10.101.16.245:9090 prometheus 10.101.16.245 9090 linux/x86_64 Up /tidb-data/prometheus-9090 /tidb-deploy/prometheus-9090 10.101.16.245:4000 tidb 10.101.16.245 4000/10080 linux/x86_64 Up - /tidb-deploy/tidb-4000 10.101.16.245:9000 tiflash 10.101.16.245 9000/8123/3930/20170/20292/8234 linux/x86_64 Disconnected /tidb-data/tiflash-9000 /tidb-deploy/tiflash-9000 10.101.16.245:20160 tikv 10.101.16.245 20160/20180 linux/x86_64 Up /tidb-data/tikv-20160 /tidb-deploy/tikv-20160 10.101.16.245:20161 tikv 10.101.16.245 20161/20181 linux/x86_64 Up /tidb-data/tikv-20161 /tidb-deploy/tikv-20161 10.101.16.245:20162 tikv 10.101.16.245 20162/20182 linux/x86_64 Up /tidb-data/tikv-20162 /tidb-deploy/tikv-20162 Total nodes: 8 更新配置 官方提供示例配置参考如下 curl -sL https://raw.githubusercontent.com/pingcap/tidb/master/config/config.toml.example -o config.toml.example 集群更新配置并重启组件 # 查看role和node tiup cluster display tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster display tidb-ddyw Cluster type: tidb Cluster name: tidb-ddyw Cluster version: v4.0.8 SSH type: builtin ID Role Host Ports OS/Arch Status Data Dir Deploy Dir -- ---- ---- ----- ------- ------ -------- ---------- 10.101.16.245:3000 grafana 10.101.16.245 3000 linux/x86_64 Up - /tidb-deploy/grafana-3000 10.101.16.245:2379 pd 10.101.16.245 2379/2380 linux/x86_64 Up|L|UI /tidb-data/pd-2379 /tidb-deploy/pd-2379 10.101.16.245:9090 prometheus 10.101.16.245 9090 linux/x86_64 Up /tidb-data/prometheus-9090 /tidb-deploy/prometheus-9090 10.101.16.245:4000 tidb 10.101.16.245 4000/10080 linux/x86_64 Up - /tidb-deploy/tidb-4000 10.101.16.245:9000 tiflash 10.101.16.245 9000/8123/3930/20170/20292/8234 linux/x86_64 Up /tidb-data/tiflash-9000 /tidb-deploy/tiflash-9000 10.101.16.245:20160 tikv 10.101.16.245 20160/20180 linux/x86_64 Up /tidb-data/tikv-20160 /tidb-deploy/tikv-20160 10.101.16.245:20161 tikv 10.101.16.245 20161/20181 linux/x86_64 Up /tidb-data/tikv-20161 /tidb-deploy/tikv-20161 10.101.16.245:20162 tikv 10.101.16.245 20162/20182 linux/x86_64 Up /tidb-data/tikv-20162 /tidb-deploy/tikv-20162 Total nodes: 8 # 重启整个集群 tiup cluster reload tidb-ddyw Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster reload tidb-ddyw + [ Serial ] - SSHKeySet: privateKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa, publicKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa.pub + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + Refresh instance configs - Refresh config pd -> 10.101.16.245:2379 ... Done - Refresh config tikv -> 10.101.16.245:20160 ... Done - Refresh config tikv -> 10.101.16.245:20161 ... Done - Refresh config tikv -> 10.101.16.245:20162 ... Done - Refresh config tidb -> 10.101.16.245:4000 ... Done - Refresh config tiflash -> 10.101.16.245:9000 ... Done - Refresh config prometheus -> 10.101.16.245:9090 ... Done - Refresh config grafana -> 10.101.16.245:3000 ... Done + Refresh monitor configs - Refresh config node_exporter -> 10.101.16.245 ... Done - Refresh config blackbox_exporter -> 10.101.16.245 ... Done + [ Serial ] - UpgradeCluster Restarting component tiflash Restarting instance 10.101.16.245 Restart 10.101.16.245 success Restarting component pd Restarting instance 10.101.16.245 Restart 10.101.16.245 success Restarting component tikv Still waitting for the PD leader to be elected Evicting 9 leaders from store 10.101.16.245:20160... Still waitting for 9 store leaders to transfer... Still waitting for 9 store leaders to transfer... Still waitting for 9 store leaders to transfer... Still waitting for 9 store leaders to transfer... Still waitting for 9 store leaders to transfer... Still waitting for 9 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Still waitting for 8 store leaders to transfer... Restarting instance 10.101.16.245 Restart 10.101.16.245 success Delete leader evicting scheduler of store 4 success Removed store leader evicting scheduler from 10.101.16.245:20160. Evicting 12 leaders from store 10.101.16.245:20161... Still waitting for 12 store leaders to transfer... Still waitting for 12 store leaders to transfer... Still waitting for 12 store leaders to transfer... Restarting instance 10.101.16.245 Restart 10.101.16.245 success Delete leader evicting scheduler of store 5 success Removed store leader evicting scheduler from 10.101.16.245:20161. Evicting 14 leaders from store 10.101.16.245:20162... Still waitting for 14 store leaders to transfer... Still waitting for 14 store leaders to transfer... Still waitting for 14 store leaders to transfer... Restarting instance 10.101.16.245 Restart 10.101.16.245 success Delete leader evicting scheduler of store 1 success Removed store leader evicting scheduler from 10.101.16.245:20162. Restarting component tidb Restarting instance 10.101.16.245 Restart 10.101.16.245 success Restarting component prometheus Restarting instance 10.101.16.245 Restart 10.101.16.245 success Restarting component grafana Restarting instance 10.101.16.245 Restart 10.101.16.245 success Reloaded cluster `tidb-ddyw` successfully # 根据node重启组件 tiup cluster reload tidb-ddyw -N 10.101.16.245:9090 Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster reload tidb-ddyw -N 10.101.16.245:9090 + [ Serial ] - SSHKeySet: privateKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa, publicKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa.pub + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + Refresh instance configs - Refresh config pd -> 10.101.16.245:2379 ... Done - Refresh config tikv -> 10.101.16.245:20160 ... Done - Refresh config tikv -> 10.101.16.245:20161 ... Done - Refresh config tikv -> 10.101.16.245:20162 ... Done - Refresh config tidb -> 10.101.16.245:4000 ... Done - Refresh config tiflash -> 10.101.16.245:9000 ... Done - Refresh config prometheus -> 10.101.16.245:9090 ... Done - Refresh config grafana -> 10.101.16.245:3000 ... Done + Refresh monitor configs - Refresh config node_exporter -> 10.101.16.245 ... Done - Refresh config blackbox_exporter -> 10.101.16.245 ... Done + [ Serial ] - UpgradeCluster Restarting component prometheus Restarting instance 10.101.16.245 Restart 10.101.16.245 success Reloaded cluster `tidb-ddyw` successfully # 根据role重启组件 tiup cluster reload tidb-ddyw -R prometheus Starting component `cluster`: /root/.tiup/components/cluster/v1.2.5/tiup-cluster reload tidb-ddyw -R prometheus + [ Serial ] - SSHKeySet: privateKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa, publicKey=/root/.tiup/storage/cluster/clusters/tidb-ddyw/ssh/id_rsa.pub + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + [Parallel] - UserSSH: user=tidb, host=10.101.16.245 + Refresh instance configs - Refresh config pd -> 10.101.16.245:2379 ... Done - Refresh config tikv -> 10.101.16.245:20160 ... Done - Refresh config tikv -> 10.101.16.245:20161 ... Done - Refresh config tikv -> 10.101.16.245:20162 ... Done - Refresh config tidb -> 10.101.16.245:4000 ... Done - Refresh config tiflash -> 10.101.16.245:9000 ... Done - Refresh config prometheus -> 10.101.16.245:9090 ... Done - Refresh config grafana -> 10.101.16.245:3000 ... Done + Refresh monitor configs - Refresh config node_exporter -> 10.101.16.245 ... Done - Refresh config blackbox_exporter -> 10.101.16.245 ... Done + [ Serial ] - UpgradeCluster Restarting component prometheus Restarting instance 10.101.16.245 Restart 10.101.16.245 success Reloaded cluster `tidb-ddyw` successfully 查看配置 # 查看配置文件 pwd;tree tidb-* pd-* tikv-* tiflash-*/conf; /tidb-deploy tidb-4000 ├── bin │ └── tidb-server ├── conf │ └── tidb.toml ├── log │ ├── tidb.log │ ├── tidb_slow_query.log │ └── tidb_stderr.log └── scripts └── run_tidb.sh pd-2379 ├── bin │ └── pd-server ├── conf │ └── pd.toml ├── log │ ├── pd.log │ └── pd_stderr.log └── scripts └── run_pd.sh tikv-20160 ├── bin │ └── tikv-server ├── conf │ └── tikv.toml ├── log │ ├── tikv.log │ └── tikv_stderr.log └── scripts └── run_tikv.sh tikv-20161 ├── bin │ └── tikv-server ├── conf │ └── tikv.toml ├── log │ ├── tikv.log │ └── tikv_stderr.log └── scripts └── run_tikv.sh tikv-20162 ├── bin │ └── tikv-server ├── conf │ └── tikv.toml ├── log │ ├── tikv.log │ └── tikv_stderr.log └── scripts └── run_tikv.sh tiflash-9000/conf ├── tiflash-learner.toml ├── tiflash-preprocessed.toml └── tiflash.toml 20 directories, 29 files # 查看配置文件内容 tree tidb-*/conf/*.toml && cat tidb-*/conf/*.toml && echo -e \"\\n+++++++++++++++++++++++++++++++++++++++++++++++\\n\";tree pd-*/conf/*.toml && cat pd-*/conf/*.toml && echo -e \"\\n+++++++++++++++++++++++++++++++++++++++++++++++\\n\";tree tikv-*/conf/*.toml && cat tikv-*/conf/*.toml && echo -e \"\\n+++++++++++++++++++++++++++++++++++++++++++++++\\n\";tree tiflash-*/conf/*.toml && cat tiflash-*/conf/*.toml; tidb-4000/conf/tidb.toml [error opening dir] 0 directories, 0 files # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # tidb: # aa.b1.c3: value # aa.b2.c4: value [log] slow-threshold = 300 +++++++++++++++++++++++++++++++++++++++++++++++ pd-2379/conf/pd.toml [error opening dir] 0 directories, 0 files # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # pd: # aa.b1.c3: value # aa.b2.c4: value [pd-server] metric-storage = \"http://10.101.16.245:9090\" [replication] enable-placement-rules = true location-labels = [\"host\"] +++++++++++++++++++++++++++++++++++++++++++++++ tikv-20160/conf/tikv.toml [error opening dir] tikv-20161/conf/tikv.toml [error opening dir] tikv-20162/conf/tikv.toml [error opening dir] 0 directories, 0 files # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # tikv: # aa.b1.c3: value # aa.b2.c4: value [readpool] [readpool.coprocessor] use-unified-pool = true [readpool.storage] use-unified-pool = false [server] [server.labels] host = \"logic-host-1\" # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # tikv: # aa.b1.c3: value # aa.b2.c4: value [readpool] [readpool.coprocessor] use-unified-pool = true [readpool.storage] use-unified-pool = false [server] [server.labels] host = \"logic-host-2\" # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # tikv: # aa.b1.c3: value # aa.b2.c4: value [readpool] [readpool.coprocessor] use-unified-pool = true [readpool.storage] use-unified-pool = false [server] [server.labels] host = \"logic-host-3\" +++++++++++++++++++++++++++++++++++++++++++++++ tiflash-9000/conf/tiflash-learner.toml [error opening dir] tiflash-9000/conf/tiflash-preprocessed.toml [error opening dir] tiflash-9000/conf/tiflash.toml [error opening dir] 0 directories, 0 files # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # tiflash-learner: # aa.b1.c3: value # aa.b2.c4: value log-file = \"/tidb-deploy/tiflash-9000/log/tiflash_tikv.log\" [raftstore] apply-pool-size = 4 store-pool-size = 4 [rocksdb] wal-dir = \"\" [security] ca-path = \"\" cert-path = \"\" key-path = \"\" [server] addr = \"0.0.0.0:20170\" advertise-addr = \"10.101.16.245:20170\" advertise-status-addr = \"10.101.16.245:20292\" engine-addr = \"10.101.16.245:3930\" status-addr = \"0.0.0.0:20292\" [storage] data-dir = \"/tidb-data/tiflash-9000/flash\" tmp_path = \"/tidb-data/tiflash-9000/tmp\" tcp_port = 9000 display_name = \"TiFlash\" default_profile = \"default\" mark_cache_size = 5368709120 http_port = 8123 listen_host = \"0.0.0.0\" path = \"/tidb-data/tiflash-9000\" [users] [users.readonly] quota = \"default\" password = \"\" profile = \"readonly\" [users.readonly.networks] ip = \"::/0\" [users.default] quota = \"default\" password = \"\" profile = \"default\" [users.default.networks] ip = \"::/0\" [status] metrics_port = 8234 [profiles] [profiles.readonly] readonly = 1 [profiles.default] use_uncompressed_cache = 0 load_balancing = \"random\" max_memory_usage = 10000000000 [quotas] [quotas.default] [quotas.default.interval] result_rows = 0 read_rows = 0 queries = 0 execution_time = 0 duration = 3600 errors = 0 [raft] pd_addr = \"10.101.16.245:2379\" [flash] service_addr = \"10.101.16.245:3930\" tidb_status_addr = \"10.101.16.245:10080\" [flash.proxy] config = \"/tidb-deploy/tiflash-9000/conf/tiflash-learner.toml\" [flash.flash_cluster] update_rule_interval = 5 master_ttl = 60 refresh_interval = 20 cluster_manager_path = \"/tidb-deploy/tiflash-9000/bin/tiflash/flash_cluster_manager\" log = \"/tidb-deploy/tiflash-9000/log/tiflash_cluster_manager.log\" [application] runAsDaemon = true [logger] log = \"/tidb-deploy/tiflash-9000/log/tiflash.log\" level = \"info\" size = \"1000M\" count = 20 errorlog = \"/tidb-deploy/tiflash-9000/log/tiflash_error.log\" # WARNING: This file is auto-generated. Do not edit! All your modification will be overwritten! # You can use 'tiup cluster edit-config' and 'tiup cluster reload' to update the configuration # All configuration items you want to change can be added to: # server_configs: # tiflash: # aa.b1.c3: value # aa.b2.c4: value default_profile = \"default\" display_name = \"TiFlash\" http_port = 8123 listen_host = \"0.0.0.0\" mark_cache_size = 5368709120 path = \"/tidb-data/tiflash-9000\" tcp_port = 9000 tmp_path = \"/tidb-data/tiflash-9000/tmp\" [application] runAsDaemon = true [flash] service_addr = \"10.101.16.245:3930\" tidb_status_addr = \"10.101.16.245:10080\" [flash.flash_cluster] cluster_manager_path = \"/tidb-deploy/tiflash-9000/bin/tiflash/flash_cluster_manager\" log = \"/tidb-deploy/tiflash-9000/log/tiflash_cluster_manager.log\" master_ttl = 60 refresh_interval = 20 update_rule_interval = 5 [flash.proxy] config = \"/tidb-deploy/tiflash-9000/conf/tiflash-learner.toml\" [logger] count = 20 errorlog = \"/tidb-deploy/tiflash-9000/log/tiflash_error.log\" level = \"info\" log = \"/tidb-deploy/tiflash-9000/log/tiflash.log\" size = \"1000M\" [profiles] [profiles.default] load_balancing = \"random\" max_memory_usage = 10000000000 use_uncompressed_cache = 0 [profiles.readonly] readonly = 1 [quotas] [quotas.default] [quotas.default.interval] duration = 3600 errors = 0 execution_time = 0 queries = 0 read_rows = 0 result_rows = 0 [raft] pd_addr = \"10.101.16.245:2379\" [status] metrics_port = 8234 [users] [users.default] password = \"\" profile = \"default\" quota = \"default\" [users.default.networks] ip = \"::/0\" [users.readonly] password = \"\" profile = \"readonly\" quota = \"default\" [users.readonly.networks] ip = \"::/0\" 1.1.5. 数据迁移 数据导入 # 下载示例数据 mkdir -p bikeshare-data && cd bikeshare-data curl -L --remote-name-all https://s3.amazonaws.com/capitalbikeshare-data/{2010..2017}-capitalbikeshare-tripdata.zip unzip \\*-tripdata.zip # 加载数据到TIDB中 ## 创建表 CREATE DATABASE bikeshare; USE bikeshare; CREATE TABLE trips ( trip_id bigint NOT NULL PRIMARY KEY AUTO_INCREMENT, duration integer not null, start_date datetime, end_date datetime, start_station_number integer, start_station varchar(255), end_station_number integer, end_station varchar(255), bike_number varchar(255), member_type varchar(255) ); ## 导入单个文件数据 LOAD DATA LOCAL INFILE '2017Q1-capitalbikeshare-tripdata.csv' INTO TABLE trips FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\\r\\n' IGNORE 1 LINES (duration, start_date, end_date, start_station_number, start_station, end_station_number, end_station, bike_number, member_type); ## 批量导入 for FILE in `ls *.csv`; do echo \"== $FILE ==\" mysql bikeshare --local-infile=1 -e \"LOAD DATA LOCAL INFILE '${FILE}' INTO TABLE trips FIELDS TERMINATED BY ',' ENCLOSED BY '\\\"' LINES TERMINATED BY '\\r\\n' IGNORE 1 LINES (duration, start_date, end_date, start_station_number, start_station, end_station_number, end_station, bike_number, member_type);\" done; 1.1.6. Sysbench测试TIDB IDC机器： 类型 名称 操作系统 中央处理器 内存 磁盘 网卡 集群拓扑 机器IP 部署实例 10.101.16.246 sysbench 10.101.16.246 tidb tikv 10.101.16.247 tikv pd 10.101.16.248 tikv tiflash TIDB配置 [log] level = \"error\" [prepared-plan-cache] enabled = true TIKV配置 默认CF：写入CF = 4：1 log-level = \"error\" [rocksdb.defaultcf] block-cache-size = \"8GB\" [rocksdb.writecf] block-cache-size = \"2GB\" Reload 配置 tiup cluster reload tidb-ddyw 测试 安装Sysbench curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | bash yum -y install sysbench Sysbench配置 # 配置8个线程 cat > config 配置TIDB # 登录数据库,配置参数 set global tidb_disable_txn_auto_retry = off; # 创建数据库 create database sbtest; 导入数据 # 导入数据(32张表,5000万数据) sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 prepare # 查看创建数据表 MySQL [sbtest]> show tables; +------------------+ | Tables_in_sbtest | +------------------+ | sbtest1 | | sbtest2 | | sbtest3 | | sbtest4 | | sbtest5 | | sbtest6 | | sbtest7 | | sbtest8 | +------------------+ 8 rows in set (0.01 sec) # 查看表结构 MySQL [sbtest]> desc sbtest1; +-------+-----------+------+------+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------+-----------+------+------+---------+----------------+ | id | int(11) | NO | PRI | NULL | auto_increment | | k | int(11) | NO | | 0 | | | c | char(120) | NO | | | | | pad | char(60) | NO | | | | +-------+-----------+------+------+---------+----------------+ 4 rows in set (0.00 sec) # 查看数据记录,由于分布式查询时间不固定 MySQL [sbtest]> select count(*) from sbtest1; +----------+ | count(*) | +----------+ | 588081 | +----------+ 1 row in set (2.07 sec) MySQL [sbtest]> select count(*) from sbtest1; +----------+ | count(*) | +----------+ | 588081 | +----------+ 1 row in set (1.22 sec) # 单表查询测试 MySQL [sbtest]> select count(id) from sbtest4; sysbench mysql 的测试类型： #1. bulk_insert.lua 批量写入操作 #2. oltp_delete.lua 写入和删除并行操作 #3. oltp_insert.lua 纯写入操作 #4. oltp_point_select.lua 只读操作，条件为唯一索引列 #5. oltp_read_only.lua 只读操作，包含聚合，去重等操作 大多数情况用于统计的压测 #6. oltp_read_write.lua 读写混合操作，最常用的脚本 用于oltp系统的压测。 #7. oltp_update_index.lua 更新操作，通过主键进行更新 #8. oltp_update_non_index.lua 更新操作，不通过索引列 #9. oltp_write_only.lua 纯写操作，常用脚本，包括insert update delete #10. select_random_points.lua 随机集合只读操作，常用脚本，聚集索引列的selete in操作 #11. select_random_ranges.lua 随机范围只读操作，常用脚本，聚集索引列的selete between操作 SELECT 测试 # selct测试 8线程,测试时间10分钟 sed -i 's/^threads=.*/threads=8/g' config sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 run # 测试结果如下 [ 10s ] thds: 8 tps: 3911.12 qps: 3911.12 (r/w/o: 3911.12/0.00/0.00) lat (ms,95%): 6.09 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 8 tps: 6136.49 qps: 6136.49 (r/w/o: 6136.49/0.00/0.00) lat (ms,95%): 3.62 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 8 tps: 6236.03 qps: 6236.03 (r/w/o: 6236.03/0.00/0.00) lat (ms,95%): 3.68 SQL statistics: queries performed: read: 3908017 write: 0 other: 0 total: 3908017 transactions: 3908017 (6512.09 per sec.) queries: 3908017 (6512.09 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1155s total number of events: 3908017 Latency (ms): min: 0.16 avg: 1.23 max: 630.21 95th percentile: 3.30 sum: 4796344.69 Threads fairness: events (avg/stddev): 488502.1250/1422.83 execution time (avg/stddev): 599.5431/0.05 # selct测试 16线程,测试时间10分钟 sed -i 's/^threads=.*/threads=16/g' config sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 run [ 10s ] thds: 16 tps: 11403.36 qps: 11403.36 (r/w/o: 11403.36/0.00/0.00) lat (ms,95%): 4.49 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 16 tps: 12498.39 qps: 12498.39 (r/w/o: 12498.39/0.00/0.00) lat (ms,95%): 3.68 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 16 tps: 11772.82 qps: 11772.82 (r/w/o: 11772.82/0.00/0.00) lat (ms,95%): 4.25 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 16 tps: 10885.84 qps: 10885.84 (r/w/o: 10885.84/0.00/0.00) lat (ms,95%): 5.00 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 16 tps: 9687.64 qps: 9687.64 (r/w/o: 9687.64/0.00/0.00) lat (ms,95%): 5.57 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 6845695 write: 0 other: 0 total: 6845695 transactions: 6845695 (11408.77 per sec.) queries: 6845695 (11408.77 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0364s total number of events: 6845695 Latency (ms): min: 0.18 avg: 1.40 max: 1114.67 95th percentile: 4.10 sum: 9593800.55 Threads fairness: events (avg/stddev): 427855.9375/1035.31 execution time (avg/stddev): 599.6125/0.01 # selct测试 32线程,测试时间10分钟 sed -i 's/^threads=.*/threads=32/g' config sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 run [ 10s ] thds: 32 tps: 10895.10 qps: 10895.10 (r/w/o: 10895.10/0.00/0.00) lat (ms,95%): 8.28 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 32 tps: 15195.12 qps: 15195.12 (r/w/o: 15195.12/0.00/0.00) lat (ms,95%): 6.32 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 32 tps: 20718.77 qps: 20718.77 (r/w/o: 20718.77/0.00/0.00) lat (ms,95%): 4.49 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 32 tps: 20349.07 qps: 20349.07 (r/w/o: 20349.07/0.00/0.00) lat (ms,95%): 4.18 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 32 tps: 20807.25 qps: 20807.25 (r/w/o: 20807.25/0.00/0.00) lat (ms,95%): 4.57 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 32 tps: 22122.78 qps: 22122.78 (r/w/o: 22122.78/0.00/0.00) lat (ms,95%): 4.49 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 10320806 write: 0 other: 0 total: 10320806 transactions: 10320806 (17198.11 per sec.) queries: 10320806 (17198.11 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1112s total number of events: 10320806 Latency (ms): min: 0.16 avg: 1.86 max: 1632.43 95th percentile: 5.67 sum: 19192033.79 Threads fairness: events (avg/stddev): 322525.1875/1060.65 execution time (avg/stddev): 599.7511/0.00 # selct测试 64线程,测试时间10分钟 sed -i 's/^threads=.*/threads=64/g' config sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 run [ 10s ] thds: 64 tps: 6306.33 qps: 6306.33 (r/w/o: 6306.33/0.00/0.00) lat (ms,95%): 30.26 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 64 tps: 18739.97 qps: 18739.97 (r/w/o: 18739.97/0.00/0.00) lat (ms,95%): 9.06 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 64 tps: 20780.49 qps: 20780.49 (r/w/o: 20780.49/0.00/0.00) lat (ms,95%): 8.28 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 64 tps: 19896.00 qps: 19896.00 (r/w/o: 19896.00/0.00/0.00) lat (ms,95%): 9.91 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 64 tps: 17163.66 qps: 17163.66 (r/w/o: 17163.66/0.00/0.00) lat (ms,95%): 11.04 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 64 tps: 22208.62 qps: 22208.62 (r/w/o: 22208.62/0.00/0.00) lat (ms,95%): 8.43 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 12871444 write: 0 other: 0 total: 12871444 transactions: 12871444 (21451.15 per sec.) queries: 12871444 (21451.15 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0336s total number of events: 12871444 Latency (ms): min: 0.16 avg: 2.98 max: 1493.53 95th percentile: 8.43 sum: 38391016.53 Threads fairness: events (avg/stddev): 201116.3125/503.93 execution time (avg/stddev): 599.8596/0.00 # selct测试 128线程,测试时间10分钟 sed -i 's/^threads=.*/threads=128/g' config sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 run [ 10s ] thds: 128 tps: 23254.21 qps: 23254.21 (r/w/o: 23254.21/0.00/0.00) lat (ms,95%): 16.12 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 128 tps: 24602.03 qps: 24602.03 (r/w/o: 24602.03/0.00/0.00) lat (ms,95%): 13.22 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 128 tps: 27695.07 qps: 27695.07 (r/w/o: 27695.07/0.00/0.00) lat (ms,95%): 13.46 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 128 tps: 26395.86 qps: 26395.86 (r/w/o: 26395.86/0.00/0.00) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 128 tps: 28633.02 qps: 28633.02 (r/w/o: 28633.02/0.00/0.00) lat (ms,95%): 12.75 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 128 tps: 28279.46 qps: 28279.46 (r/w/o: 28279.46/0.00/0.00) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 15867653 write: 0 other: 0 total: 15867653 transactions: 15867653 (26426.46 per sec.) queries: 15867653 (26426.46 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.4442s total number of events: 15867653 Latency (ms): min: 0.16 avg: 4.84 max: 1295.69 95th percentile: 13.46 sum: 76797738.06 Threads fairness: events (avg/stddev): 123966.0391/421.67 execution time (avg/stddev): 599.9823/0.08 # selct测试 256线程,测试时间10分钟 sed -i 's/^threads=.*/threads=256/g' config sysbench --config-file=config oltp_point_select --tables=24 --table-size=50000000 run [ 10s ] thds: 256 tps: 3956.05 qps: 3956.05 (r/w/o: 3956.05/0.00/0.00) lat (ms,95%): 244.38 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 256 tps: 10995.73 qps: 10995.73 (r/w/o: 10995.73/0.00/0.00) lat (ms,95%): 75.82 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 256 tps: 15475.90 qps: 15475.90 (r/w/o: 15475.90/0.00/0.00) lat (ms,95%): 55.82 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 256 tps: 14701.82 qps: 14701.82 (r/w/o: 14701.82/0.00/0.00) lat (ms,95%): 59.99 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 256 tps: 12388.89 qps: 12388.89 (r/w/o: 12388.89/0.00/0.00) lat (ms,95%): 89.16 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 256 tps: 25637.06 qps: 25637.06 (r/w/o: 25637.06/0.00/0.00) lat (ms,95%): 25.28 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 15188519 write: 0 other: 0 total: 15188519 transactions: 15188519 (25311.42 per sec.) queries: 15188519 (25311.42 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0643s total number of events: 15188519 Latency (ms): min: 0.16 avg: 10.11 max: 2198.00 95th percentile: 27.17 sum: 153591443.49 Threads fairness: events (avg/stddev): 59330.1523/254.59 execution time (avg/stddev): 599.9666/0.02 # 只读测试 sysbench --config-file=config oltp_read_only --tables=24 --table-size=50000000 run UPDATE 主键更新测试 # update index 8线程　10分钟 sed -i 's/^threads=.*/threads=8/g' config sysbench --config-file=config oltp_update_index --tables=24 --table-size=50000000 run [ 10s ] thds: 8 tps: 105.97 qps: 105.97 (r/w/o: 0.00/104.87/1.10) lat (ms,95%): 257.95 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 8 tps: 185.20 qps: 185.20 (r/w/o: 0.00/183.00/2.20) lat (ms,95%): 80.03 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 8 tps: 171.99 qps: 171.99 (r/w/o: 0.00/170.69/1.30) lat (ms,95%): 92.42 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 8 tps: 219.92 qps: 219.92 (r/w/o: 0.00/217.82/2.10) lat (ms,95%): 80.03 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 8 tps: 293.50 qps: 293.50 (r/w/o: 0.00/289.90/3.60) lat (ms,95%): 42.61 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 8 tps: 358.60 qps: 358.60 (r/w/o: 0.00/355.40/3.20) lat (ms,95%): 36.24 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 150714 other: 1363 total: 152077 transactions: 152077 (253.45 per sec.) queries: 152077 (253.45 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0337s total number of events: 152077 Latency (ms): min: 0.45 avg: 31.56 max: 2484.02 95th percentile: 62.19 sum: 4799864.38 Threads fairness: events (avg/stddev): 19009.6250/71.48 execution time (avg/stddev): 599.9830/0.01 # update index 16线程　10分钟 sed -i 's/^threads=.*/threads=16/g' config sysbench --config-file=config oltp_update_index --tables=24 --table-size=50000000 run [ 10s ] thds: 16 tps: 286.55 qps: 286.55 (r/w/o: 0.00/284.45/2.10) lat (ms,95%): 144.97 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 16 tps: 374.59 qps: 374.59 (r/w/o: 0.00/371.19/3.40) lat (ms,95%): 78.60 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 16 tps: 325.80 qps: 325.80 (r/w/o: 0.00/321.50/4.30) lat (ms,95%): 101.13 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 16 tps: 223.41 qps: 223.41 (r/w/o: 0.00/221.51/1.90) lat (ms,95%): 287.38 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 16 tps: 372.00 qps: 372.00 (r/w/o: 0.00/368.10/3.90) lat (ms,95%): 65.65 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 16 tps: 435.20 qps: 435.20 (r/w/o: 0.00/432.20/3.00) lat (ms,95%): 65.65 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 273213 other: 2444 total: 275657 transactions: 275657 (459.40 per sec.) queries: 275657 (459.40 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0415s total number of events: 275657 Latency (ms): min: 0.42 avg: 34.83 max: 2363.14 95th percentile: 75.82 sum: 9599848.16 Threads fairness: events (avg/stddev): 17228.5625/61.31 execution time (avg/stddev): 599.9905/0.01 # update index 16线程　10分钟 sed -i 's/^threads=.*/threads=32/g' config sysbench --config-file=config oltp_update_index --tables=24 --table-size=50000000 run [ 10s ] thds: 32 tps: 640.93 qps: 640.93 (r/w/o: 0.00/635.83/5.10) lat (ms,95%): 80.03 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 32 tps: 637.31 qps: 637.31 (r/w/o: 0.00/631.81/5.50) lat (ms,95%): 94.10 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 32 tps: 825.59 qps: 825.59 (r/w/o: 0.00/818.39/7.20) lat (ms,95%): 66.84 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 32 tps: 677.69 qps: 677.69 (r/w/o: 0.00/671.79/5.90) lat (ms,95%): 95.81 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 32 tps: 986.00 qps: 986.00 (r/w/o: 0.00/976.30/9.70) lat (ms,95%): 64.47 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 32 tps: 1095.82 qps: 1095.82 (r/w/o: 0.00/1085.52/10.30) lat (ms,95%): 52.89 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 674705 other: 6017 total: 680722 transactions: 680722 (1134.22 per sec.) queries: 680722 (1134.22 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1651s total number of events: 680722 Latency (ms): min: 0.38 avg: 28.21 max: 1179.24 95th percentile: 53.85 sum: 19201507.50 Threads fairness: events (avg/stddev): 21272.5625/57.52 execution time (avg/stddev): 600.0471/0.04 # update index 64线程　10分钟 sed -i 's/^threads=.*/threads=64/g' config sysbench --config-file=config oltp_update_index --tables=24 --table-size=50000000 run [ 10s ] thds: 64 tps: 2180.69 qps: 2180.69 (r/w/o: 0.00/2158.20/22.49) lat (ms,95%): 47.47 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 64 tps: 1979.82 qps: 1979.82 (r/w/o: 0.00/1960.52/19.30) lat (ms,95%): 52.89 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 64 tps: 1898.09 qps: 1898.09 (r/w/o: 0.00/1882.09/16.00) lat (ms,95%): 65.65 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 64 tps: 1968.12 qps: 1968.12 (r/w/o: 0.00/1951.42/16.70) lat (ms,95%): 50.11 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 64 tps: 2244.30 qps: 2244.30 (r/w/o: 0.00/2224.10/20.20) lat (ms,95%): 44.17 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 64 tps: 1448.81 qps: 1448.81 (r/w/o: 0.00/1436.21/12.60) lat (ms,95%): 118.92 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1145001 other: 10176 total: 1155177 transactions: 1155177 (1925.14 per sec.) queries: 1155177 (1925.14 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0474s total number of events: 1155177 Latency (ms): min: 0.45 avg: 33.24 max: 2247.91 95th percentile: 56.84 sum: 38399975.13 Threads fairness: events (avg/stddev): 18049.6406/49.49 execution time (avg/stddev): 599.9996/0.01 # update index 128线程　10分钟 sed -i 's/^threads=.*/threads=128/g' config sysbench --config-file=config oltp_update_index --tables=24 --table-size=50000000 run [ 10s ] thds: 128 tps: 2940.58 qps: 2940.58 (r/w/o: 0.00/2915.20/25.38) lat (ms,95%): 87.56 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 128 tps: 2761.92 qps: 2761.92 (r/w/o: 0.00/2737.02/24.90) lat (ms,95%): 80.03 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 128 tps: 2847.77 qps: 2847.77 (r/w/o: 0.00/2823.67/24.10) lat (ms,95%): 84.47 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 128 tps: 2562.89 qps: 2562.89 (r/w/o: 0.00/2539.79/23.10) lat (ms,95%): 74.46 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 128 tps: 2779.34 qps: 2779.34 (r/w/o: 0.00/2754.14/25.20) lat (ms,95%): 101.13 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 128 tps: 3990.12 qps: 3990.12 (r/w/o: 0.00/3952.02/38.10) lat (ms,95%): 59.99 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 2381625 other: 21414 total: 2403039 transactions: 2403039 (4004.38 per sec.) queries: 2403039 (4004.38 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1009s total number of events: 2403039 Latency (ms): min: 0.44 avg: 31.96 max: 2111.37 95th percentile: 58.92 sum: 76804828.46 Threads fairness: events (avg/stddev): 18773.7422/47.75 execution time (avg/stddev): 600.0377/0.02 # update index 256线程　10分钟 sed -i 's/^threads=.*/threads=256/g' config sysbench --config-file=config oltp_update_index --tables=24 --table-size=50000000 run [ 10s ] thds: 256 tps: 3950.63 qps: 3950.63 (r/w/o: 0.00/3916.67/33.95) lat (ms,95%): 142.39 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 256 tps: 3807.87 qps: 3807.87 (r/w/o: 0.00/3775.37/32.50) lat (ms,95%): 134.90 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 256 tps: 4390.40 qps: 4390.40 (r/w/o: 0.00/4348.90/41.50) lat (ms,95%): 112.67 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 256 tps: 4877.34 qps: 4877.34 (r/w/o: 0.00/4833.34/44.00) lat (ms,95%): 106.75 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 256 tps: 5416.16 qps: 5416.16 (r/w/o: 0.00/5371.16/45.00) lat (ms,95%): 94.10 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 256 tps: 5808.93 qps: 5808.93 (r/w/o: 0.00/5759.15/49.77) lat (ms,95%): 81.48 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 3129355 other: 27823 total: 3157178 transactions: 3157178 (5258.48 per sec.) queries: 3157178 (5258.48 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.3958s total number of events: 3157178 Latency (ms): min: 0.44 avg: 48.67 max: 2368.23 95th percentile: 97.55 sum: 153649411.42 Threads fairness: events (avg/stddev): 12332.7266/50.73 execution time (avg/stddev): 600.1930/0.11 读写混合测试 # select insert 8线程　10分钟 sed -i 's/^threads=.*/threads=8/g' config sysbench --config-file=config oltp_read_write --tables=24 --table-size=50000000 run [ 10s ] thds: 8 tps: 172.47 qps: 3460.48 (r/w/o: 2423.67/685.78/351.04) lat (ms,95%): 65.65 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 8 tps: 207.91 qps: 4156.23 (r/w/o: 2908.69/825.93/421.61) lat (ms,95%): 49.21 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 8 tps: 206.40 qps: 4132.20 (r/w/o: 2893.50/819.30/419.40) lat (ms,95%): 50.11 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 8 tps: 216.10 qps: 4320.19 (r/w/o: 3024.00/858.60/437.60) lat (ms,95%): 47.47 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 8 tps: 215.40 qps: 4310.49 (r/w/o: 3017.10/856.80/436.60) lat (ms,95%): 47.47 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 8 tps: 213.40 qps: 4264.18 (r/w/o: 2985.18/845.50/433.50) lat (ms,95%): 48.34 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 1439326 write: 408522 other: 208332 total: 2056180 transactions: 102809 (171.34 per sec.) queries: 2056180 (3426.83 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0231s total number of events: 102809 Latency (ms): min: 20.55 avg: 46.69 max: 1381.74 95th percentile: 78.60 sum: 4799764.06 Threads fairness: events (avg/stddev): 12851.1250/18.58 execution time (avg/stddev): 599.9705/0.01 # select insert 16线程　10分钟 sed -i 's/^threads=.*/threads=16/g' config sysbench --config-file=config oltp_read_write --tables=24 --table-size=50000000 run [ 10s ] thds: 16 tps: 169.97 qps: 3421.92 (r/w/o: 2398.22/676.66/347.03) lat (ms,95%): 144.97 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 16 tps: 231.90 qps: 4637.37 (r/w/o: 3246.15/921.22/470.00) lat (ms,95%): 104.84 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 16 tps: 311.78 qps: 6237.09 (r/w/o: 4366.92/1237.01/633.17) lat (ms,95%): 65.65 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 16 tps: 287.50 qps: 5753.82 (r/w/o: 4026.51/1145.20/582.10) lat (ms,95%): 69.29 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 16 tps: 284.10 qps: 5680.37 (r/w/o: 3977.48/1127.59/575.30) lat (ms,95%): 69.29 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 16 tps: 303.70 qps: 6073.10 (r/w/o: 4250.10/1206.90/616.10) lat (ms,95%): 68.05 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 2325722 write: 660088 other: 336650 total: 3322460 transactions: 166123 (276.84 per sec.) queries: 3322460 (5536.80 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0672s total number of events: 166123 Latency (ms): min: 24.50 avg: 57.79 max: 1233.54 95th percentile: 95.81 sum: 9599917.16 Threads fairness: events (avg/stddev): 10382.6875/22.65 execution time (avg/stddev): 599.9948/0.01 # select insert 32线程　10分钟 sed -i 's/^threads=.*/threads=32/g' config sysbench --config-file=config oltp_read_write --tables=24 --table-size=50000000 run [ 10s ] thds: 32 tps: 444.80 qps: 8941.18 (r/w/o: 6264.55/1774.02/902.60) lat (ms,95%): 95.81 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 32 tps: 447.59 qps: 8959.13 (r/w/o: 6272.58/1779.86/906.69) lat (ms,95%): 86.00 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 32 tps: 440.89 qps: 8805.90 (r/w/o: 6161.66/1752.06/892.18) lat (ms,95%): 87.56 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 32 tps: 505.40 qps: 10106.91 (r/w/o: 7073.90/2008.90/1024.10) lat (ms,95%): 82.96 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 32 tps: 486.40 qps: 9739.97 (r/w/o: 6820.75/1934.91/984.31) lat (ms,95%): 84.47 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 32 tps: 477.10 qps: 9532.05 (r/w/o: 6671.67/1894.19/966.20) lat (ms,95%): 87.56 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 3419850 write: 970844 other: 494806 total: 4885500 transactions: 244275 (407.05 per sec.) queries: 4885500 (8141.05 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1055s total number of events: 244275 Latency (ms): min: 23.04 avg: 78.60 max: 1441.26 95th percentile: 121.08 sum: 19200207.50 Threads fairness: events (avg/stddev): 7633.5938/16.54 execution time (avg/stddev): 600.0065/0.02 # select insert 64线程　10分钟 sed -i 's/^threads=.*/threads=64/g' config sysbench --config-file=config oltp_read_write --tables=24 --table-size=50000000 run [ 10s ] thds: 64 tps: 424.39 qps: 8593.20 (r/w/o: 6026.48/1700.35/866.37) lat (ms,95%): 253.35 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 64 tps: 451.41 qps: 9010.57 (r/w/o: 6308.92/1786.73/914.92) lat (ms,95%): 193.38 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 64 tps: 417.90 qps: 8363.85 (r/w/o: 5852.16/1664.29/847.39) lat (ms,95%): 272.27 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 64 tps: 539.30 qps: 10782.19 (r/w/o: 7547.89/2141.50/1092.80) lat (ms,95%): 161.51 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 64 tps: 515.10 qps: 10305.70 (r/w/o: 7214.40/2047.30/1044.00) lat (ms,95%): 164.45 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 64 tps: 546.27 qps: 10927.74 (r/w/o: 7649.00/2174.09/1104.64) lat (ms,95%): 158.63 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 4306274 write: 1222527 other: 623019 total: 6151820 transactions: 307591 (512.48 per sec.) queries: 6151820 (10249.68 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1948s total number of events: 307591 Latency (ms): min: 26.25 avg: 124.86 max: 1593.38 95th percentile: 183.21 sum: 38406426.60 Threads fairness: events (avg/stddev): 4806.1094/14.48 execution time (avg/stddev): 600.1004/0.05 # select insert 128线程　10分钟 sed -i 's/^threads=.*/threads=128/g' config sysbench --config-file=config oltp_read_write --tables=24 --table-size=50000000 run [ 10s ] thds: 128 tps: 590.39 qps: 12021.37 (r/w/o: 8442.39/2367.34/1211.63) lat (ms,95%): 292.60 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 128 tps: 534.41 qps: 10671.02 (r/w/o: 7462.98/2124.82/1083.21) lat (ms,95%): 511.33 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 128 tps: 589.19 qps: 11736.22 (r/w/o: 8211.87/2329.76/1194.58) lat (ms,95%): 320.17 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 128 tps: 610.91 qps: 12240.69 (r/w/o: 8574.63/2428.44/1237.62) lat (ms,95%): 282.25 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 128 tps: 570.00 qps: 11418.01 (r/w/o: 7991.91/2271.00/1155.10) lat (ms,95%): 308.84 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 128 tps: 543.47 qps: 10845.10 (r/w/o: 7586.78/2158.68/1099.64) lat (ms,95%): 404.61 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 4876886 write: 1384663 other: 705431 total: 6966980 transactions: 348349 (580.43 per sec.) queries: 6966980 (11608.66 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1523s total number of events: 348349 Latency (ms): min: 21.36 avg: 220.50 max: 1633.75 95th percentile: 325.98 sum: 76810380.95 Threads fairness: events (avg/stddev): 2721.4766/12.10 execution time (avg/stddev): 600.0811/0.03 # select insert 256线程　10分钟 sed -i 's/^threads=.*/threads=256/g' config sysbench --config-file=config oltp_read_write --tables=24 --table-size=50000000 run [ 10s ] thds: 256 tps: 481.34 qps: 9985.82 (r/w/o: 7030.71/1956.00/999.11) lat (ms,95%): 1327.91 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 256 tps: 601.64 qps: 11971.39 (r/w/o: 8379.42/2375.18/1216.79) lat (ms,95%): 590.56 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 256 tps: 626.57 qps: 12537.25 (r/w/o: 8779.05/2488.98/1269.23) lat (ms,95%): 601.29 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 256 tps: 605.78 qps: 12177.38 (r/w/o: 8511.77/2438.34/1227.27) lat (ms,95%): 634.66 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 256 tps: 575.83 qps: 11480.88 (r/w/o: 8046.78/2268.63/1165.47) lat (ms,95%): 707.07 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 256 tps: 603.38 qps: 12011.43 (r/w/o: 8397.97/2393.81/1219.65) lat (ms,95%): 612.21 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 5317970 write: 1510266 other: 768864 total: 7597100 transactions: 379855 (632.73 per sec.) queries: 7597100 (12654.61 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.3413s total number of events: 379855 Latency (ms): min: 22.27 avg: 404.48 max: 2959.56 95th percentile: 580.02 sum: 153642558.00 Threads fairness: events (avg/stddev): 1483.8086/7.28 execution time (avg/stddev): 600.1662/0.09 更新操作不通过索引列 # select insert 8线程　10分钟 sed -i 's/^threads=.*/threads=8/g' config sysbench --config-file=config oltp_update_non_index --tables=24 --table-size=50000000 run [ 10s ] thds: 8 tps: 1022.25 qps: 1022.25 (r/w/o: 0.00/1013.55/8.70) lat (ms,95%): 11.04 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 8 tps: 1022.90 qps: 1022.90 (r/w/o: 0.00/1015.90/7.00) lat (ms,95%): 10.84 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 8 tps: 1051.90 qps: 1051.90 (r/w/o: 0.00/1044.10/7.80) lat (ms,95%): 10.27 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 8 tps: 1059.20 qps: 1059.20 (r/w/o: 0.00/1050.70/8.50) lat (ms,95%): 10.46 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 8 tps: 994.00 qps: 994.00 (r/w/o: 0.00/985.90/8.10) lat (ms,95%): 10.27 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 8 tps: 1067.00 qps: 1067.00 (r/w/o: 0.00/1058.10/8.90) lat (ms,95%): 10.46 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 577971 other: 4621 total: 582592 transactions: 582592 (970.97 per sec.) queries: 582592 (970.97 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0085s total number of events: 582592 Latency (ms): min: 0.38 avg: 8.24 max: 588.56 95th percentile: 12.52 sum: 4799235.71 Threads fairness: events (avg/stddev): 72824.0000/62.01 execution time (avg/stddev): 599.9045/0.00 # select insert 16线程　10分钟 sed -i 's/^threads=.*/threads=16/g' config sysbench --config-file=config oltp_update_non_index --tables=24 --table-size=50000000 run [ 10s ] thds: 16 tps: 1894.52 qps: 1894.52 (r/w/o: 0.00/1878.92/15.60) lat (ms,95%): 12.30 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 16 tps: 1848.30 qps: 1848.30 (r/w/o: 0.00/1833.90/14.40) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 16 tps: 1949.59 qps: 1949.59 (r/w/o: 0.00/1933.69/15.90) lat (ms,95%): 10.84 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 16 tps: 1944.80 qps: 1944.80 (r/w/o: 0.00/1929.80/15.00) lat (ms,95%): 10.84 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 16 tps: 1871.71 qps: 1871.71 (r/w/o: 0.00/1857.21/14.50) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 16 tps: 1762.30 qps: 1762.30 (r/w/o: 0.00/1748.40/13.90) lat (ms,95%): 11.45 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1080035 other: 8856 total: 1088891 transactions: 1088891 (1814.79 per sec.) queries: 1088891 (1814.79 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0077s total number of events: 1088891 Latency (ms): min: 0.41 avg: 8.82 max: 757.97 95th percentile: 13.22 sum: 9598670.78 Threads fairness: events (avg/stddev): 68055.6875/61.00 execution time (avg/stddev): 599.9169/0.00 # select insert 32线程　10分钟 sed -i 's/^threads=.*/threads=32/g' config sysbench --config-file=config oltp_update_non_index --tables=24 --table-size=50000000 run [ 10s ] thds: 32 tps: 3248.33 qps: 3248.33 (r/w/o: 0.00/3224.63/23.70) lat (ms,95%): 13.95 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 32 tps: 3178.02 qps: 3178.02 (r/w/o: 0.00/3153.52/24.50) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 32 tps: 3405.79 qps: 3405.79 (r/w/o: 0.00/3376.89/28.90) lat (ms,95%): 12.52 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 32 tps: 3367.72 qps: 3367.72 (r/w/o: 0.00/3339.32/28.40) lat (ms,95%): 12.52 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 32 tps: 3265.01 qps: 3265.01 (r/w/o: 0.00/3238.41/26.60) lat (ms,95%): 13.22 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 32 tps: 3237.49 qps: 3237.49 (r/w/o: 0.00/3213.79/23.70) lat (ms,95%): 12.98 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1957338 other: 15562 total: 1957338 transactions: 1957338 (3262.18 per sec.) queries: 1957338 (3262.18 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0078s total number of events: 1957338 Latency (ms): min: 0.41 avg: 9.81 max: 902.52 95th percentile: 13.46 sum: 19197804.63 Threads fairness: events (avg/stddev): 61166.8125/61.01 execution time (avg/stddev): 599.9314/0.00 # select insert 64线程　10分钟 sed -i 's/^threads=.*/threads=64/g' config sysbench --config-file=config oltp_update_non_index --tables=24 --table-size=50000000 run [ 10s ] thds: 64 tps: 5254.99 qps: 5254.99 (r/w/o: 0.00/5212.31/42.68) lat (ms,95%): 17.95 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 64 tps: 5625.60 qps: 5625.60 (r/w/o: 0.00/5581.80/43.80) lat (ms,95%): 16.12 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 64 tps: 5203.48 qps: 5203.48 (r/w/o: 0.00/5162.28/41.20) lat (ms,95%): 19.29 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 64 tps: 3663.77 qps: 3663.77 (r/w/o: 0.00/3633.67/30.10) lat (ms,95%): 33.72 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 64 tps: 5478.25 qps: 5478.25 (r/w/o: 0.00/5434.95/43.30) lat (ms,95%): 20.00 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 64 tps: 5174.52 qps: 5174.52 (r/w/o: 0.00/5129.92/44.60) lat (ms,95%): 17.63 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 3042486 other: 24646 total: 3067132 transactions: 3067132 (5111.78 per sec.) queries: 3067132 (5111.78 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0115s total number of events: 3067132 Latency (ms): min: 0.41 avg: 12.52 max: 1070.55 95th percentile: 18.95 sum: 38396613.65 Threads fairness: events (avg/stddev): 47923.9375/58.56 execution time (avg/stddev): 599.9471/0.00 # select insert 128线程　10分钟 sed -i 's/^threads=.*/threads=128/g' config sysbench --config-file=config oltp_update_non_index --tables=24 --table-size=50000000 run [ 10s ] thds: 128 tps: 7684.84 qps: 7684.84 (r/w/o: 0.00/7622.50/62.35) lat (ms,95%): 27.66 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 128 tps: 7721.65 qps: 7721.65 (r/w/o: 0.00/7660.55/61.10) lat (ms,95%): 27.17 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 128 tps: 8408.18 qps: 8408.18 (r/w/o: 0.00/8343.68/64.50) lat (ms,95%): 25.74 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 128 tps: 7419.08 qps: 7419.08 (r/w/o: 0.00/7358.58/60.50) lat (ms,95%): 31.37 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 128 tps: 5580.22 qps: 5580.22 (r/w/o: 0.00/5537.53/42.70) lat (ms,95%): 51.02 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 128 tps: 6943.18 qps: 6943.18 (r/w/o: 0.00/6883.98/59.20) lat (ms,95%): 41.10 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 4387663 other: 35553 total: 4423216 transactions: 4423216 (7371.75 per sec.) queries: 4423216 (7371.75 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0212s total number of events: 4423216 Latency (ms): min: 0.43 avg: 17.36 max: 809.63 95th percentile: 32.53 sum: 76796124.53 Threads fairness: events (avg/stddev): 34556.3750/76.45 execution time (avg/stddev): 599.9697/0.01 # select insert 256线程　10分钟 sed -i 's/^threads=.*/threads=256/g' config sysbench --config-file=config oltp_update_non_index --tables=24 --table-size=50000000 run [ 10s ] thds: 256 tps: 10767.47 qps: 10767.57 (r/w/o: 0.00/10679.60/87.97) lat (ms,95%): 44.98 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 256 tps: 9622.65 qps: 9622.55 (r/w/o: 0.00/9538.56/84.00) lat (ms,95%): 49.21 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 256 tps: 9375.97 qps: 9375.97 (r/w/o: 0.00/9301.17/74.80) lat (ms,95%): 52.89 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 256 tps: 10087.61 qps: 10087.61 (r/w/o: 0.00/10003.91/83.70) lat (ms,95%): 54.83 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 256 tps: 9808.50 qps: 9808.50 (r/w/o: 0.00/9731.70/76.80) lat (ms,95%): 51.02 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 256 tps: 10201.78 qps: 10201.78 (r/w/o: 0.00/10119.98/81.80) lat (ms,95%): 53.85 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 5666278 other: 45899 total: 5712177 transactions: 5712177 (9519.67 per sec.) queries: 5712177 (9519.67 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0379s total number of events: 5712177 Latency (ms): min: 0.47 avg: 26.89 max: 1339.52 95th percentile: 55.82 sum: 153596923.52 Threads fairness: events (avg/stddev): 22313.1914/73.30 execution time (avg/stddev): 599.9880/0.01 只写入测试 # select insert 8线程　10分钟 sed -i 's/^threads=.*/threads=8/g' config sysbench --config-file=config oltp_write_only --tables=24 --table-size=50000000 run [ 10s ] thds: 8 tps: 383.63 qps: 2303.99 (r/w/o: 0.00/1526.03/777.96) lat (ms,95%): 26.20 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 8 tps: 362.21 qps: 2173.16 (r/w/o: 0.00/1441.24/731.92) lat (ms,95%): 27.17 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 8 tps: 394.80 qps: 2369.40 (r/w/o: 0.00/1570.70/798.70) lat (ms,95%): 26.20 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 8 tps: 397.40 qps: 2384.51 (r/w/o: 0.00/1579.81/804.70) lat (ms,95%): 25.74 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 8 tps: 395.30 qps: 2371.51 (r/w/o: 0.00/1570.11/801.40) lat (ms,95%): 25.74 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 8 tps: 384.20 qps: 2305.99 (r/w/o: 0.00/1529.09/776.90) lat (ms,95%): 26.20 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 710760 other: 361812 total: 1072572 transactions: 178762 (297.90 per sec.) queries: 1072572 (1787.37 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0819s total number of events: 178762 Latency (ms): min: 11.69 avg: 26.85 max: 1572.86 95th percentile: 41.85 sum: 4799929.32 Threads fairness: events (avg/stddev): 22345.2500/21.72 execution time (avg/stddev): 599.9912/0.02 # select insert 16线程　10分钟 sed -i 's/^threads=.*/threads=16/g' config sysbench --config-file=config oltp_write_only --tables=24 --table-size=50000000 run [ 10s ] thds: 16 tps: 576.19 qps: 3462.96 (r/w/o: 0.00/2294.47/1168.48) lat (ms,95%): 33.72 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 16 tps: 565.92 qps: 3394.51 (r/w/o: 0.00/2248.58/1145.94) lat (ms,95%): 34.95 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 16 tps: 554.40 qps: 3326.81 (r/w/o: 0.00/2204.10/1122.70) lat (ms,95%): 38.25 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 16 tps: 517.70 qps: 3105.10 (r/w/o: 0.00/2055.60/1049.50) lat (ms,95%): 40.37 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 16 tps: 543.60 qps: 3264.38 (r/w/o: 0.00/2163.99/1100.39) lat (ms,95%): 37.56 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 16 tps: 599.00 qps: 3592.12 (r/w/o: 0.00/2382.41/1209.71) lat (ms,95%): 36.24 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1056694 other: 537968 total: 1594662 transactions: 265777 (442.93 per sec.) queries: 1594662 (2657.57 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0444s total number of events: 265777 Latency (ms): min: 9.65 avg: 36.12 max: 1488.15 95th percentile: 75.82 sum: 9599571.75 Threads fairness: events (avg/stddev): 16611.0625/27.35 execution time (avg/stddev): 599.9732/0.01 # select insert 32线程　10分钟 sed -i 's/^threads=.*/threads=32/g' config sysbench --config-file=config oltp_write_only --tables=24 --table-size=50000000 run [ 10s ] thds: 32 tps: 859.62 qps: 5170.24 (r/w/o: 0.00/3428.50/1741.74) lat (ms,95%): 45.79 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 32 tps: 885.10 qps: 5307.11 (r/w/o: 0.00/3514.91/1792.20) lat (ms,95%): 45.79 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 32 tps: 799.70 qps: 4799.20 (r/w/o: 0.00/3182.60/1616.60) lat (ms,95%): 51.02 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 32 tps: 868.30 qps: 5211.60 (r/w/o: 0.00/3455.90/1755.70) lat (ms,95%): 48.34 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 32 tps: 818.26 qps: 4908.36 (r/w/o: 0.00/3254.14/1654.22) lat (ms,95%): 53.85 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 32 tps: 753.32 qps: 4520.79 (r/w/o: 0.00/2996.16/1524.63) lat (ms,95%): 56.84 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1971854 other: 1003240 total: 2975094 transactions: 495849 (826.37 per sec.) queries: 2975094 (4958.20 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0342s total number of events: 495849 Latency (ms): min: 12.46 avg: 38.72 max: 1465.32 95th percentile: 61.08 sum: 19199052.08 Threads fairness: events (avg/stddev): 15495.2812/20.61 execution time (avg/stddev): 599.9704/0.01 # select insert 64线程　10分钟 sed -i 's/^threads=.*/threads=64/g' config sysbench --config-file=config oltp_write_only --tables=24 --table-size=50000000 run [ 10s ] thds: 64 tps: 1093.67 qps: 6581.09 (r/w/o: 0.00/4362.47/2218.62) lat (ms,95%): 81.48 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 64 tps: 1178.65 qps: 7073.00 (r/w/o: 0.00/4689.40/2383.60) lat (ms,95%): 81.48 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 64 tps: 1192.91 qps: 7155.27 (r/w/o: 0.00/4743.05/2412.22) lat (ms,95%): 78.60 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 64 tps: 1112.55 qps: 6676.09 (r/w/o: 0.00/4427.20/2248.88) lat (ms,95%): 97.55 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 64 tps: 1235.21 qps: 7412.53 (r/w/o: 0.00/4919.91/2492.62) lat (ms,95%): 71.83 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 64 tps: 1168.49 qps: 7011.36 (r/w/o: 0.00/4650.08/2361.29) lat (ms,95%): 75.82 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 2584664 other: 1313014 total: 3897678 transactions: 649613 (1082.59 per sec.) queries: 3897678 (6495.57 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0507s total number of events: 649613 Latency (ms): min: 13.69 avg: 59.11 max: 1652.85 95th percentile: 97.55 sum: 38399561.18 Threads fairness: events (avg/stddev): 10150.2031/19.65 execution time (avg/stddev): 599.9931/0.01 # select insert 128线程　10分钟 sed -i 's/^threads=.*/threads=128/g' config sysbench --config-file=config oltp_write_only --tables=24 --table-size=50000000 run [ 10s ] thds: 128 tps: 1556.63 qps: 9380.96 (r/w/o: 0.00/6225.83/3155.12) lat (ms,95%): 123.28 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 128 tps: 1487.15 qps: 8921.02 (r/w/o: 0.00/5918.31/3002.71) lat (ms,95%): 127.81 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 128 tps: 1570.02 qps: 9419.09 (r/w/o: 0.00/6244.46/3174.63) lat (ms,95%): 118.92 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 128 tps: 1491.44 qps: 8948.62 (r/w/o: 0.00/5938.95/3009.67) lat (ms,95%): 142.39 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 128 tps: 1623.63 qps: 9748.36 (r/w/o: 0.00/6465.61/3282.75) lat (ms,95%): 121.08 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 128 tps: 1580.54 qps: 9487.24 (r/w/o: 0.00/6290.56/3196.69) lat (ms,95%): 127.81 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 3509747 other: 1781467 total: 5291214 transactions: 881869 (1469.53 per sec.) queries: 5291214 (8817.17 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1021s total number of events: 881869 Latency (ms): min: 14.31 avg: 87.09 max: 2300.36 95th percentile: 137.35 sum: 76803193.61 Threads fairness: events (avg/stddev): 6889.6016/17.75 execution time (avg/stddev): 600.0250/0.03 # select insert 256线程　10分钟 sed -i 's/^threads=.*/threads=256/g' config sysbench --config-file=config oltp_write_only --tables=24 --table-size=50000000 run [ 10s ] thds: 256 tps: 1466.07 qps: 8894.63 (r/w/o: 0.00/5909.39/2985.24) lat (ms,95%): 363.18 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 256 tps: 1367.40 qps: 8193.03 (r/w/o: 0.00/5432.02/2761.01) lat (ms,95%): 580.02 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 256 tps: 1569.85 qps: 9425.41 (r/w/o: 0.00/6254.20/3171.20) lat (ms,95%): 227.40 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 256 tps: 938.90 qps: 5636.79 (r/w/o: 0.00/3741.69/1895.10) lat (ms,95%): 877.61 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 256 tps: 1494.38 qps: 8950.90 (r/w/o: 0.00/5931.53/3019.37) lat (ms,95%): 277.21 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 256 tps: 1281.90 qps: 7691.88 (r/w/o: 0.00/5101.29/2590.59) lat (ms,95%): 493.24 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 3973855 other: 2015555 total: 5989410 transactions: 998235 (1663.36 per sec.) queries: 5989410 (9980.18 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.1287s total number of events: 998235 Latency (ms): min: 14.18 avg: 153.88 max: 3163.24 95th percentile: 287.38 sum: 153611743.50 Threads fairness: events (avg/stddev): 3899.3555/16.97 execution time (avg/stddev): 600.0459/0.04 写入和删除并行 # select insert 8线程　10分钟 sed -i 's/^threads=.*/threads=8/g' config sysbench --config-file=config oltp_delete --tables=24 --table-size=50000000 run [ 10s ] thds: 8 tps: 348.04 qps: 348.04 (r/w/o: 0.00/345.34/2.70) lat (ms,95%): 38.25 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 8 tps: 544.31 qps: 544.31 (r/w/o: 0.00/541.51/2.80) lat (ms,95%): 23.52 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 8 tps: 716.41 qps: 716.41 (r/w/o: 0.00/711.31/5.10) lat (ms,95%): 17.95 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 8 tps: 831.29 qps: 831.29 (r/w/o: 0.00/825.79/5.50) lat (ms,95%): 13.95 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 8 tps: 808.50 qps: 808.50 (r/w/o: 0.00/802.50/6.00) lat (ms,95%): 13.95 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 8 tps: 853.80 qps: 853.80 (r/w/o: 0.00/847.00/6.80) lat (ms,95%): 13.70 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 399568 other: 7832 total: 407400 transactions: 407400 (678.99 per sec.) queries: 407400 (678.99 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0086s total number of events: 407400 Latency (ms): min: 0.42 avg: 11.78 max: 613.16 95th percentile: 21.50 sum: 4799469.44 Threads fairness: events (avg/stddev): 50925.0000/81.12 execution time (avg/stddev): 599.9337/0.00 # select insert 16线程　10分钟 sed -i 's/^threads=.*/threads=16/g' config sysbench --config-file=config oltp_delete --tables=24 --table-size=50000000 run [ 10s ] thds: 16 tps: 1207.72 qps: 1207.72 (r/w/o: 0.00/1168.83/38.89) lat (ms,95%): 23.52 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 16 tps: 1433.40 qps: 1433.40 (r/w/o: 0.00/1380.60/52.80) lat (ms,95%): 15.83 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 16 tps: 1440.80 qps: 1440.80 (r/w/o: 0.00/1393.40/47.40) lat (ms,95%): 15.55 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 16 tps: 1304.10 qps: 1304.10 (r/w/o: 0.00/1261.30/42.80) lat (ms,95%): 16.12 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 16 tps: 1359.79 qps: 1359.79 (r/w/o: 0.00/1312.89/46.90) lat (ms,95%): 16.71 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 16 tps: 1305.90 qps: 1305.90 (r/w/o: 0.00/1258.40/47.50) lat (ms,95%): 17.32 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 706819 other: 40642 total: 747461 transactions: 747461 (1245.71 per sec.) queries: 747461 (1245.71 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0255s total number of events: 747461 Latency (ms): min: 0.43 avg: 12.84 max: 670.51 95th percentile: 26.20 sum: 9599297.16 Threads fairness: events (avg/stddev): 46716.3125/67.97 execution time (avg/stddev): 599.9561/0.01 # select insert 32线程　10分钟 sed -i 's/^threads=.*/threads=32/g' config sysbench --config-file=config oltp_delete --tables=24 --table-size=50000000 run [ 10s ] thds: 32 tps: 1224.24 qps: 1224.24 (r/w/o: 0.00/1130.06/94.18) lat (ms,95%): 62.19 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 32 tps: 1434.09 qps: 1434.09 (r/w/o: 0.00/1321.99/112.10) lat (ms,95%): 57.87 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 32 tps: 1347.71 qps: 1347.71 (r/w/o: 0.00/1242.32/105.39) lat (ms,95%): 59.99 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 32 tps: 1515.80 qps: 1515.80 (r/w/o: 0.00/1389.79/126.01) lat (ms,95%): 57.87 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 32 tps: 1355.64 qps: 1355.64 (r/w/o: 0.00/1248.84/106.80) lat (ms,95%): 56.84 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 32 tps: 2596.62 qps: 2596.62 (r/w/o: 0.00/2384.71/211.91) lat (ms,95%): 19.29 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1200197 other: 155257 total: 1355454 transactions: 1355454 (2259.03 per sec.) queries: 1355454 (2259.03 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0144s total number of events: 1355454 Latency (ms): min: 0.40 avg: 14.16 max: 994.74 95th percentile: 31.37 sum: 19198477.28 Threads fairness: events (avg/stddev): 42357.9375/92.54 execution time (avg/stddev): 599.9524/0.00 # select insert 64线程　10分钟 sed -i 's/^threads=.*/threads=64/g' config sysbench --config-file=config oltp_delete --tables=24 --table-size=50000000 run [ 10s ] thds: 64 tps: 3905.66 qps: 3905.66 (r/w/o: 0.00/3305.53/600.14) lat (ms,95%): 26.68 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 64 tps: 4473.34 qps: 4473.34 (r/w/o: 0.00/3772.41/700.93) lat (ms,95%): 23.10 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 64 tps: 4169.47 qps: 4169.47 (r/w/o: 0.00/3521.56/647.91) lat (ms,95%): 24.38 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 64 tps: 3177.36 qps: 3177.36 (r/w/o: 0.00/2673.27/504.09) lat (ms,95%): 50.11 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 64 tps: 4199.14 qps: 4199.14 (r/w/o: 0.00/3525.13/674.01) lat (ms,95%): 29.19 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 64 tps: 3963.50 qps: 3963.50 (r/w/o: 0.00/3330.60/632.90) lat (ms,95%): 29.19 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 1899872 other: 501168 total: 2401040 transactions: 2401040 (4001.57 per sec.) queries: 2401040 (4001.57 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0238s total number of events: 2401040 Latency (ms): min: 0.42 avg: 15.99 max: 1186.76 95th percentile: 34.95 sum: 38397943.33 Threads fairness: events (avg/stddev): 37516.2500/103.31 execution time (avg/stddev): 599.9679/0.01 # select insert 128线程　10分钟 sed -i 's/^threads=.*/threads=128/g' config sysbench --config-file=config oltp_delete --tables=24 --table-size=50000000 run [ 10s ] thds: 128 tps: 6033.46 qps: 6033.46 (r/w/o: 0.00/4447.60/1585.86) lat (ms,95%): 38.94 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 128 tps: 6400.79 qps: 6400.79 (r/w/o: 0.00/4686.63/1714.16) lat (ms,95%): 38.25 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 128 tps: 6386.65 qps: 6386.65 (r/w/o: 0.00/4668.27/1718.38) lat (ms,95%): 39.65 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 128 tps: 6619.81 qps: 6619.81 (r/w/o: 0.00/4822.01/1797.80) lat (ms,95%): 38.94 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 128 tps: 6397.74 qps: 6397.74 (r/w/o: 0.00/4629.15/1768.58) lat (ms,95%): 40.37 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 128 tps: 6618.83 qps: 6618.83 (r/w/o: 0.00/4771.96/1846.87) lat (ms,95%): 39.65 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 2502286 other: 1256654 total: 3758940 transactions: 3758940 (6264.62 per sec.) queries: 3758940 (6264.62 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0259s total number of events: 3758940 Latency (ms): min: 0.41 avg: 20.43 max: 1379.84 95th percentile: 50.11 sum: 76796715.81 Threads fairness: events (avg/stddev): 29366.7188/123.76 execution time (avg/stddev): 599.9743/0.01 # select insert 256线程　10分钟 sed -i 's/^threads=.*/threads=256/g' config sysbench --config-file=config oltp_delete --tables=24 --table-size=50000000 run [ 10s ] thds: 256 tps: 4187.74 qps: 4187.74 (r/w/o: 0.00/2502.24/1685.50) lat (ms,95%): 231.53 err/s: 0.00 reconn/s: 0.00 [ 20s ] thds: 256 tps: 8577.92 qps: 8577.92 (r/w/o: 0.00/5148.07/3429.85) lat (ms,95%): 73.13 err/s: 0.00 reconn/s: 0.00 [ 30s ] thds: 256 tps: 9409.80 qps: 9409.80 (r/w/o: 0.00/5631.80/3778.00) lat (ms,95%): 71.83 err/s: 0.00 reconn/s: 0.00 [ 40s ] thds: 256 tps: 8414.93 qps: 8414.93 (r/w/o: 0.00/4979.84/3435.09) lat (ms,95%): 74.46 err/s: 0.00 reconn/s: 0.00 [ 50s ] thds: 256 tps: 9383.60 qps: 9383.60 (r/w/o: 0.00/5558.87/3824.73) lat (ms,95%): 71.83 err/s: 0.00 reconn/s: 0.00 [ 60s ] thds: 256 tps: 9210.60 qps: 9210.60 (r/w/o: 0.00/5412.52/3798.09) lat (ms,95%): 74.46 err/s: 0.00 reconn/s: 0.00 SQL statistics: queries performed: read: 0 write: 2847913 other: 2531947 total: 5379860 transactions: 5379860 (8965.40 per sec.) queries: 5379860 (8965.40 per sec.) ignored errors: 0 (0.00 per sec.) reconnects: 0 (0.00 per sec.) General statistics: total time: 600.0677s total number of events: 5379860 Latency (ms): min: 0.40 avg: 28.55 max: 1388.04 95th percentile: 81.48 sum: 153602514.04 Threads fairness: events (avg/stddev): 21015.0781/128.10 execution time (avg/stddev): 600.0098/0.02 测试结果 主键查询 线程 QPS 查询总数 TPS 事物总数 95% latency (ms) ８ 6512.09 3908017 6512.09 3908017 3.30 16 11408.77 6845695 11408.77 6845695 4.10 32 17198.11 10320806 17198.11 10320806 5.67 64 21451.15 12871444 21451.15 12871444 8.43 128 26426.46 15867653 26426.46 15867653 13.46 256 25311.42 15188519 25311.42 15188519 27.17 主键更新 线程 QPS 查询总数 TPS 事物总数 写入数 95% latency (ms) ８ 253.45 152077 253.45 152077 150714 62.19 16 459.40 275657 459.40 275657 273213 75.82 32 1134.22 680722 1134.22 680722 674705 53.85 64 1925.14 1155177 1925.14 1155177 1145001 56.84 128 4004.38 2403039 4004.38 2403039 2381625 58.92 256 5258.48 3157178 5258.48 3157178 3129355 97.55 读写混合 线程 QPS 查询总数 TPS 事物总数 读入数 写入数 95% latency (ms) ８ 3426.83 2056180 171.34 102809 1439326 408522 78.60 16 5536.80 3322460 276.84 166123 2325722 660088 95.81 32 8141.05 4885500 407.05 244275 3419850 970844 121.08 64 10249.68 6151820 512.48 307591 4306274 1222527 183.21 128 11608.66 6966980 580.43 348349 4876886 1384663 325.98 256 12654.61 7597100 632.73 379855 5317970 1510266 580.02 更新不通过索引 线程 QPS 查询总数 TPS 事物总数 写入数 95% latency (ms) ８ 970.97 582592 970.97 582592 577971 12.52 16 1814.79 1088891 1814.79 1088891 1080035 13.22 32 3262.18 1957338 3262.18 1957338 1957338 13.46 64 5111.78 3067132 5111.78 3067132 3042486 18.95 128 7371.75 4423216 7371.75 4423216 4387663 32.53 256 9519.67 5712177 9519.67 5712177 5666278 55.82 只写入 线程 QPS 查询总数 TPS 事物总数 写入数 95% latency (ms) ８ 1787.37 1072572 297.90 178762 710760 41.85 16 2657.57 1594662 442.93 265777 1056694 75.82 32 4958.20 2975094 826.37 495849 1971854 61.08 64 6495.57 3897678 1082.59 649613 2584664 97.55 128 8817.17 5291214 1469.53 881869 3509747 137.35 256 9980.18 5989410 1663.36 998235 3973855 287.38 写入和删除并行 线程 QPS 查询总数 TPS 事物总数 写入数 95% latency (ms) ８ 678.99 407400 678.99 407400 399568 21.50 16 1245.71 747461 1245.71 747461 706819 26.2 32 2259.03 1355454 2259.03 1355454 1200197 31.37 64 4001.57 2401040 4001.57 2401040 1899872 34.95 128 6264.62 3758940 6264.62 3758940 2502286 50.11 256 8965.40 5379860 8965.40 5379860 2847913 81.48 前95%毫秒数 线程 主键查询 主键更新 读写混合 更新非索引 只写入 写入和删除并行 ８ 3.30 62.19 78.60 12.52 41.85 21.50 16 4.10 75.82 95.81 13.22 75.82 26.2 32 5.67 53.85 121.08 13.46 61.08 31.37 64 8.43 56.84 183.21 18.95 97.55 34.95 128 13.46 58.92 325.98 32.53 137.35 50.11 256 27.17 97.55 580.02 55.82 287.38 81.48 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"解决方案/延迟队列解决方案.html":{"url":"解决方案/延迟队列解决方案.html","title":"延迟队列解决方案","keywords":"","body":"1.1. 延迟队列实现方式总结1.2. Redis实现延迟队列1.2.1. redis失效监听事件1.2.2. 使用redisson实现延迟队列1.1. 延迟队列实现方式总结 如基于RabbitMQ的队列ttl+死信路由策略：通过设置一个队列的超时未消费时间，配合死信路由策略，到达时间未消费后，回会将此消息路由到指定队列 基于RabbitMQ延迟队列插件（rabbitmq-delayed-message-exchange）：发送消息时通过在请求头添加延时参数（headers.put(\"x-delay\", 5000)）即可达到延迟队列的效果 使用redis的zset有序性，轮询zset中的每个元素，到点后将内容迁移至待消费的队列，redisson已有实现 使用redis的key的过期通知策略，设置一个key的过期时间为延迟时间，过期后通知客户端1.2. Redis实现延迟队列 失效监听 redisson实现发布订阅延迟1.2.1. redis失效监听事件 集成KeyExpirationEventMessageListener类实现redis失效监听事件此种实现面临的问题 redis的失效监听事件会存在一定的时间差，并且当数据量越大时，误差会越大。 redis的失效监听事件会将所有key失效都会通知到onMessage,如果针对一个key，分布式业务的场景下，会出现重复消费的问题。（可以增加分布式锁的实现，但是redisson分布式锁提供了另一种延迟队列的实现方式）开发准备 redis需要在服务端开启配置，打开redis服务的配置文件 添加notify-keyspace-events Ex 相关参数如下：K：keyspace事件，事件以__keyspace@__为前缀进行发布； E：keyevent事件，事件以__keyevent@__为前缀进行发布； g：一般性的，非特定类型的命令，比如del，expire，rename等； $：字符串特定命令； l：列表特定命令； s：集合特定命令； h：哈希特定命令； z：有序集合特定命令； x：过期事件，当某个键过期并删除时会产生该事件； e：驱逐事件，当某个键因maxmemore策略而被删除时，产生该事件； A：g$lshzxe的别名，因此”AKE”意味着所有事件。 基础实现 加入依赖 org.springframework.boot spring-boot-starter-data-redis 可正常连接存取redis数据之后，创建监听类RedisKeyExpirationListener继承KeyExpirationEventMessageListener，重写onMessage方法。（key失效之后，会发出onMessage方法，之呢个获取失效的key值，不能获取key对应的value值）。 import com.dadi01.scrm.service.member.api.common.MemberStatusEnum; import com.dadi01.scrm.service.member.provider.service.base.IBaseMemberService; import lombok.extern.slf4j.Slf4j; import org.springframework.data.redis.connection.Message; import org.springframework.data.redis.listener.KeyExpirationEventMessageListener; import org.springframework.data.redis.listener.RedisMessageListenerContainer; import org.springframework.stereotype.Component; /** * @author lviter */ @Component @Slf4j public class RedisKeyExpirationListener extends KeyExpirationEventMessageListener { private final IBaseMemberService baseMemberService; private final static String MEMBER_LOCK_ACCOUNT_SUFFIX = \".lock_account\"; private final static String MEMBER_LOCK_ACCOUNT_DOMAIN_SUFFIX = \"T\"; private final static String MEMBER_LOCK_ACCOUNT_MEMBER_SUFFIX = \"M\"; private final static String MEMBER_REDISSON_LOCK = \".member_lock_redisson\"; private final static int WAIT_TIME = 5; private final static int LEASE_TIME = 10; public RedisKeyExpirationListener(RedisMessageListenerContainer redisMessageListenerContainer, IBaseMemberService baseMemberService) { super(redisMessageListenerContainer); this.baseMemberService = baseMemberService; } @Override public void onMessage(Message message, byte[] pattern) { //获取失效的key String expiredKey = message.toString(); log.info(\"================================get on message:{}====================\", expiredKey); if (expiredKey.endsWith(MEMBER_LOCK_ACCOUNT_SUFFIX)) { log.info(\"================================on message:{}====================\", expiredKey); try { log.info(\"=======待解锁账号解锁======expiredKey:{}\", expiredKey); String tenantId = expiredKey.substring(expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_DOMAIN_SUFFIX) + 1, expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_MEMBER_SUFFIX)); String memberId = expiredKey.substring(expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_MEMBER_SUFFIX) + 1, expiredKey.indexOf(MEMBER_LOCK_ACCOUNT_SUFFIX)); baseMemberService.updateAccount(Integer.parseInt(tenantId), Long.parseLong(memberId), MemberStatusEnum.NORMAL.getCode(), null); } catch (Exception exception) { log.info(\"auto unlock fail,expired key:{},exception:{}\", expiredKey, exception.getMessage()); } } } } 创建一个配置类RedisConfig /** * @author lviter */ @Configuration public class RedisConfig { @Value(\"${redis.dbIndex}\") private Integer dbIndex; private final String TOPIC = \"__keyevent@\" + dbIndex + \"__:expired\"; private final RedisConnectionFactory redisConnectionFactory; public RedisConfig(RedisConnectionFactory redisConnectionFactory) { this.redisConnectionFactory = redisConnectionFactory; } @Bean public RedisMessageListenerContainer redisMessageListenerContainer() { RedisMessageListenerContainer redisMessageListenerContainer = new RedisMessageListenerContainer(); redisMessageListenerContainer.setConnectionFactory(redisConnectionFactory); //keyevent事件，事件以__keyevent@__为前缀进行发布 //db为redis第几个库 db2... // redisMessageListenerContainer.addMessageListener(redisKeyExpirationListener, new PatternTopic(TOPIC)); return redisMessageListenerContainer; } } 1.2.2. 使用redisson实现延迟队列 由于延时队列持久化在redis中，所以机器宕机数据不会异常丢失，机器重启后，会正常消费队列中积累的任务 redisson实现延迟队列的原理 使用redis的zset有序性，轮询zset中的每个元素，到点后将内容迁移至待消费的队列 延迟队列配置 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import org.redisson.api.RBlockingQueue; import org.redisson.api.RDelayedQueue; import org.redisson.api.RedissonClient; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * @author lviter * redisson延迟队列 */ @Configuration public class RedissonQueueConfig { private final String queueName = \"queue\"; @Bean public RBlockingQueue rBlockingQueue(@Qualifier(\"redissonSingle\") RedissonClient redissonClient) { return redissonClient.getBlockingQueue(queueName); } @Bean(name = \"rDelayedQueue\") public RDelayedQueue rDelayedQueue(@Qualifier(\"redissonSingle\") RedissonClient redissonClient, @Qualifier(\"rBlockingQueue\") RBlockingQueue blockQueue) { return redissonClient.getDelayedQueue(blockQueue); } } 定义队列使用接口 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import java.util.concurrent.TimeUnit; /** * @author lviter */ public interface DelayQueue { /** * 发布 * * @param object * @return */ Boolean offer(Object object); /** * 带延迟功能的队列 * * @param object * @param time * @param timeUnit */ void offer(Object object, Long time, TimeUnit timeUnit); void offerAsync(Object object, Long time, TimeUnit timeUnit); Boolean offerAsync(Object object); } 延迟队列实现 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import org.redisson.api.RDelayedQueue; import org.redisson.api.RFuture; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; import javax.annotation.Resource; import java.util.concurrent.ExecutionException; import java.util.concurrent.TimeUnit; /** * @author lviter */ @Component public class RedissonDelayQueue implements DelayQueue { private static Logger log = LoggerFactory.getLogger(RedissonDelayQueue.class); @Resource(name = \"rDelayedQueue\") private RDelayedQueue rDelayedQueue; @Override public Boolean offer(Object object) { return rDelayedQueue.offer(object); } @Override public void offer(Object object, Long time, TimeUnit timeUnit) { rDelayedQueue.offer(object, time, timeUnit); } @Override public void offerAsync(Object object, Long time, TimeUnit timeUnit) { rDelayedQueue.offerAsync(object, time, timeUnit); } @Override public Boolean offerAsync(Object object) { boolean flag = false; RFuture rFuture = rDelayedQueue.offerAsync(object); try { flag = rFuture.get(); } catch (InterruptedException | ExecutionException e) { log.info(\"offerAsync exception:{}\", e.getMessage()); e.printStackTrace(); } return flag; } } 启动一个后台监控线程 package com.dadi01.scrm.service.member.provider.config.redisson.delay; import org.redisson.api.RBlockingQueue; import org.springframework.stereotype.Component; import javax.annotation.PostConstruct; import javax.annotation.Resource; /** * @author lviter */ @Component public class RedissonTask { @Resource(name = \"rBlockingQueue\") private RBlockingQueue rBlockingQueue; @PostConstruct public void take() { new Thread(() -> { while (true) { try { System.out.println(\"=========================\" + rBlockingQueue.take()); } catch (InterruptedException e) { e.printStackTrace(); } } }).start(); } } 使用延迟队列发送 package com.dadi01.scrm.service.member.provider.impl; import org.junit.Test; import org.junit.runner.RunWith; import org.mybatis.spring.annotation.MapperScan; import org.redisson.api.RDelayedQueue; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.ActiveProfiles; import org.springframework.test.context.junit4.SpringRunner; import javax.annotation.Resource; import java.util.concurrent.TimeUnit; @RunWith(SpringRunner.class) @SpringBootTest @ActiveProfiles(value = \"llh\") @MapperScan(\"com.dadi01.scrm.service.member.provider.mapper\") public class RDelayQueueTests { @Resource(name = \"rDelayedQueue\") private RDelayedQueue rDelayedQueue; @Test public void offerAsync() { rDelayedQueue.offerAsync(\"llh send message\", 20, TimeUnit.SECONDS); } } Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"计算机网络原理/":{"url":"计算机网络原理/","title":"计算机网络原理","keywords":"","body":"1.1. 计算机网络专区1.1. 计算机网络专区 osi七层参考模型 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "},"计算机网络原理/计算机网络概述.html":{"url":"计算机网络原理/计算机网络概述.html","title":"计算机网络概述","keywords":"","body":"1. 计算机网络概述1. 计算机网络概述 Copyright & copy lviter@163.com            updated 2024-08-01 09:49:44 "}}